[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "",
    "text": "Instructor Information",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "",
    "text": "Name: Tyler Wallett, M.S.\n\nTerm: Fall 2025\n\nOffice location: Samson Hall Room 310\n\nOffice hours: TBD\n\nE-mail: twallett@gwu.edu\n\nGithub: twallett\n\nZoom: Meeting Link",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Course Description",
    "text": "Course Description\nThe aim of this course is to provide a comprehensive understanding of the reinforcement learning framework. The course will explore the key distinctions between reinforcement learning and other artificial intelligence learning paradigms, delve into relevant industry applications, and examine both classical and deep learning approaches. Additionally, the course will cover the taxonomy of reinforcement learning and offer hands-on experience through practical implementations using OpenAI Gymnasium and other learning environments.\nThe classical approach will focus on learning methods designed to find optimal solutions in tabular environments, whereas the deep learning approach will tackle the challenge of finding approximate optimal solutions in large or continuous environments through the use of deep learning architectures.\nThe course will introduce the taxonomy of reinforcement learning by focusing on model-free value-based methods, transitioning to value function approximation and deep learning approaches, followed by novel policy-based methods using state-of-the-art architectures to tackle complex environments.\nTo conclude, a discussion on advanced topics, applications, and outlook of reinforcement learning will be provided.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement Reinforcement Learning frameworks using numpy and tensorflow. \nDesign decision-making systems using classical and deep learning architectures. \nExplain the Reinforcement Learning taxonomy. \nIdentify Reinforcement Learning‚Äôs challenges, current research, and future outlook.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Resources",
    "text": "Resources\n\nReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto (Web Link)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#software-requirements",
    "href": "index.html#software-requirements",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nProgramming: Python.\n\npip install numpy\npip install tensorflow\npip install pygame\npip install gymnasium\npip install stable-baselines3\n\nCloud Services: Google Colab.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Course Outline",
    "text": "Course Outline\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nQuiz/Exams\nSubjects\n\n\n\n\nAug 29, 2025\nIntroduction to Reinforcement Learning\n\n- Course Outline  - Why should I study Reinforcement Learning?  - What is Reinforcement Learning?  - Where is Reinforcement Learning Applied?  - How is Reinforcement Learning Structured? \n\n\nSep 5, 2025\nMathematical Foundations\n\n- Set Theory  - Axiomatic Probability  - Conditioning  - Independence  - Random Variables  - Expectation  - Probability Distribution\n\n\nSep 12, 2025\nMulti-Armed Bandits\nQuiz 1\n- Multi-Armed Bandit Framework  - \\(\\epsilon\\)-Greedy  - Upper Confidence Boundary (UCB)  - Thompson Sampling\n\n\nSep 19, 2025\nDynamic Programming\nQuiz 2\n- Markov Chain  - Markov Decision Process (MDPs)  - Dynamic Programming\n\n\nSep 26, 2025\nMonte Carlo\nQuiz 3\n- OpenAI Gymansium Environment: GridWorld  - Monte Carlo Prediction  - Exploring Starts Monte Carlo  - On-Policy Monte Carlo  - Off-Policy Monte Carlo\n\n\nOct 3, 2025\nTemporal Difference\nQuiz 4\n- OpenAI Gymansium Environment: GridWorld  - Temporal Difference (TD) Prediction  - SARSA  - Q-Learning  - Double Q-Learning  - (Optional) n-step TD\n\n\nOct 10, 2025\nFunction Approximation\nExam 1\n- OpenAI Gymansium Environment: MountainCar  - Value Function Approximation (VFA)  - On-Policy Function Approximation  - Semi-gradient SARSA  - Limitations of Off-Policy Function Approximation\n\n\nOct 24, 2025\nDeep Q-Networks\nQuiz 5\n- OpenAI Gymansium Environment: BreakOut  - Multi-Layered Perceprtons (MLPs)  - Convolutional Neural Networks (CNNs)  - Experience Replay  - Fixed Targets  - Vanilla Deep Q-Network\n\n\nOct 31, 2025\nPolicy Gradients\nQuiz 6\n- OpenAI Gymansium Environment: CartPole  - Policy Gradient Theorem  - Vanilla Policy Gradient\n\n\nNov 7, 2025\nAdvanced Policy Gradients\nQuiz 7\n- OpenAI Gymansium Environment: HalfCheetah  - Trust Region Policy Optimization (TRPO)  - Proximal Policy Optimization: KL-Divergence  - Proximal Policy Optimization: Clip\n\n\nNov 14, 2025\nThanksgiving Break\n\n\n\n\nNov 21, 2025\nMonte Carlo Tree Search\nExam 2\n- OpenAI Gymansium Environment: Tic Tac Toe  - Model-based Reinforcement Learning  - Monte Carlo Tree Search  - AlphaGo  - MuZero\n\n\nOct 28, 2025\nConclusion\n\n\n\n\nDec 5, 2025\nFinal Project Submission",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nDATS 6101 - Introduction to Data Science",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#assignments-grading",
    "href": "index.html#assignments-grading",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\n\n\n\nAssignment\nPoints\n\n\n\n\nQuizzes (5 best scores)\n25\n\n\nExam 1\n25\n\n\nExam 2\n25\n\n\nFinal Project\n25\n\n\n\n\n\n\n\n\n\nAverage Learning Per Week\n\n\n\nStudents are expected to spend a minimum of 100 minutes of out-of-class work for every 50 minutes of direct instruction, for a minimum total of 2.5 hours a week. A 3-credit course should include 2.5 hours of direct instruction and a minimum of 5 hours of independent learning or 7.5 hours per week.\n\n\n\n\n\n\n\n\nOnline Resources\n\n\n\nFor technical requirements and support, student services, obtaining a GWorld card, and state contact information please check HERE\n\n\n\n\n\n\n\n\nClassroom Recording\n\n\n\nThe particular class recordings will be available to students who are registered on an individual basis, upon request. Please let me know in advance if you have any medical issues or emergencies that will prevent you from joining the class.\n\n\n\n\n\n\n\n\nVirtual Academic Support\n\n\n\nA full range of academic support is offered virtually in fall 2020. See HERE for updates. Tutoring and course review sessions are offered through Academic Commons in an online format. See HERE. Writing and research consultations are available online. See HERE. Coaching, offered through the Office of Student Success, is available in a virtual format. See HERE. Academic Commons offers several short videos addressing different virtual learning strategies for the unique circumstances of the fall 2020 semester. See HERE. They also offer a variety of live virtual workshops to equip students with the tools they need to succeed in a virtual environment. See HERE.\n\n\n\n\n\n\n\n\nSafety and Security\n\n\n\nIn an emergency: call GWPD 202-994-6111 or 911. For situation-specific actions: review the Emergency Response Handbook in HERE. In an active violence situation: Get Out, Hide Out, or Take Out. See HERE. Stay informed: safety.gwu.edu/stay-informed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html",
    "href": "lecture1/lecture1-1.html",
    "title": "1.1 Why should I study Reinforcement Learning?",
    "section": "",
    "text": "Advances in Artificial Intelligence\nIn essence, the AI model‚Äôs behavior closely mirrors human-like actions. This is expected, given its training on extensive datasets derived from human behavior. However, continued training on the same datasets will likely result in models that perform at a human-equivalent level. To achieve significant breakthroughs, we must explore learning methods that transcend typical human capabilities.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why should I study Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html#advances-in-artificial-intelligence",
    "href": "lecture1/lecture1-1.html#advances-in-artificial-intelligence",
    "title": "1.1 Why should I study Reinforcement Learning?",
    "section": "",
    "text": "June 2018: OpenAI introduces the Generative Pre-trained Transformer (GPT), laying the foundation for subsequent LLMs.\nFebruary 2019: OpenAI releases GPT-2, demonstrating significant improvements in text generation capabilities.\nJune 2020: GPT-3 is unveiled, featuring 175 billion parameters and showcasing advanced language understanding and generation.\nJanuary 2021: OpenAI announces DALL-E, a model capable of generating images from textual descriptions.\nApril 2022: DALL-E 2 is introduced, offering enhanced image resolution and greater realism in generated images.\nNovember 2022: OpenAI releases ChatGPT, a conversational AI based on the GPT-3.5 architecture, enabling more interactive and contextually relevant dialogues.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt GWU, where minds aspire,  Reinforcement learners never tire  In search of policies, bold and bright,  They train their agents, day and night.   With Sutton, Barto as their guide,  They walk the path, rewards beside.  Exploring states with epsilon‚Äôs grace,  They find the optimal embrace.   Gridworlds vast, mazes deep,  In code they sow, in dreams they reap.  The future‚Äôs theirs, they push, they strive‚Äì  GWU‚Äôs learners, alive, alive!\n\n\n\nPoem and images generated by Large Language Model: ChatGPT 3.5 & Diffusion Models: DALL-E, correspondingly.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why should I study Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html#the-goal-of-reinforcement-learning",
    "href": "lecture1/lecture1-1.html#the-goal-of-reinforcement-learning",
    "title": "1.1 Why should I study Reinforcement Learning?",
    "section": "The Goal of Reinforcement Learning",
    "text": "The Goal of Reinforcement Learning\n\n\n\n\n\n\nEmergent Behavior\n\n\n\nReinforcement Learning is concerned with seeking emergent behavior, or behavior that goes beyond what people might do or think of. \n\n\nUltimately, as scientists, we want to discover new solutions for a task so that when an agent, or decision-maker, is placed in a novel situation it can respond intelligently (Levine 2019).\n\nExample: AlphaGO\n\n\n\n\n\nAlphaGo Move 37\n\n\n\n‚ÄúIt‚Äôs not a human move. I‚Äôve never seen a human play this move.‚Äù ‚Äì Commentator on Move 37, AlphaGo (2017)\n\n\n\n\nExample: Matrix Multiplication\n\n\n\n\n\nDiscovering faster matrix multiplication algorithms with reinforcement learning\n\n\n\n‚ÄúTrained from scratch, AlphaTensor discovers matrix multiplication algorithms that are more efficient than existing human and computer-designed algorithms.‚Äù - Discovering faster matrix multiplication algorithms with reinforcement learning",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why should I study Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html",
    "href": "lecture1/lecture1-2.html",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "",
    "text": "Learning Optimal Sequential Decision-Making Under Uncertainty\nGood news! We can sum up the core idea of Reinforcement Learning in just one powerful sentence (Brunskill 2022):\nBut what exactly does that mean? Let‚Äôs break it down!",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html#learning",
    "href": "lecture1/lecture1-2.html#learning",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "Learning",
    "text": "Learning\nAt its core, learning in Reinforcement Learning occurs through trial and error, where an agent refines its actions based on evaluative feedback from the environment.\n\n\n\n\n\n\nEvaluative Feedback\n\n\n\nEvaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action. \n\nIntuition: Learning through experience\n\n\n\n\n\n\n\n\n\nLearning to Walk\n\n\n\n\n\n\n\n\nEthics (Right vs.¬†Wrong)\n\n\n\n\n\nUnlike both supervised/unsupervised learning which rely on instructive feedback through gradient based optimization.\n\n\n\n\n\n\nInstructive Feedback\n\n\n\nInstructive feedback indicates the correct action to take, independently of the action actually taken. \n\nIntuition: Learning through ground truth\n\n\n\n\n\n\n\nSupervised/Unsupervised Learning\n\n\n\nFor example, supervised/unsupervised learning focus on identifying what makes an image a cheetah by learning patterns from a dataset of animal images. In contrast, Reinforcement Learning is about teaching a cheetah how to run by interacting with its environment (Lecture 10).\n\n\n\n ‚ÄúHere‚Äôs some examples (images), now learn patterns in these examples‚Ä¶‚Äù\n\n\n\n ‚ÄúHere‚Äôs an environment, now learn patterns by exploring it‚Ä¶‚Äù",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html#optimal",
    "href": "lecture1/lecture1-2.html#optimal",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "Optimal",
    "text": "Optimal\nThe goal of Reinforcement Learning is to maximize rewards over time by finding the best possible strategy. This involves seeking:\n\nA Maximized discounted sum of rewards, or goal \\(G\\).\nOptimal Value Functions \\(V^{*}\\).\nOptimal Action-Value Functions \\(Q^{*}\\).\nOptimal Policies \\(\\pi^{*}\\).\nA Balance between exploration vs.¬†exploitation.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html#sequential-decision-making",
    "href": "lecture1/lecture1-2.html#sequential-decision-making",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "Sequential Decision-Making",
    "text": "Sequential Decision-Making\nUnlike a one-time choice, Reinforcement Learning involves a chain of decisions where each action affects the next.\n\n\n\n\nMarkov Decision Process\n\n\n\n\n\\(\\pi: S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, ... , S_{T-1}, A_{T-1}, R_{T}\\)\n\n\nMarkov Decision Process (MDP) is a formal framework for modeling decision-making.\nThe agent selects actions over multiple time steps, shaping its future states and rewards.\nEach decision affects not only immediate rewards but also the trajectory of future outcomes.\n\n\n\n\n\nBrunskill, Emma. 2022. ‚ÄúCS234: Reinforcement Learning - Lecture 1.‚Äù Course Lecture Slides, Stanford University. https://web.stanford.edu/class/cs234/slides/lecture1pre.pdf.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html",
    "href": "lecture1/lecture1-3.html",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "",
    "text": "Recommendation Systems\nReinforcement Learning powers modern recommendation systems by dynamically adapting to user preferences, optimizing content suggestions in platforms like Netflix, YouTube, and Spotify using techniques such as Multi-Armed Bandits (MAB).",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#recommendation-systems",
    "href": "lecture1/lecture1-3.html#recommendation-systems",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "",
    "text": "Trained using MAB (Lecture 2)\nLink to Online Article",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#games",
    "href": "lecture1/lecture1-3.html#games",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Games",
    "text": "Games\nReinforcement Learning has revolutionized gaming by enabling AI to master complex environments, from Atari classics to advanced strategy games, using deep learning techniques like Deep Q-Networks (DQN) to achieve superhuman performance.\n\n\nTrained using DQN (Lecture 9)\nLink to Research Paper",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#robotics",
    "href": "lecture1/lecture1-3.html#robotics",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Robotics",
    "text": "Robotics\nIn robotics, Reinforcement Learning enables autonomous agents to learn complex motor skills, such as dexterous manipulation and locomotion, through continuous interaction and training with Proximal Policy Optimization (PPO).\n\n\nTrained using PPO (Lecture 11)\nLink to Blog",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#autonomous-vehicles",
    "href": "lecture1/lecture1-3.html#autonomous-vehicles",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Autonomous Vehicles",
    "text": "Autonomous Vehicles\nSelf-driving cars rely on Reinforcement Learning to navigate complex environments, optimize decision-making, and improve safety, often incorporating PPO and deep learning to refine real-time control strategies.\n\n   Partly trained using PPO (Lecture 11)",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#natural-language-processing",
    "href": "lecture1/lecture1-3.html#natural-language-processing",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nReinforcement Learning from Human Feedback (RLHF) enhances AI language models like ChatGPT, allowing them to refine responses based on user interactions and align better with human preferences.\n\n\nTrained using PPO (Lecture 11)\nLink to Research Paper",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#finance",
    "href": "lecture1/lecture1-3.html#finance",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Finance",
    "text": "Finance\nIn financial markets, RL is applied to portfolio optimization, algorithmic trading, and risk management, leveraging techniques like PPO to make data-driven investment decisions.\n\n\nTrained using PPO (Lecture 11)\nLink to Research Paper",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html#so-why-should-i-study-reinforcement-leanring",
    "href": "lecture1/lecture1-1.html#so-why-should-i-study-reinforcement-leanring",
    "title": "Motivation",
    "section": "So why should I study Reinforcement Leanring?",
    "text": "So why should I study Reinforcement Leanring?\n\n\n\n\n\n\nReinforcement Learning (RL) is not just another machine learning paradigm; it is a fundamental framework for decision-making, control, and optimizing sequential actions in uncertain environments. Studying RL enables us to: \n‚úÖ Understand Intelligence  ‚úÖ Develop Cutting-Edge AI  ‚úÖ Solve Real-World Problems  ‚úÖ Push Beyond Human Limits \nBy studying RL, we gain the tools to create AI that learns, adapts, and innovates beyond predefined rules and static datasets. üöÄ",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Motivation</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html#so-why-should-i-study-reinforcement-learning",
    "href": "lecture1/lecture1-1.html#so-why-should-i-study-reinforcement-learning",
    "title": "1.1 Why should I study Reinforcement Learning?",
    "section": "So why should I study Reinforcement Learning?",
    "text": "So why should I study Reinforcement Learning?\nReinforcement Learning (RL) is not just another machine learning paradigm; it is a fundamental framework for decision-making, control, and optimizing sequential actions in uncertain environments. Studying RL enables us to: \n\n‚úÖ Understand Intelligence  ‚úÖ Develop Cutting-Edge AI  ‚úÖ Solve Real-World Problems  ‚úÖ Push Beyond Human Limits \n\nBy studying RL, we gain the tools to create AI that learns, adapts, and innovates beyond predefined rules and static datasets. üöÄ\n\n\n\n\nLevine, Sergey. 2019. ‚ÄúIntroduction to Deep Reinforcement Learning.‚Äù Course Lecture Slides, Deep RL Course, UC Berkeley. https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-1.pdf.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why should I study Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-4.html",
    "href": "lecture1/lecture1-4.html",
    "title": "1.4 How is Reinforcement Learning Structured?",
    "section": "",
    "text": "Taxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>1.4 How is Reinforcement Learning Structured?</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunskill, Emma. 2022. ‚ÄúCS234: Reinforcement Learning - Lecture\n1.‚Äù Course Lecture Slides, Stanford University. https://web.stanford.edu/class/cs234/slides/lecture1pre.pdf.\n\n\nLevine, Sergey. 2019. ‚ÄúIntroduction to Deep Reinforcement\nLearning.‚Äù Course Lecture Slides, Deep RL Course, UC Berkeley. https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-1.pdf.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html",
    "href": "lecture2/lecture2-1.html",
    "title": "2.1 Set Theory",
    "section": "",
    "text": "Sets\nA set is a collection of things.  The things are called elements of a set.\n\\[\nColors = \\{Red, Blue, Green\\}\n\\]\n\\[\nNumbers = \\{1,2,3\\}\n\\]\nSets can be finite or infinite.\n\\[\nSome\\ Even\\ Integers = \\{2,4,6\\}\n\\]\n\\[\nAll\\ Even\\ Integers = \\{..., -4, -2, 0, 2, 4, ...\\}\n\\]\nThe number of elements in a set is called the cardinality.\n\\[\n|Colors| = 3\n\\]\nTwo sets are equal if they share exactly the same elements.\n\\[\nA = \\{2,4,6\\}, B = \\{4,2,6\\}, C = \\{4,2,7\\}\n\\]\n\\[\nA = B\n\\] \\[\nA \\neq C\n\\]\nTo express that \\(2\\) is an element of \\(A\\), we denote:\n\\[\n2 \\in A\n\\] \\[\n\\text{2 exists in A}\n\\]\n\\[\n5 \\notin A\n\\] \\[\n\\text{5 does not exist in A}\n\\]\nSome sets are so significant that we reserve special symbols for them:\n\\[\n\\emptyset = \\{\\} \\quad \\textbf{(empty set)}\n\\]\n\\[\n\\mathbb{N} = \\{1, 2, 3, ... \\} \\quad \\textbf{(natural numbers)}\n\\]\n\\[\n\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ... \\} \\quad \\textbf{(integers)}\n\\]\n\\[\n\\mathbb{R} = \\{..., -0.22,...,0,...,1,..., \\pi, ... \\} \\quad \\textbf{(real numbers)}\n\\]\nWe visualize \\(\\mathbb{R}\\) as an infinitely long number line.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#sets",
    "href": "lecture2/lecture2-1.html#sets",
    "title": "2.1 Set Theory",
    "section": "",
    "text": "$$",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#sets-continued",
    "href": "lecture2/lecture2-1.html#sets-continued",
    "title": "2.1 Set Theory",
    "section": "Sets (continued)",
    "text": "Sets (continued)\n\nTwo sets are equal if they share exactly the same elements.\n\n\nExamples:\n\\[\nA = \\{2,4,6\\}, B = \\{4,2,6\\}, C = \\{4,2,7\\}\n\\]\n\\[\nA = B, A \\neq C\n\\]\n\nTo express that (2) is an element of (A), we denote:\n\n\\[\n2 \\in A \\quad \\text{\"2 exists in A\"}\n\\]\n\\[\n5 \\notin A \\quad \\text{\"5 does not exist in A\"}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#important-sets",
    "href": "lecture2/lecture2-1.html#important-sets",
    "title": "2.1 Set Theory",
    "section": "Important Sets",
    "text": "Important Sets\n\nSome sets are so significant that we reserve special symbols for them:\n\n\\[\n\\emptyset = \\{\\} \\quad \\text{(empty set)}\n\\]\n\\[\n\\mathbb{N} = \\{0, 1, 2, 3, ... \\} \\quad \\text{(natural numbers)}\n\\]\n\\[\n\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ... \\} \\quad \\text{(integers)}\n\\]\n\\[\n\\mathbb{R} = \\{..., -0.22,...,0,...,1,..., \\pi, ... \\} \\quad \\text{(real numbers)}\n\\]\n\nWe visualize () as an infinitely long number line.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#set-builder-notation",
    "href": "lecture2/lecture2-1.html#set-builder-notation",
    "title": "2.1 Set Theory",
    "section": "Set-Builder Notation",
    "text": "Set-Builder Notation\nA special notation called set-builder notation is used to describe sets that are too big or complex to list between braces.\n\\[\nAll\\ Even\\ Integers_{1} = \\{..., -4, -2, 0, 2, 4, ...\\}\n\\]\n\\[\nAll\\ Even\\ Integers_{2} = \\{2x: x \\in \\mathbb{Z} \\}\n\\]\n\\[\n\\text{The set of all numbers of the form } 2x \\text{ such that } x \\in \\mathbb{Z}\n\\]\n\\[\nAll\\ Even\\ Integers_{1} = All\\ Even\\ Integers_{2}\n\\]\n\nExercise\nWrite the following sets in set-builder notation:\n\n\\(\\{ 2, 4, 8, 16, 32, 64, ... \\}\\)\n\\(\\{ 0, 1, 4, 9, 16, 25, 36, ... \\}\\)\n\\(\\{ 3, 4, 5, 6, 7, 8 \\}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\{ 2^x: x \\in \\mathbb{N} \\}\\)\n\\(\\{ x^2: x \\in \\mathbb{Z} \\}\\)\n\\(\\{ x \\in \\mathbb{Z}: 3 \\le x \\le 8 \\}\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#example-sets",
    "href": "lecture2/lecture2-1.html#example-sets",
    "title": "2.1 Set Theory",
    "section": "Example: Sets",
    "text": "Example: Sets\n\nSketch the following set of points in the x-y plane:\n\n\\[\n\\{(x,y) : x, y \\in \\mathbb{R}, \\ x^{2} + y^{2} \\leq 1 \\}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#subsets",
    "href": "lecture2/lecture2-1.html#subsets",
    "title": "2.1 Set Theory",
    "section": "Subsets",
    "text": "Subsets\nSuppose \\(A\\) and \\(B\\) are sets.  If every element of \\(A\\) is also an element of \\(B\\), then we say \\(A\\) is a subset of \\(B\\), denoted \\(A \\subseteq B\\).  We write \\(A \\not\\subseteq B\\) if \\(A\\) is not a subset of \\(B\\), that is, if it is not true that every element of \\(A\\) is also an element of \\(B\\). Thus \\(A \\not\\subseteq B\\) means that there is at least one element of \\(A\\) that is not an element of \\(B\\).\n\\[\n\\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{R}\n\\]\n\\[\nA = \\{1,2\\}, B = \\{2,3,4\\}\n\\] \\[\nA \\not\\subseteq B\n\\]\n\nExercise\nList all the subsets of the following sets:\n\n\\(\\{1,2,3\\}\\)\n\\(\\{1,\\{2,3\\}\\}\\)\n\\(\\{\\mathbb{N}, \\mathbb{Z}, \\mathbb{R}\\}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\{\\}, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\)\n\\(\\{\\}, \\{1\\}, \\{\\{2,3\\}\\}, \\{1,\\{2,3\\}\\}\\)\n\\(\\{\\}, \\{\\mathbb{N}\\}, \\{\\mathbb{Z}\\}, \\{\\mathbb{R}\\}, \\{\\mathbb{N},\\mathbb{Z}\\}, \\{\\mathbb{N},\\mathbb{R}\\}, \\{\\mathbb{Z},\\mathbb{R}\\}, \\{\\mathbb{N},\\mathbb{Z},\\mathbb{R}\\}\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#example-subsets",
    "href": "lecture2/lecture2-1.html#example-subsets",
    "title": "2.1 Set Theory",
    "section": "Example: Subsets",
    "text": "Example: Subsets\n\nList all the subsets of the following sets:\n\n({1,2,3})\n({1,{2,3}})\n({, , })",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#union-intersection-and-difference",
    "href": "lecture2/lecture2-1.html#union-intersection-and-difference",
    "title": "2.1 Set Theory",
    "section": "Union, Intersection, and Difference",
    "text": "Union, Intersection, and Difference\nSuppose \\(A\\) and \\(B\\) are sets.\nA union of \\(A\\) and \\(B\\) is the set: \\[\nA \\cup B = \\{x: x \\in A \\text{ or } x \\in B \\}\n\\]\nA intersection of \\(A\\) and \\(B\\) is the set: \\[\nA \\cap B = \\{x: x \\in A \\text{ and } x \\in B \\}\n\\]\nA difference of \\(A\\) and \\(B\\) is the set: \\[\nA - B = \\{x: x \\in A \\text{ and } x \\notin B \\}\n\\]\n\n\n\n\nExercise\nShade in the region matching the expression:\n\n\\((A \\cap B) \\cap C\\)\n\\((A \\cup B) \\cap C\\)\n\\((A \\cup B) - C\\)\n\n\n\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#example-union-intersection-and-difference",
    "href": "lecture2/lecture2-1.html#example-union-intersection-and-difference",
    "title": "2.1 Set Theory",
    "section": "Example: Union, Intersection, and Difference",
    "text": "Example: Union, Intersection, and Difference\n\nShade in the region matching the expression:\n\n((A B) C)\n((A B) C)\n((A B) - C)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#complements",
    "href": "lecture2/lecture2-1.html#complements",
    "title": "2.1 Set Theory",
    "section": "Complements",
    "text": "Complements\nSuppose \\(A\\) is a set.  A universal set is a larger set that encompasses other sets.  The complement of \\(A\\), denoted \\(\\bar{A}\\), is the set \\(\\bar{A} = U - A\\).\n\\[\nP = \\{2, 3, 5, 7, ...\\} \\quad \\textbf{(prime numbers)}\n\\]\n\\[\n\\bar{P} = \\mathbb{N} - P = \\{1, 4, 6, ...\\}\n\\]\n\nExercise\nFind \\(\\bar{A}\\):\n\\[\nA = \\{1,2,3\\}, U = \\{0,1,2,3,4,5\\}\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\bar{A} = \\{0, 4, 5\\}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#example-complements",
    "href": "lecture2/lecture2-1.html#example-complements",
    "title": "2.1 Set Theory",
    "section": "Example: Complements",
    "text": "Example: Complements\n\nFind the complement of the following set:\n\n\\[\n\\{(x,y) : x, y \\in \\mathbb{R}, \\ x^{2} + y^{2} \\leq 1 \\}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#homework-lecture-2",
    "href": "lecture2/lecture2-1.html#homework-lecture-2",
    "title": "2.1 Set Theory",
    "section": "Homework: Lecture 2",
    "text": "Homework: Lecture 2\n\nQuestion 1.\nLink to GitHub Homework.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html",
    "href": "lecture2/lecture2-2.html",
    "title": "2.2 Axiomatic Probability",
    "section": "",
    "text": "Methodology\nSteps to perform a probabilistic model:",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html",
    "href": "lecture2/lecture2-3.html",
    "title": "2.3 Conditioning",
    "section": "",
    "text": "Conditional Probability\nSuppose \\(A\\) and \\(B\\) are events.  Conditional probability is the likelihood of an event occurring given that we know another event has occurred.\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIntuition: conditioned event becomes the new sample space.  If \\(P(B) \\neq 0\\), then the probability of the intersection normalized by the conditioned space.  Else \\(P(B) = 0\\), then \\(P(A|B)\\) is undefined.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#conditional-probability",
    "href": "lecture2/lecture2-3.html#conditional-probability",
    "title": "2.3 Conditioning",
    "section": "",
    "text": "Intuition: conditioned event becomes the new sample space.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#conditional-probability-definition",
    "href": "lecture2/lecture2-3.html#conditional-probability-definition",
    "title": "2.3 Conditioning",
    "section": "Conditional Probability Definition",
    "text": "Conditional Probability Definition\nIf \\(P(B) \\neq 0\\), then:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n‚ÄúThe Probability of the intersection normalized by the conditioned space.‚Äù\nElse \\(P(B) = 0\\), then \\(P(A|B)\\) is undefined.\nMultiplication Rule:\n\\[\nP(A \\cap B) = P(A|B) P(B) = P(B|A) P(A)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#total-probability-theorem",
    "href": "lecture2/lecture2-3.html#total-probability-theorem",
    "title": "2.3 Conditioning",
    "section": "Total Probability Theorem",
    "text": "Total Probability Theorem\nSuppose \\(A_{1,...,n}\\) and \\(B\\) are sets.  The total probability theorem allows us to compute the likelihood of an event by summing over conditional probabilities of different partitions of the sample space.\n\\[\nP(B) = P(B|A_1) P(A_1) + P(B|A_2) P(A_2) + P(B|A_3) P(A_3)\n\\]\n\n\n\nIn general terms:\n\\[\nP(B) = P(B|A_1) P(A_1) + ... + P(B|A_n) P(A_n)\n\\]\n\nExercise\nExperiment: Classifying emails as spam \\(S\\) or not spam \\(NS\\) based on the word \\(W\\) or not the word \\(NW\\) ‚Äúfree‚Äù.\n\\[\nP(S) = 0.4\n\\]\n\\[\nP(NS) = 0.6\n\\]\n\\[\nP(W|S) = 0.7\n\\]\n\\[\nP(W|NS) = 0.1\n\\]\nFind the probability of \\(P(S|W)\\):\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\nP(S|W) = \\frac{P(S \\cap W)}{P(W)} = \\underbrace{\\frac{P(W|S)P(S)}{P(W|S)P(S) + P(W|NS)P(NS)}}_{Bayes' \\ Theorem} = \\frac{0.7 * 0.4}{0.7 * 0.4 + 0.1 * 0.6} = 0.82\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#example-conditional-probability",
    "href": "lecture2/lecture2-3.html#example-conditional-probability",
    "title": "2.3 Conditioning",
    "section": "Example: Conditional Probability",
    "text": "Example: Conditional Probability\n\nExperiment: Diagnosing a rare disease\nFind the following probabilities:\n\n\\(P(Disease \\cap Pos)\\)\n\\(P(Pos)\\)\n\\(P(Disease | Pos)\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html",
    "href": "lecture2/lecture2-4.html",
    "title": "2.4 Independence",
    "section": "",
    "text": "Independent Events\nSuppose \\(A\\) and \\(B\\) are two events.  Two events are independent if the occurrence of event \\(B\\) provides no information about the occurrence of event \\(A\\):\n\\[\nP(A|B) = P(A)\n\\]\nAnother definition of independence:\n\\[\nP(A \\cap B) = P(A) P(B)\n\\]\nFor multiple events:\n\\[\nP(A_{1} \\cap A_{2} \\cap ... A_{n}) = P(A_{1}) P(A_{2}) ... P(A_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#definition-of-independence",
    "href": "lecture2/lecture2-4.html#definition-of-independence",
    "title": "2.4 Independence",
    "section": "Definition of Independence",
    "text": "Definition of Independence\n\nSuppose \\(A\\) and \\(B\\) are two events. They are independent if:\n\\[ P(A|B) = P(A) \\]\n‚ÄúThe occurrence of event \\(B\\) provides no information about the occurrence of event \\(A\\).‚Äù\nAnother definition:\n\\[ P(A \\cap B) = P(A) P(B) \\]\nFor multiple events:\n\\[ P(A_{1} \\cap A_{2} \\cap ... A_{n}) = P(A_{1}) P(A_{2}) ... P(A_{n}) \\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#example-independence",
    "href": "lecture2/lecture2-4.html#example-independence",
    "title": "2.4 Independence",
    "section": "Example: Independence",
    "text": "Example: Independence\n\nWhich of the Venn diagrams shows 2 independent events?",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#conditioning-and-independence",
    "href": "lecture2/lecture2-4.html#conditioning-and-independence",
    "title": "2.4 Independence",
    "section": "Conditioning and Independence",
    "text": "Conditioning and Independence\nSuppose \\(A\\), \\(B\\), and \\(C\\) are events.  If \\(A\\) and \\(B\\) are independent, conditioning on \\(C\\) may remove that independence.  When we condition on \\(C\\), events \\(A\\) and \\(B\\) may no longer be independent.\n\n\n\n\nExercise\nThe king comes from a family of two children. What is the probability that his sibling is female \\(F\\) and not male \\(M\\)?\nLet all outcomes be equally likely.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\nSample \\ Space = \\{(FF), (FM), (MF), (MM)\\} = \\{(\\not F \\not F), (FM), (MF), (MM)\\}\n\\]\n\\[\nP(F|M) = \\frac{P(F \\cap M)}{M} = \\frac{2}{3} \\approx 0.6\\bar{6}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#example-conditioning-and-independence",
    "href": "lecture2/lecture2-4.html#example-conditioning-and-independence",
    "title": "2.4 Independence",
    "section": "Example: Conditioning and Independence",
    "text": "Example: Conditioning and Independence\n\nThe king comes from a family of two children. What is the probability that his sibling is female?",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html",
    "href": "lecture2/lecture2-5.html",
    "title": "2.5 Discrete Random Variables",
    "section": "",
    "text": "Discrete Random Variable\nA discrete random variable is a mapping \\(X\\) of all of the outcomes of a sample space to numerical values \\(x \\in \\mathbb{Z}\\).\n\\[\nX: Outcome \\in Sample \\ Space \\to x \\in \\mathbb{Z}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#definition",
    "href": "lecture2/lecture2-5.html#definition",
    "title": "2.5 Random Variables",
    "section": "Definition",
    "text": "Definition\nA continuous random variable is a mapping \\(X\\) of all the events of a sample space to numerical values \\(\\lambda\\) in \\(\\mathbb{R}\\).\n\nNotation\n\\[ X: event \\in Sample \\ Space \\to \\lambda \\in \\mathbb{R} \\]\n\n\nProbability Density Function (PDF)\nA probability density function (PDF) is a mapping of values \\(\\lambda\\) of intervals of continuous random variables \\(X\\) to probabilities \\([0,1]\\).\n\nNotation\n\\[ P(a \\leq X \\leq b) = \\int_{a}^{b} f_{X}(\\lambda) \\ d\\lambda \\]\n\n\nProperties\n\n$ f_{X}() $\n$ {-}^{} f{X}() ¬†d= 1 $\n\n\n\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for continuous random variables is defined as:\n\\[ F_{X}(\\lambda) = P(X \\leq \\lambda) = \\int_{-\\infty}^{\\lambda} f_{X}(u) \\ du \\]\n\nRelationship between CDF and PDF:\n\n$ f_{X}() = $\n$ F_{X}() = {-}^{} f{X}(u) ¬†du $\n\n\n\n\nExpectation and Variance\n\nExpectation (Mean)\n\\[ E[X] = \\int_{-\\infty}^{\\infty} \\lambda \\ f_{X}(\\lambda) d\\lambda \\]\n\n\nVariance\n\\[ \\text{Var}[X] = E[(X - E[X])^{2}] = \\int_{-\\infty}^{\\infty} (\\lambda - E[X])^2 \\ f_{X}(\\lambda) d\\lambda \\]\n\n\n\nJoint Probability Density Function (Joint PDF)\nFor two continuous random variables: \\[ P((X,Y) \\in A) = \\int \\int_{A} f_{X,Y}(\\lambda_{1}, \\lambda_{2}) d\\lambda_{1} \\ d\\lambda_{2} \\]\n\nMarginal PDFs\n\n$ f_{X}({1}) = {-}^{} f_{X,Y}({1}, {2}) d_{2} $\n$ f_{Y}({2}) = {-}^{} f_{X,Y}({1},{2}) d_{1} $\n\n\n\n\nConditional PDFs\nThe conditional probability density function: \\[ f_{X|Y}(\\lambda_{1}| \\lambda_{2}) = \\frac{f_{X,Y}(\\lambda_{1}, \\lambda_{2})}{f_{Y}(\\lambda_{2})} \\]\n\nConditional Expectation\n\\[ \\mathbb{E}[X| Y = \\lambda_{2}] = \\int_{\\lambda_{1} \\in X} \\lambda_{1} \\frac{f_{X,Y}(\\lambda_{1}, \\lambda_{2})}{f_{Y}(\\lambda_{2})} d\\lambda_{1} \\]\n\n\n\nIndependence\nTwo continuous random variables are independent if: \\[ f_{X,Y}(\\lambda_{1}, \\lambda_{2}) = f_{X}(\\lambda_{1}) f_{Y}(\\lambda_{2}) \\]\nThis can be extended to multiple random variables: \\[ f_{X_{1},...,X{n}}(\\lambda_{1}, ..., \\lambda_{n}) = f_{X_{1}}(\\lambda_{1}) ... \\ f_{X_{n}}(\\lambda_{n}) \\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#probability-mass-function-pmf",
    "href": "lecture2/lecture2-5.html#probability-mass-function-pmf",
    "title": "2.5 Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nA probability mass function (PMF) is a mapping of values \\(x\\) of discrete random variables \\(X\\) to probabilities \\([0,1]\\).\n\\[\np_{X}(x) = P(X = x)\n\\]\nPMF has properties: \\[\np_{X}(x) \\geq 0, \\quad \\sum_{x} p_{X}(x) = 1\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#cumulative-distribution-function-cdf",
    "href": "lecture2/lecture2-5.html#cumulative-distribution-function-cdf",
    "title": "2.5 Discrete Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for discrete random variables is defined as:\n\\[\nF_{X}(x) = P(X \\leq x) = \\sum_{k \\leq x} p_{X}(k)\n\\]\nRelation to PMF: \\[\np_{X}(x) = \\frac{dF_{X}(x)}{dx}\n\\]\n\\[\nF_{X}(x) = \\sum_{k \\leq x}p_{X}(k)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#expectation-and-variance",
    "href": "lecture2/lecture2-5.html#expectation-and-variance",
    "title": "2.5 Discrete Random Variables",
    "section": "Expectation and Variance",
    "text": "Expectation and Variance\nThe expectation of a discrete random variable is the sum of all possible values weighted by their probabilities, representing the long-term average outcome.\n\\[\nE[X] = \\sum_{x} x \\ p_{X}(x)\n\\]\nThe variance of a discrete random variable quantifies the spread of its values around the mean by calculating the expected squared deviation from the mean. \\[\nVar[X] = E[(X - E[X])^{2}] = \\sum_{x}(x - E[X])^2 p_{X}(x)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#joint-and-marginal-distributions",
    "href": "lecture2/lecture2-5.html#joint-and-marginal-distributions",
    "title": "2.5 Discrete Random Variables",
    "section": "Joint and Marginal Distributions",
    "text": "Joint and Marginal Distributions\nThe joint PMF gives the probability that two discrete random variables take on specific values simultaneously. \\[\np_{X,Y}(x_{1},x_{2}) = P(X = x_{1}, Y = x_{2})\n\\]\nThe marginal PMF of a discrete random variable is found by summing the joint PMF over all possible values of the other variable, giving the probability of a single variable. \\[\np_{X}(x_{1}) = \\sum_{x_{2}} p_{X,Y}(X = x_{1},Y = x_{2})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#conditional-expectation",
    "href": "lecture2/lecture2-5.html#conditional-expectation",
    "title": "2.5 Discrete Random Variables",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nThe conditional PMF gives the probability distribution of a discrete random variable given that another variable has a specific value. \\[\np_{X|Y}(x_{1}| x_{2}) = \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]\nThe conditional expectation is the expected value of a discrete random variable given that another variable is fixed at a specific value. \\[\nE[X| Y = x_{2}] = \\sum_{x_{1}} x_{1} \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#independence",
    "href": "lecture2/lecture2-5.html#independence",
    "title": "2.5 Discrete Random Variables",
    "section": "Independence",
    "text": "Independence\nTwo discrete random variables are independent if:\n\\[\np_{X,Y}(x_{1}, x_{2}) = p_{X}(x_{1}) p_{Y}(x_{2})\n\\]\nFor multiple discrete random variables: \\[\np_{X_{1},...,X_{n}}(x_{1}, ..., x_{n}) = p_{X_{1}}(x_{1}) ... \\ p_{X_{n}}(x_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#example-problems",
    "href": "lecture2/lecture2-5.html#example-problems",
    "title": "2.5 Discrete Random Variables",
    "section": "Example Problems",
    "text": "Example Problems\n\nCompute the expectation and variance of a fair die roll.\nShow that two independent fair die rolls satisfy the independence property.\nCompute the joint and marginal PMFs for two unfair dice.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#continuous-random-variables",
    "href": "lecture2/lecture2-5.html#continuous-random-variables",
    "title": "2.5 Random Variables",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nDefinition\nA continuous random variable is a mapping \\(X\\) of all the events of a sample space to numerical values \\(\\lambda\\) in \\(\\mathbb{R}\\).\n\nNotation\n\\[ X: event \\in Sample \\ Space \\to \\lambda \\in \\mathbb{R} \\]\n\n\n\nProbability Density Function (PDF)\nA probability density function (PDF) is a mapping of values \\(\\lambda\\) of intervals of continuous random variables \\(X\\) to probabilities \\([0,1]\\).\n\nNotation\n\\[ P(a \\leq X \\leq b) = \\int_{a}^{b} f_{X}(\\lambda) \\ d\\lambda \\]\n\n\nProperties\n\n$ f_{X}() $\n$ {-}^{} f{X}() ¬†d= 1 $\n\n\n\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for continuous random variables is defined as:\n\\[ F_{X}(\\lambda) = P(X \\leq \\lambda) = \\int_{-\\infty}^{\\lambda} f_{X}(u) \\ du \\]\n\nRelationship between CDF and PDF:\n\n$ f_{X}() = $\n$ F_{X}() = {-}^{} f{X}(u) ¬†du $\n\n\n\n\nExpectation and Variance\n\nExpectation (Mean)\n\\[ E[X] = \\int_{-\\infty}^{\\infty} \\lambda \\ f_{X}(\\lambda) d\\lambda \\]\n\n\nVariance\n\\[ \\text{Var}[X] = E[(X - E[X])^{2}] = \\int_{-\\infty}^{\\infty} (\\lambda - E[X])^2 \\ f_{X}(\\lambda) d\\lambda \\]\n\n\n\nJoint Probability Density Function (Joint PDF)\nFor two continuous random variables: \\[ P((X,Y) \\in A) = \\int \\int_{A} f_{X,Y}(\\lambda_{1}, \\lambda_{2}) d\\lambda_{1} \\ d\\lambda_{2} \\]\n\nMarginal PDFs\n\n$ f_{X}({1}) = {-}^{} f_{X,Y}({1}, {2}) d_{2} $\n$ f_{Y}({2}) = {-}^{} f_{X,Y}({1},{2}) d_{1} $\n\n\n\n\nConditional PDFs\nThe conditional probability density function: \\[ f_{X|Y}(\\lambda_{1}| \\lambda_{2}) = \\frac{f_{X,Y}(\\lambda_{1}, \\lambda_{2})}{f_{Y}(\\lambda_{2})} \\]\n\nConditional Expectation\n\\[ \\mathbb{E}[X| Y = \\lambda_{2}] = \\int_{\\lambda_{1} \\in X} \\lambda_{1} \\frac{f_{X,Y}(\\lambda_{1}, \\lambda_{2})}{f_{Y}(\\lambda_{2})} d\\lambda_{1} \\]\n\n\n\nIndependence\nTwo continuous random variables are independent if: \\[ f_{X,Y}(\\lambda_{1}, \\lambda_{2}) = f_{X}(\\lambda_{1}) f_{Y}(\\lambda_{2}) \\]\nThis can be extended to multiple random variables: \\[ f_{X_{1},...,X{n}}(\\lambda_{1}, ..., \\lambda_{n}) = f_{X_{1}}(\\lambda_{1}) ... \\ f_{X_{n}}(\\lambda_{n}) \\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html",
    "href": "lecture2/lecture2-6.html",
    "title": "2.6 Continuous Random Variables",
    "section": "",
    "text": "Continuous Random Variable\nA continuous random variable is a mapping \\(X\\) of all the events of a sample space to numerical values \\(x \\in \\mathbb{R}\\). \\[\nX: Event \\in Sample \\ Space \\to x \\in \\mathbb{R}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#bernoulli-distribution",
    "href": "lecture2/lecture2-6.html#bernoulli-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\nBernoulli Distribution PMF\n\\[\np_{X}(\\lambda) =\n\\begin{cases}\n    p & \\text{if } \\lambda = 1, \\\\\n    q = 1-p & \\text{if } \\lambda = 0.\n\\end{cases}\n\\]\n\nThe discrete random variable can take value \\(1\\) with probability \\(p\\) or value \\(0\\) with probability \\(q = 1 - p\\)\nNot to be confused with the binomial distribution, since only one trial is being conducted.\n\\(\\mathbb{E}[X] = p\\)\n\\(Var[X] = pq\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#plot-bernoulli-distribution",
    "href": "lecture2/lecture2-6.html#plot-bernoulli-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Plot: Bernoulli Distribution",
    "text": "Plot: Bernoulli Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#code-bernoulli-distribution",
    "href": "lecture2/lecture2-6.html#code-bernoulli-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Code: Bernoulli Distribution",
    "text": "Code: Bernoulli Distribution\nTo create Bernoulli distributed data:\n# Bernoulli distribution example\nimport numpy as np\nnp.random.binomial(1, p, size=1000)\nTo calculate the PMF:\nfrom scipy.stats import bernoulli\np = 0.5  # Example probability\nx = [0, 1]\npmf = bernoulli.pmf(x, p)\nprint(pmf)\n\n\n# Gaussian Distribution\n\n## Gaussian Distribution PDF\n\n$$\nf_{X}(\\lambda)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2} }}e^{-\\frac{(\\lambda-\\mu)^{2}}{2\\sigma^{2}}}\n$$\n\n- Used frequently to represent real-valued random variables whose distributions are not known.\n- Its importance is derived from the **central limit theorem** that states, under some conditions, the average of many samples of a random variable is itself a random variable that converges to a Gaussian distribution as it increases.\n- $E[X] = \\mu$\n- $Var[X] = \\sigma^{2}$\n\n## Plot: Gaussian Distribution\n\n![](Math/imgs/Gaussian_distribution.pdf)\n\n## Code: Gaussian Distribution\n\nTo create Gaussian distributed data:\n\n```python\n# Gaussian distribution data generation\n# Lines 8-10 from Math/python_files/Gaussian.py\nTo calculate the PDF:\n# Gaussian distribution PDF calculation\n# Lines 11-23 from Math/python_files/Gaussian.py",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#beta-distribution-pdf",
    "href": "lecture2/lecture2-6.html#beta-distribution-pdf",
    "title": "2.6 Probability Distributions",
    "section": "Beta Distribution PDF",
    "text": "Beta Distribution PDF\n\\[\nf_{X}(\\lambda) = {\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}\\,\\lambda^{\\alpha -1}(1-\\lambda)^{\\beta -1}\n\\]\nwhere \\(\\Gamma\\) is the gamma function defined as:\n\\[\n\\Gamma (z)=\\int _{0}^{\\infty}t^{z-1}e^{-t}\\,dt\n\\]\n\nGamma functions are used to model factorial functions of complex numbers \\(z\\).\nBeta functions are used to model behavior of random variables in intervals of finite length.\n\\(E[X] = \\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(Var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#plot-uniform-beta-distribution",
    "href": "lecture2/lecture2-6.html#plot-uniform-beta-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Plot: Uniform Beta Distribution",
    "text": "Plot: Uniform Beta Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#plot-alpha-weighted-beta-distribution",
    "href": "lecture2/lecture2-6.html#plot-alpha-weighted-beta-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Plot: Alpha Weighted Beta Distribution",
    "text": "Plot: Alpha Weighted Beta Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#plot-beta-weighted-beta-distribution",
    "href": "lecture2/lecture2-6.html#plot-beta-weighted-beta-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Plot: Beta Weighted Beta Distribution",
    "text": "Plot: Beta Weighted Beta Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#code-beta-distribution",
    "href": "lecture2/lecture2-6.html#code-beta-distribution",
    "title": "2.6 Probability Distributions",
    "section": "Code: Beta Distribution",
    "text": "Code: Beta Distribution\nTo create Beta distributed data:\n# Beta distribution data generation\n# Lines 9-10 from Math/python_files/Beta.py\nTo calculate the PDF:\n# Beta distribution PDF calculation\n# Lines 12-28 from Math/python_files/Beta.py",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html",
    "href": "homework/homework1.html",
    "title": "Homework 1",
    "section": "",
    "text": "No Homework! Enjoy! üòä",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q1",
    "href": "homework/homework1.html#q1",
    "title": "Homework 1",
    "section": "",
    "text": "\\(\\{ 5x-1: x \\in \\mathbb{Z} \\}\\)\n\\(\\{ x \\in \\mathbb{R}: \\sin \\pi x = 0 \\}\\)\n\\(\\{X : X \\subseteq \\{3,2,a\\} \\text{ and } |X|=2 \\}\\)\n\\(\\{ 2, 4, 8, 16, 32, 64, ...\\}\\)\n\\(\\{0,1,4,9,16,25,36, ...\\}\\)\n\\(\\{..., \\frac{1}{8},\\frac{1}{4},\\frac{1}{2},1,2,4,8,... \\}\\)",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q2",
    "href": "homework/homework1.html#q2",
    "title": "Homework 1",
    "section": "Q2",
    "text": "Q2\nA retail store accepts either American Express or VISA. The percentages of customers carrying each card are: - American Express: 24% - VISA: 61% - Both: 11%\nWhat percentage of customers carry a card accepted by the store?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q3",
    "href": "homework/homework1.html#q3",
    "title": "Homework 1",
    "section": "Q3",
    "text": "Q3\nSixty percent of students wear neither a ring nor a necklace. Given: - 20% wear a ring - 30% wear a necklace\nFind the probability that a randomly chosen student wears:\n\nA ring or a necklace\nBoth a ring and a necklace",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q4",
    "href": "homework/homework1.html#q4",
    "title": "Homework 1",
    "section": "Q4",
    "text": "Q4\nTwo fair dice are rolled. Find the conditional probability that at least one die lands on 6, given that they land on different numbers.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q5",
    "href": "homework/homework1.html#q5",
    "title": "Homework 1",
    "section": "Q5",
    "text": "Q5\nAn urn contains 6 white and 9 black balls. If 4 balls are selected without replacement, what is the probability that the first 2 are white and the last 2 are black?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q6",
    "href": "homework/homework1.html#q6",
    "title": "Homework 1",
    "section": "Q6",
    "text": "Q6\nA defendant is judged guilty if at least 2 out of 3 judges vote guilty. Given: - Probability of a guilty vote when defendant is guilty: 0.7 - Probability of a guilty vote when defendant is innocent: 0.2 - 70% of defendants are guilty\nCompute the conditional probability that judge 3 votes guilty given: 1. Judges 1 and 2 vote guilty. 2. Judges 1 and 2 split votes. 3. Judges 1 and 2 vote not guilty.\nAre the judges‚Äô votes independent? Conditionally independent? Explain.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q7",
    "href": "homework/homework1.html#q7",
    "title": "Homework 1",
    "section": "Q7",
    "text": "Q7\nGiven the distribution function of \\(X\\):\n[ F_{X}() =\n\\[\\begin{cases}\n0, & \\lambda &lt; 0 \\\\\n\\frac{1}{2}, & 0 \\leq \\lambda &lt; 1 \\\\\n\\frac{3}{5}, & 1 \\leq \\lambda &lt; 2 \\\\\n\\frac{4}{5}, & 2 \\leq \\lambda &lt; 3 \\\\\n\\frac{9}{10}, & 3 \\leq \\lambda &lt; 3.5 \\\\\n1, & \\lambda \\geq 3.5 \\\\\n\\end{cases}\\]\n]\nFind \\(p_X(\\lambda)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#q8",
    "href": "homework/homework1.html#q8",
    "title": "Homework 1",
    "section": "Q8",
    "text": "Q8\nA player rolls a fair die and flips a fair coin. If heads, they win twice the die value; if tails, they win half. Determine the expected winnings.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#e1",
    "href": "homework/homework1.html#e1",
    "title": "Homework 1",
    "section": "E1",
    "text": "E1\nThe binomial distribution PMF is:\n[ p_X() = {n k} ^n (1-)^{n-k} ]\nUsing Python, generate binomial data and create visualizations for \\(p_X(\\lambda)\\) and \\(F_X(\\lambda)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-1",
    "href": "homework/homework1.html#question-1",
    "title": "Homework 1",
    "section": "",
    "text": "\\(\\{ 5x-1: x \\in \\mathbb{Z} \\}\\)\n\\(\\{ x \\in \\mathbb{R}: \\sin \\pi x = 0 \\}\\)\n\\(\\{X : X \\subseteq \\{3,2,a\\} \\text{ and } |X|=2 \\}\\)\n\n\n\n\\(\\{ 2, 4, 8, 16, 32, 64, ...\\}\\)\n\\(\\{0,1,4,9,16,25,36, ...\\}\\)\n\\(\\{..., \\frac{1}{8},\\frac{1}{4},\\frac{1}{2},1,2,4,8,... \\}\\)",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-2",
    "href": "homework/homework1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nA retail store accepts either American Express or VISA. The percentages of customers carrying each card are: \n\nAmerican Express: \\(24%\\)\nVISA: \\(61%\\)\nBoth: \\(11%\\)\n\nWhat percentage of customers carry a card accepted by the store?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-3",
    "href": "homework/homework1.html#question-3",
    "title": "Homework 1",
    "section": "Question 3",
    "text": "Question 3\nSixty percent of students wear neither a ring nor a necklace. Given: \n\n\\(20%\\) wear a ring\n\\(30%\\) wear a necklace\n\nFind the probability that a randomly chosen student wears:\n\nA ring or a necklace\nBoth a ring and a necklace",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-4",
    "href": "homework/homework1.html#question-4",
    "title": "Homework 1",
    "section": "Question 4",
    "text": "Question 4\nTwo fair dice are rolled. Find the conditional probability that at least one die lands on \\(6\\), given that they land on different numbers.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-5",
    "href": "homework/homework1.html#question-5",
    "title": "Homework 1",
    "section": "Question 5",
    "text": "Question 5\nAn urn contains \\(6\\) white and \\(9\\) black balls. If \\(4\\) balls are selected without replacement, what is the probability that the first \\(2\\) are white and the last \\(2\\) are black?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-6",
    "href": "homework/homework1.html#question-6",
    "title": "Homework 1",
    "section": "Question 6",
    "text": "Question 6\nA defendant is judged guilty if at least \\(2\\) out of \\(3\\) judges vote guilty. Given:\n\nProbability of a guilty vote when defendant is guilty: \\(0.7\\) \nProbability of a guilty vote when defendant is innocent: \\(0.2\\) \n\\(70%\\) of defendants are guilty\n\nCompute the conditional probability that judge \\(3\\) votes guilty given:\n\nJudges \\(1\\) and \\(2\\) vote guilty. \nJudges \\(1\\) and \\(2\\) split votes. \nJudges \\(1\\) and \\(2\\) vote not guilty.\n\nAre the judges‚Äô votes independent? Conditionally independent? Explain.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-7",
    "href": "homework/homework1.html#question-7",
    "title": "Homework 1",
    "section": "Question 7",
    "text": "Question 7\nGiven the distribution function of \\(X\\):\n\\[\nF_{X}(\\lambda) =\n\\begin{cases}\n0, & \\lambda &lt; 0 \\\\\n\\frac{1}{2}, & 0 \\leq \\lambda &lt; 1 \\\\\n\\frac{3}{5}, & 1 \\leq \\lambda &lt; 2 \\\\\n\\frac{4}{5}, & 2 \\leq \\lambda &lt; 3 \\\\\n\\frac{9}{10}, & 3 \\leq \\lambda &lt; 3.5 \\\\\n1, & \\lambda \\geq 3.5 \\\\\n\\end{cases}\n\\]\nFind \\(p_X(\\lambda)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#question-8",
    "href": "homework/homework1.html#question-8",
    "title": "Homework 1",
    "section": "Question 8",
    "text": "Question 8\nA player rolls a fair die and flips a fair coin. If heads, they win twice the die value; if tails, they win half. Determine the expected winnings.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "homework/homework1.html#coding-exercise-1",
    "href": "homework/homework1.html#coding-exercise-1",
    "title": "Homework 1",
    "section": "Coding Exercise 1",
    "text": "Coding Exercise 1\nThe binomial distribution PMF is:\n\\[\np_X(\\lambda) = {n \\choose k} \\lambda^n (1-\\lambda)^{n-k}\n\\]\nUsing Python, generate binomial data and create visualizations for \\(p_X(\\lambda)\\) and \\(F_X(\\lambda)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#exercise",
    "href": "lecture2/lecture2-1.html#exercise",
    "title": "2.1 Set Theory",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is (2 + 2)?\n\n\nWrite your answer below:",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#solution",
    "href": "lecture2/lecture2-1.html#solution",
    "title": "2.1 Set Theory",
    "section": "Solution",
    "text": "Solution\n\n\nClick to reveal the solution\n\nThe answer is 4.",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#methodology",
    "href": "lecture2/lecture2-2.html#methodology",
    "title": "2.2 Axiomatic Probability",
    "section": "",
    "text": "Specify sample space.\nDefine probability law (must align with probability axioms).\nIdentify event of interest.\nCalculate‚Ä¶",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#sample-space",
    "href": "lecture2/lecture2-2.html#sample-space",
    "title": "2.2 Axiomatic Probability",
    "section": "Sample Space",
    "text": "Sample Space\nA sample space is a set of all possible outcomes from an experiment. \n\\[\nSample \\ Space = \\{Heads, Tails\\}\n\\]\nAn experiment is any procedure that can be repeated and has a well-defined set of outcomes. \n\\[\nFlipping \\ a \\ fair \\ coin\n\\]\nAn outcome is the end result of an experiment, or an element in the sample space.\n\\[\nHeads  \n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#illustration-discrete-sample-space",
    "href": "lecture2/lecture2-2.html#illustration-discrete-sample-space",
    "title": "2.2 Axiomatic Probability",
    "section": "Illustration: Discrete Sample Space",
    "text": "Illustration: Discrete Sample Space\nExperiment: Rolling two fair die at the same time.\n\\[\nSample \\ Space = \\{ (x, y) : x,y \\in \\mathbb{N}, 1 \\leq x, y \\leq 6  \\}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#illustration-continuous-sample-space",
    "href": "lecture2/lecture2-2.html#illustration-continuous-sample-space",
    "title": "2.2 Axiomatic Probability",
    "section": "Illustration: Continuous Sample Space",
    "text": "Illustration: Continuous Sample Space\nExperiment: Measure two continuous variables in the range \\([0,1]\\)\n\\[ Sample\\ Space = \\{ (x, y) : x,y \\in \\mathbb{R}, 0 \\leq x, y \\leq 1  \\} \\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#events-and-experiment",
    "href": "lecture2/lecture2-2.html#events-and-experiment",
    "title": "2.2 Axiomatic Probability",
    "section": "Events and Experiment",
    "text": "Events and Experiment\nAn event is a subset of the sample space.  Events are important because they ultimately get assigned probabilities.\nExperiment: Rolling a die once.\n\\[\nSample \\ Space = \\{1,2,3,4,5,6\\}\n\\]\nWhat is the event of rolling a \\(1\\)?\n\\[\n\\{1\\} \\subseteq \\{1,2,3,4,5,6\\}\n\\]\nWhat is the event of rolling an odd number?\n\\[\n\\{1, 3, 5\\} \\subseteq \\{1,2,3,4,5,6\\}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#probability-axioms-and-probability-law",
    "href": "lecture2/lecture2-2.html#probability-axioms-and-probability-law",
    "title": "2.2 Axiomatic Probability",
    "section": "Probability Axioms and Probability Law",
    "text": "Probability Axioms and Probability Law\nKolmogorov probability axioms are the foundations of axiomatic probability theory:\n\nNonnegativity: \\(P(Event) \\geq 0\\)\nNormalization: \\(P(Sample\\ Space) = 1\\)\nAdditivity: If \\(A \\cap B = \\emptyset, P(A \\cup B) = P(A) + P(B)\\)\n\nProbability laws are additional axioms mathematically derived from Kolmogorov probability axioms.\n\nExercise\nExperiment: Rolling two fair die at the same time.\nLet all outcomes be equally likely.\n\\[\nP(A) = \\frac{|A|}{|Sample\\ Space|}\n\\]\n\n\n\nFind the following probabilities:\n\n\\(P(die_1 = 1)\\)\n\\(P(max(die_1, die_2) = 2)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(P(die_1 = 1) = \\frac{6}{36} \\approx 0.1\\bar{6}\\)\n\\(P(max(die_1, die_2) = 2) = \\frac{2}{36} \\approx 0.0\\bar{5}\\)\n\n\n\n\n\n\nExercise\nExperiment: Measure two continuous variables in the range \\([0,1]\\)\n\\[\nSample\\ Space = \\{ (x, y) : x,y \\in \\mathbb{R}, 0 \\leq x, y \\leq 1  \\}\n\\]\n\n\n\nFind the following probabilities:\n\n\\(P(x = 0.5 , y = 0.5)\\)\n\\(P(x+y \\geq 1)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(P(x = 0.5 , y = 0.5) = 0\\)\n\\(P(x+y \\geq 1) = 0.5\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#example-discrete-uniform-law",
    "href": "lecture2/lecture2-2.html#example-discrete-uniform-law",
    "title": "2.2 Axiomatic Probability",
    "section": "Example: Discrete Uniform Law",
    "text": "Example: Discrete Uniform Law\nExperiment: Rolling two fair die at the same time\n\nLet all outcomes be equally likely\n\n\\[\nP(A) = \\frac{|A|}{|Sample\\ Space|}\n\\]\n\n\n\nFind the following probabilities:\n\n\\(P(die_1 = 1)\\)\n\\(P(Max(die_1, die_2) = 2)\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#example-continuous-uniform-law",
    "href": "lecture2/lecture2-2.html#example-continuous-uniform-law",
    "title": "2.2 Axiomatic Probability",
    "section": "Example: Continuous Uniform Law",
    "text": "Example: Continuous Uniform Law\nExperiment: Measure two continuous variables in the range \\([0,1]\\)\n\\[\nSample\\ Space = \\{ (x, y) : x,y \\in \\mathbb{R}, 0 \\leq x, y \\leq 1  \\}\n\\]\n\n\n\nFind the following probabilities:\n\n\\(P((x,y) = (0.5 , 0.5))\\)\n\\(P(x+y \\geq 0.5)\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#independence",
    "href": "lecture2/lecture2-4.html#independence",
    "title": "2.4 Independence",
    "section": "",
    "text": "Exercise\nWhich of the Venn diagrams shows 2 independent events?\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#definition",
    "href": "lecture2/lecture2-4.html#definition",
    "title": "2.4 Independence",
    "section": "",
    "text": "Exercise\nWhich of the Venn diagrams shows 2 independent events?\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#multiplication-rule",
    "href": "lecture2/lecture2-3.html#multiplication-rule",
    "title": "2.3 Conditioning",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nThe Multiplication Rule states that the probability of two events \\(A\\) and \\(B\\) occurring together (\\(A \\cap B\\)) is given by the probability of one event occurring given the other (\\(P(A|B)\\) or \\(P(B|A)\\)) multiplied by the probability of the other event.\n\\[\nP(A \\cap B) = P(A|B) P(B) = P(B|A) P(A)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#product-rule",
    "href": "lecture2/lecture2-3.html#product-rule",
    "title": "2.3 Conditioning",
    "section": "Product Rule",
    "text": "Product Rule\nSuppose \\(A\\) and \\(B\\) are events.  The product rule states that the probability of two events \\(A\\) and \\(B\\) occurring together \\(A \\cap B\\) is given by the probability of one event occurring given the other \\(P(A|B)\\) or \\(P(B|A)\\) multiplied by the probability of the other event.\n\\[\nP(A \\cap B) = P(A|B) P(B) = P(B|A) P(A)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#probability-mass-function",
    "href": "lecture2/lecture2-5.html#probability-mass-function",
    "title": "2.5 Discrete Random Variables",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nA probability mass function (PMF) is a mapping of values \\(x\\) of discrete random variables \\(X\\) to probabilities \\([0,1]\\).\n\\[\np_{X}(x) = P(X = x)\n\\]\nPMF has properties: \\[\np_{X}(x) \\geq 0, \\quad \\sum_{x} p_{X}(x) = 1\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#cumulative-distribution-function",
    "href": "lecture2/lecture2-5.html#cumulative-distribution-function",
    "title": "2.5 Discrete Random Variables",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nThe cumulative distribution function (CDF) is defined as:\n\\[\nF_{X}(x) = P(X \\leq x) = \\sum_{k \\leq x} p_{X}(k)\n\\]\nRelation to PMF: \\[\np_{X}(x) = \\frac{dF_{X}(x)}{dx}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#definition",
    "href": "lecture2/lecture2-6.html#definition",
    "title": "2.6 Continuous Random Variables",
    "section": "Definition",
    "text": "Definition\nA continuous random variable is a mapping \\(X\\) of all the events of a sample space to numerical values \\(\\lambda\\) in \\(\\mathbb{R}\\).\n\nNotation\n\\[ X: event \\in Sample \\ Space \\to \\lambda \\in \\mathbb{R} \\]\n\n\nProbability Density Function (PDF)\nA probability density function (PDF) is a mapping of values \\(\\lambda\\) of intervals of continuous random variables \\(X\\) to probabilities \\([0,1]\\).\n\nNotation\n\\[ P(a \\leq X \\leq b) = \\int_{a}^{b} f_{X}(\\lambda) \\ d\\lambda \\]\n\n\nProperties\n\n$ f_{X}() $\n$ {-}^{} f{X}() ¬†d= 1 $\n\n\n\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for continuous random variables is defined as:\n\\[ F_{X}(\\lambda) = P(X \\leq \\lambda) = \\int_{-\\infty}^{\\lambda} f_{X}(u) \\ du \\]\n\nRelationship between CDF and PDF:\n\n$ f_{X}() = $\n$ F_{X}() = {-}^{} f{X}(u) ¬†du $\n\n\n\n\nExpectation and Variance\n\nExpectation (Mean)\n\\[ E[X] = \\int_{-\\infty}^{\\infty} \\lambda \\ f_{X}(\\lambda) d\\lambda \\]\n\n\nVariance\n\\[ \\text{Var}[X] = E[(X - E[X])^{2}] = \\int_{-\\infty}^{\\infty} (\\lambda - E[X])^2 \\ f_{X}(\\lambda) d\\lambda \\]\n\n\n\nJoint Probability Density Function (Joint PDF)\nFor two continuous random variables: \\[ P((X,Y) \\in A) = \\int \\int_{A} f_{X,Y}(\\lambda_{1}, \\lambda_{2}) d\\lambda_{1} \\ d\\lambda_{2} \\]\n\nMarginal PDFs\n\n$ f_{X}({1}) = {-}^{} f_{X,Y}({1}, {2}) d_{2} $\n$ f_{Y}({2}) = {-}^{} f_{X,Y}({1},{2}) d_{1} $\n\n\n\n\nConditional PDFs\nThe conditional probability density function: \\[ f_{X|Y}(\\lambda_{1}| \\lambda_{2}) = \\frac{f_{X,Y}(\\lambda_{1}, \\lambda_{2})}{f_{Y}(\\lambda_{2})} \\]\n\nConditional Expectation\n\\[ \\mathbb{E}[X| Y = \\lambda_{2}] = \\int_{\\lambda_{1} \\in X} \\lambda_{1} \\frac{f_{X,Y}(\\lambda_{1}, \\lambda_{2})}{f_{Y}(\\lambda_{2})} d\\lambda_{1} \\]\n\n\n\nIndependence\nTwo continuous random variables are independent if: \\[ f_{X,Y}(\\lambda_{1}, \\lambda_{2}) = f_{X}(\\lambda_{1}) f_{Y}(\\lambda_{2}) \\]\nThis can be extended to multiple random variables: \\[ f_{X_{1},...,X{n}}(\\lambda_{1}, ..., \\lambda_{n}) = f_{X_{1}}(\\lambda_{1}) ... \\ f_{X_{n}}(\\lambda_{n}) \\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html",
    "href": "lecture2/lecture2-7.html",
    "title": "2.7 Probability Distributions",
    "section": "",
    "text": "Bernoulli Distribution\n\\[\np_{X}(x) =\n\\begin{cases}\n    p & \\text{if } x = 1, \\\\\n    q = 1-p & \\text{if } x = 0.\n\\end{cases}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#bernoulli-distribution",
    "href": "lecture2/lecture2-7.html#bernoulli-distribution",
    "title": "2.7 Probability Distributions",
    "section": "",
    "text": "The discrete random variable can take value \\(1\\) with probability \\(p\\) or value \\(0\\) with probability \\(q = 1 - p\\)\nNot to be confused with the binomial distribution, since only one trial is being conducted.\n\\(\\mathbb{E}[X] = p\\)\n\\(Var[X] = pq\\) \n\n\n\nviewof p = Inputs.range([0, 1], {\n  step: 0.01,\n  value: 0.5,\n  label: tex`p`,\n  width: 200\n})\n\ndata = {\n  return [\n    {outcome: \"0\", probability: 1 - p},\n    {outcome: \"1\", probability: p}\n  ];\n}\n\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {\n    grid: true,\n    label: \"Probability\",\n    domain: [0, 1]\n  },\n  x: {\n    label: \"Outcome\",\n    padding: 0.2\n  },\n  marks: [\n    Plot.barY(data, {\n      x: \"outcome\",\n      y: \"probability\",\n      fill: \"steelblue\"\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  &lt;p&gt;${tex`\\mathbb{E}[X] =`} ${p.toFixed(3)}&lt;/p&gt;\n  &lt;p&gt;${tex`\\text{Var}(X) =`} ${(p * (1-p)).toFixed(3)}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nPython Code: Bernoulli Distribution\nTo create Bernoulli distributed data using numpy:\nimport numpy as np\n\ninterval = [0,1]\nsize = (1000,1)\np = [1-0.5, 0.5]\n\ndata = np.random.choice(interval, size, p = p)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#plot-bernoulli-distribution",
    "href": "lecture2/lecture2-7.html#plot-bernoulli-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Plot: Bernoulli Distribution",
    "text": "Plot: Bernoulli Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#code-bernoulli-distribution",
    "href": "lecture2/lecture2-7.html#code-bernoulli-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Code: Bernoulli Distribution",
    "text": "Code: Bernoulli Distribution\nTo create Bernoulli distributed data:\n# Bernoulli distribution example\nimport numpy as np\nnp.random.binomial(1, p, size=1000)\nTo calculate the PMF:\nfrom scipy.stats import bernoulli\np = 0.5  # Example probability\nx = [0, 1]\npmf = bernoulli.pmf(x, p)\nprint(pmf)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#beta-distribution-pdf",
    "href": "lecture2/lecture2-7.html#beta-distribution-pdf",
    "title": "2.7 Probability Distributions",
    "section": "Beta Distribution PDF",
    "text": "Beta Distribution PDF\n\\[\nf_{X}(\\lambda) = {\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}\\,\\lambda^{\\alpha -1}(1-\\lambda)^{\\beta -1}\n\\]\nwhere \\(\\Gamma\\) is the gamma function defined as:\n\\[\n\\Gamma (z)=\\int _{0}^{\\infty}t^{z-1}e^{-t}\\,dt\n\\]\n\nGamma functions are used to model factorial functions of complex numbers \\(z\\).\nBeta functions are used to model behavior of random variables in intervals of finite length.\n\\(E[X] = \\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(Var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#plot-uniform-beta-distribution",
    "href": "lecture2/lecture2-7.html#plot-uniform-beta-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Plot: Uniform Beta Distribution",
    "text": "Plot: Uniform Beta Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#plot-alpha-weighted-beta-distribution",
    "href": "lecture2/lecture2-7.html#plot-alpha-weighted-beta-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Plot: Alpha Weighted Beta Distribution",
    "text": "Plot: Alpha Weighted Beta Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#plot-beta-weighted-beta-distribution",
    "href": "lecture2/lecture2-7.html#plot-beta-weighted-beta-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Plot: Beta Weighted Beta Distribution",
    "text": "Plot: Beta Weighted Beta Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#code-beta-distribution",
    "href": "lecture2/lecture2-7.html#code-beta-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Code: Beta Distribution",
    "text": "Code: Beta Distribution\nTo create Beta distributed data:\n# Beta distribution data generation\n# Lines 9-10 from Math/python_files/Beta.py\nTo calculate the PDF:\n# Beta distribution PDF calculation\n# Lines 12-28 from Math/python_files/Beta.py",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#gaussian-distribution",
    "href": "lecture2/lecture2-7.html#gaussian-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Gaussian Distribution",
    "text": "Gaussian Distribution\n\\[\nf_{X}(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2} }}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\n\\]\n\nUsed frequently to represent real-valued random variables whose distributions are not known.\nIts importance is derived from the central limit theorem that states, under some conditions, the average of many samples of a random variable is itself a random variable that converges to a Gaussian distribution as it increases.\n\\(E[X] = \\mu\\)\n\\(Var[X] = \\sigma^{2}\\)\n\n\n\nviewof mu = Inputs.range([-1, 1], {\n  step: 0.1,\n  value: 0,\n  label: tex`\\mu`,\n  width: 200\n})\n\nviewof sigma = Inputs.range([0.1, 2], {\n  step: 0.1,\n  value: 1,\n  label: tex`\\sigma`,\n  width: 200\n})\n\n// Generate points for the normal distribution curve\npointsGaussian = {\n  const x = d3.range(-5, 5, 0.1);\n  return x.map(x =&gt; ({\n    x,\n    y: (1 / (sigma * Math.sqrt(2 * Math.PI))) * \n       Math.exp(-0.5 * Math.pow((x - mu) / sigma, 2))\n  }));\n}\n\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {\n    grid: true,\n    label: \"Density\"\n  },\n  x: {\n    label: \"x\",\n    domain: [-5, 5]\n  },\n  marks: [\n    Plot.line(pointsGaussian, {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  &lt;p&gt;${tex`\\mathbb{E}[X] =`} ${mu.toFixed(3)}&lt;/p&gt;\n  &lt;p&gt;${tex`\\text{Var}(X) =`} ${(sigma * sigma).toFixed(3)}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nPython Code: Gaussian Distribution\nTo create Gaussian distributed data using numpy:\nimport numpy as np\n\nmu = 0\nsigma = 1\nsize = (1000,1)\n\ndata = np.random.normal(mu, sigma, size)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#plot-gaussian-distribution",
    "href": "lecture2/lecture2-7.html#plot-gaussian-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Plot: Gaussian Distribution",
    "text": "Plot: Gaussian Distribution",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#code-gaussian-distribution",
    "href": "lecture2/lecture2-7.html#code-gaussian-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Code: Gaussian Distribution",
    "text": "Code: Gaussian Distribution\nTo create Gaussian distributed data:\n# Gaussian distribution data generation\n# Lines 8-10 from Math/python_files/Gaussian.py\nTo calculate the PDF:\n# Gaussian distribution PDF calculation\n# Lines 11-23 from Math/python_files/Gaussian.py",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#beta-distribution",
    "href": "lecture2/lecture2-7.html#beta-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\\[\nf_{X}(x) = {\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}\\,x^{\\alpha -1}(1-x)^{\\beta -1}\n\\]\nwhere \\(\\Gamma\\) is the gamma function defined as:\n\\[\n\\Gamma (z)=\\int _{0}^{\\infty}t^{z-1}e^{-t}\\,dt\n\\]\n\nGamma functions are used to model factorial functions of complex numbers \\(z\\).\nBeta functions are used to model behavior of random variables in intervals of finite length.\n\\(E[X] = \\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(Var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\n\n\n\nviewof alpha = Inputs.range([0.1, 10], {\n  step: 0.1,\n  value: 1,\n  label: tex`\\alpha`,\n  width: 200\n})\n\nviewof beta = Inputs.range([0.1, 10], {\n  step: 0.1,\n  value: 1,\n  label: tex`\\beta`,\n  width: 200\n})\n\n// Gamma function approximation using Lanczos approximation\nfunction gamma(z) {\n    const p = [676.5203681218851, -1259.1392167224028, 771.32342877765313,\n        -176.61502916214059, 12.507343278686905, -0.13857109526572012,\n        9.9843695780195716e-6, 1.5056327351493116e-7];\n    \n    if (z &lt; 0.5) {\n        return Math.PI / (Math.sin(Math.PI * z) * gamma(1 - z));\n    }\n    \n    z -= 1;\n    let x = 0.99999999999980993;\n    for (let i = 0; i &lt; p.length; i++) {\n        x += p[i] / (z + i + 1);\n    }\n    \n    const t = z + p.length - 0.5;\n    return Math.sqrt(2 * Math.PI) * Math.pow(t, z + 0.5) * Math.exp(-t) * x;\n}\n\n// Beta function using gamma function\nfunction betaFunc(x, y) {\n    return (gamma(x) * gamma(y)) / gamma(x + y);\n}\n\n// Beta probability density function\nfunction betaPDF(x, a, b) {\n    if (x &lt;= 0 || x &gt;= 1) return 0;\n    return Math.pow(x, a - 1) * Math.pow(1 - x, b - 1) / betaFunc(a, b);\n}\n\n// Generate points for the beta distribution curve\npoints = Array.from({length: 100}, (_, i) =&gt; {\n  let x = 0.001 + i * 0.01;\n  return { x, y: betaPDF(x, alpha, beta) };\n});\n\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {\n    grid: true,\n    label: \"Density\"\n  },\n  x: {\n    label: \"x\",\n    domain: [0, 1]\n  },\n  marks: [\n    Plot.line(points, {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  &lt;p&gt;${tex`\\mathbb{E}[X] =`} ${(alpha/(alpha + beta)).toFixed(3)}&lt;/p&gt;\n  &lt;p&gt;${tex`\\text{Var}(X) =`} ${((alpha * beta)/((alpha + beta)**2 * (alpha + beta + 1))).toFixed(3)}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nPython Code: Beta Distribution\nTo create Beta distributed data using numpy:\nimport numpy as np \n\nalpha = 0.5\nbeta = 0.5\nsize = (1000,1)\n\ndata = np.random.beta(alpha, beta, size)",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#illustration-pmf-and-cdf",
    "href": "lecture2/lecture2-5.html#illustration-pmf-and-cdf",
    "title": "2.5 Discrete Random Variables",
    "section": "Illustration: PMF and CDF",
    "text": "Illustration: PMF and CDF",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#expectation-and-variance-of-pmf",
    "href": "lecture2/lecture2-5.html#expectation-and-variance-of-pmf",
    "title": "2.5 Discrete Random Variables",
    "section": "Expectation and Variance of PMF",
    "text": "Expectation and Variance of PMF\nThe expectation of a discrete random variable is the sum of all possible values weighted by their probabilities, representing the long-term average outcome.\n\\[\nE[X] = \\sum_{x} x \\ p_{X}(x)\n\\]\nThe variance of a discrete random variable quantifies the spread of its values around the mean by calculating the expected squared deviation from the mean. \\[\nVar[X] = E[(X - E[X])^{2}] = \\sum_{x}(x - E[X])^2 p_{X}(x)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#joint-and-marginal-pmfs",
    "href": "lecture2/lecture2-5.html#joint-and-marginal-pmfs",
    "title": "2.5 Discrete Random Variables",
    "section": "Joint and Marginal PMFs",
    "text": "Joint and Marginal PMFs\nThe joint PMF calculates the intersection of two discrete random variables: \\[\np_{X,Y}(x_{1},x_{2}) = P(X = x_{1}, Y = x_{2})\n\\]\nFrom the previous definition we can also compute the marginal PMF for a particular random variable: \\[\np_{X}(x_{1}) = \\sum_{x_{2}} p_{X,Y}(X = x_{1},Y = x_{2})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#conditional-expectation-of-pmf",
    "href": "lecture2/lecture2-5.html#conditional-expectation-of-pmf",
    "title": "2.5 Discrete Random Variables",
    "section": "Conditional Expectation of PMF",
    "text": "Conditional Expectation of PMF\nThe conditional PMF gives the probability distribution of a discrete random variable given that another variable has a specific value. \\[\np_{X|Y}(x_{1}| x_{2}) = \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]\nThe conditional expectation is the expected value of a discrete random variable given that another variable is fixed at a specific value. \\[\nE[X| Y = x_{2}] = \\sum_{x_{1}} x_{1} \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#independence-of-pmf",
    "href": "lecture2/lecture2-5.html#independence-of-pmf",
    "title": "2.5 Discrete Random Variables",
    "section": "Independence of PMF",
    "text": "Independence of PMF\nTwo discrete random variables are independent if:\n\\[\np_{X,Y}(x_{1}, x_{2}) = p_{X}(x_{1}) p_{Y}(x_{2})\n\\]\nFor multiple discrete random variables: \\[\np_{X_{1},...,X_{n}}(x_{1}, ..., x_{n}) = p_{X_{1}}(x_{1}) ... \\ p_{X_{n}}(x_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#probability-density-function-pdf",
    "href": "lecture2/lecture2-6.html#probability-density-function-pdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nA probability density function (PDF) is a mapping of values \\(x\\) of intervals of continuous random variables \\(X\\) to probabilities \\([0,1]\\).\n\\[\nP(a \\leq X \\leq b) = \\int_{a}^{b} f_{X}(x) \\ dx\n\\]\nPDF has properties: \\[\nf_{X}(x) \\geq 0, \\ \\int_{-\\infty}^{\\infty} f_{X}(x) \\ dx = 1\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#cumulative-distribution-function-cdf",
    "href": "lecture2/lecture2-6.html#cumulative-distribution-function-cdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for continuous random variables is defined as:\n\\[ F_{X}(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_{X}(u) \\ du \\]\nRelation to PDF:\n\\[\nf_{X}(x) = \\frac{dF_{X}(x)}{dx}\n\\] \\[\nF_{X}(x) = \\int_{-\\infty}^{x} f_{X}(u) \\ du\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#relationship-between-cdf-and-pdf",
    "href": "lecture2/lecture2-6.html#relationship-between-cdf-and-pdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Relationship between CDF and PDF:",
    "text": "Relationship between CDF and PDF:\n\\(f_{X}(x) = \\frac{dF_{X}(x)}{dx}\\) \\(F_{X}(x) = \\int_{-\\infty}^{x} f_{X}(u) \\ du\\)\n\nExpectation and Variance\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\ f_{X}(x) dx\n\\]\n\\[\n\\text{Var}[X] = E[(X - E[X])^{2}] = \\int_{-\\infty}^{\\infty} (x - E[X])^2 \\ f_{X}(x) dx\n\\]\n\n\nJoint Probability Density Function (Joint PDF)\nFor two continuous random variables: \\[ P((X,Y) \\in A) = \\int \\int_{A} f_{X,Y}(x_{1}, x_{2}) dx_{1} \\ dx_{2} \\]\n\nMarginal PDFs\n\n$ f_{X}(x_{1}) = {-}^{} f{X,Y}(x_{1}, x_{2}) dx_{2} $\n$ f_{Y}(x_{2}) = {-}^{} f{X,Y}(x_{1},x_{2}) dx_{1} $\n\n\n\n\nConditional PDFs\nThe conditional probability density function: \\[ f_{X|Y}(x_{1}| x_{2}) = \\frac{f_{X,Y}(x_{1}, x_{2})}{f_{Y}(x_{2})} \\]\n\nConditional Expectation\n\\[ \\mathbb{E}[X| Y = x_{2}] = \\int_{x_{1} \\in X} x_{1} \\frac{f_{X,Y}(x_{1}, x_{2})}{f_{Y}(x_{2})} dx_{1} \\]\n\n\n\nIndependence\nTwo continuous random variables are independent if: \\[ f_{X,Y}(x_{1}, x_{2}) = f_{X}(x_{1}) f_{Y}(x_{2}) \\]\nThis can be extended to multiple random variables: \\[ f_{X_{1},...,X{n}}(x_{1}, ..., x_{n}) = f_{X_{1}}(x_{1}) ... \\ f_{X_{n}}(x_{n}) \\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#joint-and-marginal",
    "href": "lecture2/lecture2-5.html#joint-and-marginal",
    "title": "2.5 Discrete Random Variables",
    "section": "Joint and Marginal",
    "text": "Joint and Marginal\nThe joint PMF gives the probability that two discrete random variables take on specific values simultaneously. \\[\np_{X,Y}(x_{1},x_{2}) = P(X = x_{1}, Y = x_{2})\n\\]\nThe marginal PMF of a discrete random variable is found by summing the joint PMF over all possible values of the other variable, giving the probability of a single variable. \\[\np_{X}(x_{1}) = \\sum_{x_{2}} p_{X,Y}(X = x_{1},Y = x_{2})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#expectation-and-variance-of-pmfs",
    "href": "lecture2/lecture2-5.html#expectation-and-variance-of-pmfs",
    "title": "2.5 Discrete Random Variables",
    "section": "Expectation and Variance of PMFs",
    "text": "Expectation and Variance of PMFs\nDiscrete expectation, or mean, is the average numerical value that the discrete random variable takes over the PMF. \\[\nE[X] = \\sum_{x} x \\ p_{X}(x)\n\\]\nDiscrete variance is the expected squared difference from the mean of a PMF. \\[\nVar[X] = E[(X - E[X])^{2}] = \\sum_{x}(x - E[X])^2 p_{X}(x)\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#conditional-expectation-of-pmfs",
    "href": "lecture2/lecture2-5.html#conditional-expectation-of-pmfs",
    "title": "2.5 Discrete Random Variables",
    "section": "Conditional Expectation of PMFs",
    "text": "Conditional Expectation of PMFs\nThe conditional PMF gives the probability distribution of a discrete random variable given that another variable has a specific value. \\[\np_{X|Y}(x_{1}| x_{2}) = \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]\nThe conditional expectation is the expected value of a discrete random variable given that another variable is fixed at a specific value. \\[\nE[X| Y = x_{2}] = \\sum_{x_{1}} x_{1} \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#expectation-and-variance-of-pdfs",
    "href": "lecture2/lecture2-6.html#expectation-and-variance-of-pdfs",
    "title": "2.6 Continuous Random Variables",
    "section": "Expectation and Variance of PDFs",
    "text": "Expectation and Variance of PDFs\nContinuous expectation, or mean, is the average numerical value that the continuous random variable takes over the PDF. \\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\ f_{X}(x) dx\n\\]\nContinuous variance is the expected squared difference from the mean of a PDF. \\[\n\\text{Var}[X] = E[(X - E[X])^{2}] = \\int_{-\\infty}^{\\infty} (x - E[X])^2 \\ f_{X}(x) dx\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#joint-and-marginal-pdfs",
    "href": "lecture2/lecture2-6.html#joint-and-marginal-pdfs",
    "title": "2.6 Continuous Random Variables",
    "section": "Joint and Marginal PDFs",
    "text": "Joint and Marginal PDFs\nThe joint PDF calculates the intersection of two continuous random variables: \\[\nP((X,Y) \\in A) = \\int \\int_{A} f_{X,Y}(x_{1}, x_{2}) dx_{1} \\ dx_{2}\n\\]\nFrom the previous definition we can also compute the marginal PDF for a particular random variable:\n\\[\nf_{X}(x_{1}) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x_{1}, x_{2}) dx_{2}\n\\]\n\\[\nf_{Y}(x_{2}) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x_{1},x_{2}) dx_{1}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#conditional-expectation-of-pdfs",
    "href": "lecture2/lecture2-6.html#conditional-expectation-of-pdfs",
    "title": "2.6 Continuous Random Variables",
    "section": "Conditional Expectation of PDFs",
    "text": "Conditional Expectation of PDFs\nThe conditional PDF gives the probability distribution of a continuous random variable given that another variable has a specific value. \\[\nf_{X|Y}(x_{1}| x_{2}) = \\frac{f_{X,Y}(x_{1}, x_{2})}{f_{Y}(x_{2})}\n\\]\nThe conditional expectation is the expected value of a continuous random variable given that another variable is fixed at a specific value. \\[\n\\mathbb{E}[X| Y = x_{2}] = \\int_{x_{1} \\in X} x_{1} \\frac{f_{X,Y}(x_{1}, x_{2})}{f_{Y}(x_{2})} dx_{1}\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#independence",
    "href": "lecture2/lecture2-6.html#independence",
    "title": "2.6 Continuous Random Variables",
    "section": "Independence",
    "text": "Independence\nTwo continuous random variables are independent if: \\[\nf_{X,Y}(x_{1}, x_{2}) = f_{X}(x_{1}) f_{Y}(x_{2})\n\\]\nThis can be extended to multiple random variables: \\[\nf_{X_{1},...,X{n}}(x_{1}, ..., x_{n}) = f_{X_{1}}(x_{1}) ... \\ f_{X_{n}}(x_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#illustration-pdf-and-cdf",
    "href": "lecture2/lecture2-6.html#illustration-pdf-and-cdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Illustration: PDF and CDF",
    "text": "Illustration: PDF and CDF",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture3/lo.html",
    "href": "lecture3/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 3: Multi-Armed Bandits üéØ\n\n\n\n\nMulti-Amed Bandits Framework.\n\\(\\epsilon\\)-greedy.\nUpper Confidence Boundary (UCB).\nThompson Sampling.\nBernoulli & Gaussian generated environment using numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html",
    "href": "homework/homework2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\nWrite some of the elements of the following sets:\nWrite the following sets in set notation:",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-1",
    "href": "homework/homework2.html#question-1",
    "title": "Homework 2",
    "section": "",
    "text": "\\(\\{ 5x-1: x \\in \\mathbb{Z} \\}\\)\n\\(\\{ x \\in \\mathbb{R}: \\sin \\pi x = 0 \\}\\)\n\\(\\{X : X \\subseteq \\{3,2,a\\} \\text{ and } |X|=2 \\}\\)\n\n\n\n\\(\\{ 2, 4, 8, 16, 32, 64, ...\\}\\)\n\\(\\{0,1,4,9,16,25,36, ...\\}\\)\n\\(\\{..., \\frac{1}{8},\\frac{1}{4},\\frac{1}{2},1,2,4,8,... \\}\\)",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-1",
    "href": "homework/homework2.html#coding-exercise-1",
    "title": "Homework 2",
    "section": "Coding Exercise 1",
    "text": "Coding Exercise 1\nThe binomial distribution PMF is:\n\\[\np_X(\\lambda) = {n \\choose k} \\lambda^n (1-\\lambda)^{n-k}\n\\]\nUsing Python, generate binomial data and create visualizations for \\(p_X(\\lambda)\\) and \\(F_X(\\lambda)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html",
    "href": "lecture3/lecture3-1.html",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "",
    "text": "Bandit\nA bandit is a slot machine.\nIt is used as an analogy to represent the action an agent can make in one state.\nEach action selection is like a play of one of the slot machine‚Äôs levers, and the rewards are the payoffs for hitting the jackpot, according to its underlying probability distribution.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#bandit",
    "href": "lecture3/lecture3-1.html#bandit",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "",
    "text": "\\(\\Huge{\\to}\\)\nRewards",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#multi-armed-bandit",
    "href": "lecture3/lecture3-1.html#multi-armed-bandit",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Multi-Armed Bandit",
    "text": "Multi-Armed Bandit\n\n\n\n\n\n\nNonassociative Environments\n\n\n\nA nonassociative environment is a setting that involves learning how to act in one state. The best example of a nonassociative environment is Multi-Armed Bandit‚Äôs.\n\n\nA Multi-Armed Bandit can be interpreted as k-actions, or k-arms of the slot machines, to decide from.  Through repeated action selections, you maximize your winnings by concentrating actions on the best levers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we decide the most appropriate action? ü§î",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#expectation-of-a-bandit",
    "href": "lecture3/lecture3-1.html#expectation-of-a-bandit",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Expectation of a Bandit",
    "text": "Expectation of a Bandit\nEach bandit has an expected reward given a particular action is selected, called the action value.\n\\[\nQ_t(a) = \\mathbb{E}[R_t | A_t = a]\n\\]\nWhere:\n\n\\(Q_t(a)\\) is the conditional expectation of the rewards \\(R_t\\) given the selection of an action \\(A_t\\).\n\\(R_t\\) is the random variable for the reward at time step \\(t\\).\n\\(A_t\\) is the random variable for the action selected at time step \\(t\\).",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#action-value-method",
    "href": "lecture3/lecture3-1.html#action-value-method",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Action Value Method",
    "text": "Action Value Method\nTo compute expectations of action values and select actions, we use action value methods.\n\\[\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i * \\mathbf{1}_{A_i = a}}{\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a}}\n\\]\nWhere:\n\n\\(Q_t(a)\\) is the action value for a particular action \\(a\\).\n\\(\\mathbf{1}\\) is a predicate, which denotes whether \\(A_i = a\\) is true or false.\n\nIf the denominator is \\(0\\), then we denote \\(Q_t(a)\\) as \\(0\\).",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#action-value-method-update",
    "href": "lecture3/lecture3-1.html#action-value-method-update",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Action Value Method Update",
    "text": "Action Value Method Update\nTo avoid computationally expensive updates using the predicate method, we can update action values in an incremental fashion:\n\\[\nQ_{t+1} = Q_t + \\frac{1}{t} (R_t - Q_t)\n\\]\nor\n\\[\nNewEstimate \\gets OldEstimate + StepSize [Target - OldEstimate]\n\\]\n\n\n\n\n\n\nShould we always pick actions with the highest expected value? ü§î",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html",
    "href": "lecture3/lecture3-2.html",
    "title": "3.2 Œµ-Greedy",
    "section": "",
    "text": "Exploring vs.¬†Exploiting\nIntuition: Acting randomly.\nIntuition: Acting systematically.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html#exploring-vs.-exploiting",
    "href": "lecture3/lecture3-2.html#exploring-vs.-exploiting",
    "title": "3.2 Œµ-Greedy",
    "section": "",
    "text": "We are exploring when we randomly select an action.\n\n\n\nWe are exploiting when an action is selected based on its expected value. When we act this way, we are said to be acting in a greedy manner.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html#conflict-of-exploring-vs.-exploiting",
    "href": "lecture3/lecture3-2.html#conflict-of-exploring-vs.-exploiting",
    "title": "3.2 Œµ-Greedy",
    "section": "Conflict of Exploring vs.¬†Exploiting",
    "text": "Conflict of Exploring vs.¬†Exploiting\n\nExploring all of the time does not permit you to exploit your knowledge of expected values.\nExploiting all of the time does not permit you to explore all of the options.\n\nThus, our decision-making must encompass a balance of exploring and exploiting actions.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html#the-role-of-epsilon",
    "href": "lecture3/lecture3-2.html#the-role-of-epsilon",
    "title": "3.2 Œµ-Greedy",
    "section": "The Role of Epsilon",
    "text": "The Role of Epsilon\n\nEpsilon (\\(\\epsilon\\)) is a fixed proportion that decides whether we explore or exploit our actions.\n\n\\[\n  A_t \\gets\n  \\begin{cases}\n      \\text{a random action with probability } \\epsilon \\\\\n      \\arg\\max_a Q(a) \\text{ with probability } 1 - \\epsilon\n  \\end{cases}\n\\]\n\nHence, Epsilon Greedy is an algorithm that allows us to balance our decision-making in this simple manner.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html#epsilon-greedy-pseudocode",
    "href": "lecture3/lecture3-2.html#epsilon-greedy-pseudocode",
    "title": "3.2 Œµ-Greedy",
    "section": "Epsilon Greedy Pseudocode",
    "text": "Epsilon Greedy Pseudocode\n\n\n\\begin{algorithm} \\caption{\\textbf{Algorithm 3-1} MAB Epsilon Greedy} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\\\~\\\\ \\For{$t$ in range($len(data)$)} \\State $A_t \\gets \\begin{cases} \\text{a random action with probability } \\epsilon \\\\ \\text{argmax}_a\\, Q(a) \\text{ with probability } 1-\\epsilon \\end{cases}$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-3.html",
    "href": "lecture3/lecture3-3.html",
    "title": "3.3 Upper Confidence Boundary (UCB)",
    "section": "",
    "text": "Upper Confidence Boundaries\nUpper Confidence Boundaries allow us to select among the non-greedy actions according to their potential for actually being optimal.\n\\[\nA_t \\gets \\arg\\max_a \\left[ Q(a) + \\sqrt{\\frac{2 \\ln(t)}{N(a)}} \\right]\n\\]\nWhere:",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.3 Upper Confidence Boundary (UCB)</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-3.html#upper-confidence-boundaries",
    "href": "lecture3/lecture3-3.html#upper-confidence-boundaries",
    "title": "3.3 Upper Confidence Boundary (UCB)",
    "section": "",
    "text": "\\(\\sqrt{\\frac{2 \\ln(t)}{N(a)}}\\) is the measure of variance of the action \\(a\\).\nThe natural logarithm increases get smaller over time but are unbounded, so all actions will be selected.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.3 Upper Confidence Boundary (UCB)</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-3.html#ucb-exploring-vs.-exploiting",
    "href": "lecture3/lecture3-3.html#ucb-exploring-vs.-exploiting",
    "title": "3.3 Upper Confidence Boundary (UCB)",
    "section": "UCB Exploring vs.¬†Exploiting",
    "text": "UCB Exploring vs.¬†Exploiting\nEach time \\(a\\) is selected, the uncertainty is presumably reduced: \\(N(a)\\) increments, and as it appears in the denominator, the uncertainty term decreases.\n\\[\nVAR \\downarrow = \\sqrt{\\frac{2 \\ln(t)}{N(a)\\uparrow}}\n\\]\nEach time an action other than \\(a\\) is selected, \\(t\\) increases but \\(N(a)\\) does not; because \\(t\\) appears in the numerator, the uncertainty estimate increases.\n\\[\nVAR \\uparrow = \\sqrt{\\frac{2 \\ln(t) \\uparrow}{N(a)}}\n\\]\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{MAB Upper Confidence Boundary (UCB)} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\\\ \\For{$t$ in range($len(data)$)} \\State $A_t \\gets argmax_a[Q(a) + \\sqrt{(\\frac{2 ln(t)}{N(a)})}]$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.3 Upper Confidence Boundary (UCB)</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-3.html#ucb-pseudocode",
    "href": "lecture3/lecture3-3.html#ucb-pseudocode",
    "title": "3.3 Upper Confidence Boundary (UCB)",
    "section": "UCB Pseudocode",
    "text": "UCB Pseudocode",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.3 Upper Confidence Boundary (UCB)</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html",
    "href": "lecture3/lecture3-4.html",
    "title": "3.4 Thompson Sampling",
    "section": "",
    "text": "Beta Distribution\nThompson sampling is an algorithm that leverages the beta distribution for its action value method and action selections.\nInitialize beta distributed with parameters:\n\\[\n\\alpha(a) = (\\alpha_{1}, . . . , \\alpha_{k}) = 1\n\\] \\[\n\\beta(a) = (\\beta_{1}, . . . , \\beta_{k}) = 1\n\\]\nNow for each action \\(a\\), the prior probability density function of our action value method \\(Q(a)\\) is:\n\\[\n  Q(a) = \\frac{\\Gamma(\\alpha(a) + \\beta(a))}{\\Gamma(\\alpha(a)) \\ \\Gamma(\\beta(a))} a^{\\alpha(a)-1} (1 a)^{\\beta(a)-1}\n\\]",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html#thompson-sampling",
    "href": "lecture3/lecture3-4.html#thompson-sampling",
    "title": "3.4 Thompson Sampling",
    "section": "",
    "text": "Thompson sampling is an algorithm that leverages the beta distribution for its action value method and action selections.\nInitialize beta distributed with parameters:\n\\[\n\\alpha(a) = (\\alpha_{1}, . . . , \\alpha_{k}) = 1\n\\] \\[\n\\beta(a) = (\\beta_{1}, . . . , \\beta_{k}) = 1\n\\]\nNow for each action \\(a\\), the prior probability density function of our action value method \\(Q(a)\\) is:\n\\[\nQ(a) = \\frac{\\Gamma(\\alpha(a) + \\beta(a))}{\\Gamma(\\alpha(a)) \\ \\Gamma(\\beta(a))} a^{\\alpha(a)-1} (1 - a)^{\\beta(a)-1}\n\\]",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html#action-value-method-update",
    "href": "lecture3/lecture3-4.html#action-value-method-update",
    "title": "3.4 Thompson Sampling",
    "section": "Action Value Method Update",
    "text": "Action Value Method Update\n\nThe agent updates its prior belief using the following action value method:\n\\[\n\\alpha(A_{t}) \\gets \\alpha(A_{t}) + R_{t}\n\\] \\[\n\\beta(A_{t}) \\gets \\beta(A_{t}) + 1 - R_{t}\n\\]\nNotice that for those actions selected \\(A_t\\), we increase its corresponding \\(\\alpha\\) parameter (\\(R_t = 1\\)) and maintain its \\(\\beta\\) parameter the same as before (\\(1 - R_t = 1 - 1 = 0\\)).\nThis update allows the algorithm to draw accurate samples and strike a balance between exploring and exploiting.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html#thompson-sampling-pseudocode",
    "href": "lecture3/lecture3-4.html#thompson-sampling-pseudocode",
    "title": "3.4 Thompson Sampling",
    "section": "Thompson Sampling Pseudocode",
    "text": "Thompson Sampling Pseudocode\n\n\n\nThompson Sampling Pseudocode",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#instructions",
    "href": "homework/homework2.html#instructions",
    "title": "Homework 3: Reinforcement Learning",
    "section": "Instructions",
    "text": "Instructions\n\nShow ALL Work, Neatly and in Order.\nNo credit for Answers Without Work.\nSubmit a single PDF file that includes all of your solutions.\nDO NOT submit individual files or images.\nFor coding questions, submit ONE .py file and include comments.\n\n\n\n\n\n\n\nNote\n\n\n\nFor this homework, you only need numpy, pandas, and sklearn‚Äôs MinMaxScaler function.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#exercises",
    "href": "homework/homework2.html#exercises",
    "title": "Homework 3: Reinforcement Learning",
    "section": "Exercises",
    "text": "Exercises\n\nE.1: Load Environments\nLoad existing Bernoulli and Gaussian environments from Utils.env using the default options and a random seed of 123.\n\n\nE.2: Epsilon-Greedy Recommendation System\nUsing the existing Epsilon-Greedy (\\(\\epsilon = 0.10\\)) code, create a recommendation system.\n\n\nE.3: UCB Algorithm and Recommendation System\n\nAlgorithm: Upper Confidence Boundary (UCB)\nInitialize, for a = 1 to k:\n  Q(a) ‚Üê 0\n  N(a) ‚Üê 0\n\nFor t in range(len(data)):\n  A_t ‚Üê argmax_a [Q(a) + sqrt((2 ln(t)) / N(a))]\n  R_t ‚Üê bandit(A_t)\n  N(A_t) ‚Üê N(A_t) + 1\n  Q(A_t) ‚Üê Q(A_t) + (1 / N(A_t)) [R_t - Q(A_t)]\n\n\n\nE.4: Thompson Sampling Algorithm and Recommendation System\n\nAlgorithm: Thompson Sampling\nInitialize, for a = 1 to k:\n  Œ±(a) ‚Üê 1\n  Œ≤(a) ‚Üê 1\n\nFor t in range(len(data)):\n  Q(a) ‚Üê beta(Œ±(a), Œ≤(a))\n  A_t ‚Üê argmax_a Q(a)\n  R_t ‚Üê bandit(A_t)\n  Œ±(A_t) ‚Üê Œ±(A_t) + R_t\n  Œ≤(A_t) ‚Üê Œ≤(A_t) + (1 - R_t)\n\n\n\nE.5: Algorithm Performance Comparison\nFor 10,000 recommendations:\n\nDoes Epsilon-Greedy (\\(\\epsilon = 0.10\\)) perform better in the Bernoulli or Gaussian environment?\nDoes UCB perform better in the Bernoulli or Gaussian environment?\nDoes Thompson Sampling perform better in the Bernoulli or Gaussian environment?\nWhich algorithm performs best in the Bernoulli environment?\nWhich algorithm performs best in the Gaussian environment?\n\nHint: Check the performance of each MAB by observing the most frequently recommended arm.\n\n\nE.6: Random Seed Analysis\nUsing random seeds 0-50, for 10,000 recommendations, do the algorithms perform the same?\n\n\nE.7: Amazon Dataset Analysis\nFor the Amazon.csv dataset, repeat exercise E.6 and find the most frequently recommended arm.\n\n\nE.8: EXP3 Algorithm and Recommendation System\n\nAlgorithm: EXP3\nInitialize, for a = 1 to k:\n  Q(a) ‚Üê 1/k\n  W(a) ‚Üê 1\n\nFor t in range(len(data)):\n  Q(a) ‚Üê (1 - Œ≥) * (W(a) / sum(W)) + (Œ≥ / k)\n  A_t ‚Üê random choice with probabilities Q(a)\n  R_t ‚Üê bandit(A_t)\n  RÃÇ_t(a) ‚Üê (R_t / Q(A_t)) if A_t = a else 0\n  W(a) ‚Üê W(a) * exp(Œ≥ * RÃÇ_t(a) / k)\n\n\n\nE.9: Gradient Method Algorithm and Recommendation System\n\nAlgorithm: Gradient Method\nInitialize, for a = 1 to k:\n  Q(a) ‚Üê 0\n  N(a) ‚Üê 0\n  H(a) ‚Üê 0\n\nFor t in range(len(data)):\n  œÄ(a) ‚Üê softmax(H(a))\n  A_t ‚Üê argmax_a(œÄ(a))\n  R_t ‚Üê bandit(A_t)\n  N(A_t) ‚Üê N(A_t) + 1\n  Q(A_t) ‚Üê Q(A_t) + (1 / N(A_t)) [R_t - Q(A_t)]\n  H(a) ‚Üê update using softmax gradient update rule\n\nThis document provides a structured layout for your homework submission, including algorithms and instructions in a Quarto-compatible format.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.2-epsilon-greedy-recommendation-system",
    "href": "homework/homework2.html#e.2-epsilon-greedy-recommendation-system",
    "title": "Homework 2",
    "section": "E.2: Epsilon-Greedy Recommendation System",
    "text": "E.2: Epsilon-Greedy Recommendation System\nUsing the existing Epsilon-Greedy (\\(\\epsilon = 0.10\\)) code, create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.3-ucb-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#e.3-ucb-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "E.3: UCB Algorithm and Recommendation System",
    "text": "E.3: UCB Algorithm and Recommendation System\n\nAlgorithm: Upper Confidence Boundary (UCB)\nInitialize, for a = 1 to k:\n  Q(a) ‚Üê 0\n  N(a) ‚Üê 0\n\nFor t in range(len(data)):\n  A_t ‚Üê argmax_a [Q(a) + sqrt((2 ln(t)) / N(a))]\n  R_t ‚Üê bandit(A_t)\n  N(A_t) ‚Üê N(A_t) + 1\n  Q(A_t) ‚Üê Q(A_t) + (1 / N(A_t)) [R_t - Q(A_t)]",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.4-thompson-sampling-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#e.4-thompson-sampling-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "E.4: Thompson Sampling Algorithm and Recommendation System",
    "text": "E.4: Thompson Sampling Algorithm and Recommendation System\n\nAlgorithm: Thompson Sampling\nInitialize, for a = 1 to k:\n  Œ±(a) ‚Üê 1\n  Œ≤(a) ‚Üê 1\n\nFor t in range(len(data)):\n  Q(a) ‚Üê beta(Œ±(a), Œ≤(a))\n  A_t ‚Üê argmax_a Q(a)\n  R_t ‚Üê bandit(A_t)\n  Œ±(A_t) ‚Üê Œ±(A_t) + R_t\n  Œ≤(A_t) ‚Üê Œ≤(A_t) + (1 - R_t)",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.5-algorithm-performance-comparison",
    "href": "homework/homework2.html#e.5-algorithm-performance-comparison",
    "title": "Homework 2",
    "section": "E.5: Algorithm Performance Comparison",
    "text": "E.5: Algorithm Performance Comparison\nFor 10,000 recommendations:\n\nDoes Epsilon-Greedy (\\(\\epsilon = 0.10\\)) perform better in the Bernoulli or Gaussian environment?\nDoes UCB perform better in the Bernoulli or Gaussian environment?\nDoes Thompson Sampling perform better in the Bernoulli or Gaussian environment?\nWhich algorithm performs best in the Bernoulli environment?\nWhich algorithm performs best in the Gaussian environment?\n\nHint: Check the performance of each MAB by observing the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.6-random-seed-analysis",
    "href": "homework/homework2.html#e.6-random-seed-analysis",
    "title": "Homework 2",
    "section": "E.6: Random Seed Analysis",
    "text": "E.6: Random Seed Analysis\nUsing random seeds 0-50, for 10,000 recommendations, do the algorithms perform the same?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.7-amazon-dataset-analysis",
    "href": "homework/homework2.html#e.7-amazon-dataset-analysis",
    "title": "Homework 2",
    "section": "E.7: Amazon Dataset Analysis",
    "text": "E.7: Amazon Dataset Analysis\nFor the Amazon.csv dataset, repeat exercise E.6 and find the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.8-exp3-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#e.8-exp3-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "E.8: EXP3 Algorithm and Recommendation System",
    "text": "E.8: EXP3 Algorithm and Recommendation System\n\nAlgorithm: EXP3\nInitialize, for a = 1 to k:\n  Q(a) ‚Üê 1/k\n  W(a) ‚Üê 1\n\nFor t in range(len(data)):\n  Q(a) ‚Üê (1 - Œ≥) * (W(a) / sum(W)) + (Œ≥ / k)\n  A_t ‚Üê random choice with probabilities Q(a)\n  R_t ‚Üê bandit(A_t)\n  RÃÇ_t(a) ‚Üê (R_t / Q(A_t)) if A_t = a else 0\n  W(a) ‚Üê W(a) * exp(Œ≥ * RÃÇ_t(a) / k)",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#e.9-gradient-method-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#e.9-gradient-method-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "E.9: Gradient Method Algorithm and Recommendation System",
    "text": "E.9: Gradient Method Algorithm and Recommendation System\n\nAlgorithm: Gradient Method\nInitialize, for a = 1 to k:\n  Q(a) ‚Üê 0\n  N(a) ‚Üê 0\n  H(a) ‚Üê 0\n\nFor t in range(len(data)):\n  œÄ(a) ‚Üê softmax(H(a))\n  A_t ‚Üê argmax_a(œÄ(a))\n  R_t ‚Üê bandit(A_t)\n  N(A_t) ‚Üê N(A_t) + 1\n  Q(A_t) ‚Üê Q(A_t) + (1 / N(A_t)) [R_t - Q(A_t)]\n  H(a) ‚Üê update using softmax gradient update rule",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-2-epsilon-greedy-recommendation-system",
    "href": "homework/homework2.html#coding-exercise-2-epsilon-greedy-recommendation-system",
    "title": "Homework 2",
    "section": "Coding Exercise 2: Epsilon-Greedy Recommendation System",
    "text": "Coding Exercise 2: Epsilon-Greedy Recommendation System\nUsing the existing Epsilon-Greedy (\\(\\epsilon = 0.10\\)) code, create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-3-ucb-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#coding-exercise-3-ucb-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "Coding Exercise 3: UCB Algorithm and Recommendation System",
    "text": "Coding Exercise 3: UCB Algorithm and Recommendation System\nCode the Upper Confidence Boundary (UCB) algorithm and create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-4-thompson-sampling-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#coding-exercise-4-thompson-sampling-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "Coding Exercise 4: Thompson Sampling Algorithm and Recommendation System",
    "text": "Coding Exercise 4: Thompson Sampling Algorithm and Recommendation System\nCode the Thompson Sampling algorithm and create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-5-algorithm-performance-comparison",
    "href": "homework/homework2.html#coding-exercise-5-algorithm-performance-comparison",
    "title": "Homework 2",
    "section": "Coding Exercise 5: Algorithm Performance Comparison",
    "text": "Coding Exercise 5: Algorithm Performance Comparison\nFor 10,000 recommendations:\n\nDoes Epsilon-Greedy (\\(\\epsilon = 0.10\\)) perform better in the Bernoulli or Gaussian environment?\nDoes UCB perform better in the Bernoulli or Gaussian environment?\nDoes Thompson Sampling perform better in the Bernoulli or Gaussian environment?\nWhich algorithm performs best in the Bernoulli environment?\nWhich algorithm performs best in the Gaussian environment?\n\nHint: Check the performance of each MAB by observing the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-6-random-seed-analysis",
    "href": "homework/homework2.html#coding-exercise-6-random-seed-analysis",
    "title": "Homework 2",
    "section": "Coding Exercise 6: Random Seed Analysis",
    "text": "Coding Exercise 6: Random Seed Analysis\nUsing random seeds 0-50, for 10,000 recommendations, do the algorithms perform the same?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-7-amazon-dataset-analysis",
    "href": "homework/homework2.html#coding-exercise-7-amazon-dataset-analysis",
    "title": "Homework 2",
    "section": "Coding Exercise 7: Amazon Dataset Analysis",
    "text": "Coding Exercise 7: Amazon Dataset Analysis\nFor the Amazon.csv dataset, repeat exercise Coding Exercise 6 and find the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-8-exp3-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#coding-exercise-8-exp3-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "Coding Exercise 8: EXP3 Algorithm and Recommendation System",
    "text": "Coding Exercise 8: EXP3 Algorithm and Recommendation System\n\n\n\\begin{algorithm} \\caption{MAB EXP3} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets \\frac{1}{k}$ \\State $W(a) \\gets 1$ \\\\ \\For{$t$ in range($len(data)$)} \\State $Q(a) \\gets (1 - \\gamma) \\frac{W(a)}{\\sum_{i=1}^{k} W(a_{i})} + \\frac{\\gamma}{k}$ \\\\ \\State $A_t \\gets$ random choice with probabilities $Q(a)$ \\\\ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $\\hat{R_t} \\gets \\begin{cases} \\frac{R_t}{Q(A_t)} & \\text{if } A_t = a\\\\ 0 & \\text{else} \\end{cases}$ \\State $W(a) \\gets W(a) e^{\\frac{\\gamma \\hat{R_t}}{k}}$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-9-gradient-method-algorithm-and-recommendation-system",
    "href": "homework/homework2.html#coding-exercise-9-gradient-method-algorithm-and-recommendation-system",
    "title": "Homework 2",
    "section": "Coding Exercise 9: Gradient Method Algorithm and Recommendation System",
    "text": "Coding Exercise 9: Gradient Method Algorithm and Recommendation System\n\n\n\\begin{algorithm} \\caption{MAB Gradient Method} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\State $H(a) \\gets 0$ \\\\ \\For{$t$ in range($len(data)$)} \\State $\\pi(a) \\gets \\text{softmax}(H(a))$ \\State $A_t \\gets \\text{argmax}_a(\\pi(a))$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\State $H(a) \\gets \\begin{cases} H(A_t) + \\alpha (R_t - Q(A_t)) (1 - \\pi(A_t)) & \\text{if } A_t = a\\\\ H(a) - \\alpha (R_t - Q(a)) \\pi(a) & \\text{else} \\end{cases}$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html#beta-distribution",
    "href": "lecture3/lecture3-4.html#beta-distribution",
    "title": "3.4 Thompson Sampling",
    "section": "",
    "text": "Thompson sampling is an algorithm that leverages the beta distribution for its action value method and action selections.\nInitialize beta distributed with parameters:\n\\[\n\\alpha(a) = (\\alpha_{1}, . . . , \\alpha_{k}) = 1\n\\] \\[\n\\beta(a) = (\\beta_{1}, . . . , \\beta_{k}) = 1\n\\]\nNow for each action \\(a\\), the prior probability density function of our action value method \\(Q(a)\\) is:\n\\[\nQ(a) = \\frac{\\Gamma(\\alpha(a) + \\beta(a))}{\\Gamma(\\alpha(a)) \\ \\Gamma(\\beta(a))} a^{\\alpha(a)-1} (1 - a)^{\\beta(a)-1}\n\\]",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html#thompson-sampling-exploring-vs.-exploiting",
    "href": "lecture3/lecture3-4.html#thompson-sampling-exploring-vs.-exploiting",
    "title": "3.4 Thompson Sampling",
    "section": "Thompson Sampling Exploring vs.¬†Exploiting",
    "text": "Thompson Sampling Exploring vs.¬†Exploiting\nThe agent updates its prior belief using the following action value method:\n\\[\n\\alpha(A_{t}) \\gets \\alpha(A_{t}) + R_{t}\n\\] \\[\n\\beta(A_{t}) \\gets \\beta(A_{t}) + 1 R_{t}\n\\]\nNotice that for those actions selected \\(A_t\\), we increase its corresponding \\(\\alpha\\) parameter (\\(R_t = 1\\)) and maintain its \\(\\beta\\) parameter the same as before (\\(1 - R_t = 1 - 1 = 0\\)).\nThis update allows the algorithm to draw accurate samples and strike a balance between exploring and exploiting.\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{MAB Thompson Sampling} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $\\alpha(a) \\gets 1$ \\State $\\beta(a) \\gets 1$ \\\\ \\For{$t$ in range($len(data)$)} \\State $Q(a) \\leftarrow beta(\\alpha(a),\\beta(a))$ \\State $A_t \\leftarrow argmax_a Q(a)$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $\\alpha(A_t) \\leftarrow \\alpha(A_t) + R_t$ \\State $\\beta(A_t) \\leftarrow \\beta(A_t) + 1 - R_t$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html#the-role-of-Œµ",
    "href": "lecture3/lecture3-2.html#the-role-of-Œµ",
    "title": "3.2 Œµ-Greedy",
    "section": "The Role of Œµ",
    "text": "The Role of Œµ\nEpsilon (\\(\\epsilon\\)) is a fixed proportion that decides whether we explore or exploit our actions. \\[\n  A_t \\gets\n  \\begin{cases}\n      \\text{a random action with probability } \\epsilon \\\\\n      \\arg\\max_a Q(a) \\text{ with probability } 1 - \\epsilon\n  \\end{cases}\n\\]\nHence, Epsilon Greedy is an algorithm that allows us to balance our decision-making in this simple manner.\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{MAB $\\epsilon$-Greedy} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\\\ \\For{$t$ in range($len(data)$)} \\State $A_t \\gets \\begin{cases} \\text{a random action with probability } \\epsilon \\\\ \\text{argmax}_a\\, Q(a) \\text{ with probability } 1-\\epsilon \\end{cases}$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html#pseudocode",
    "href": "lecture3/lecture3-2.html#pseudocode",
    "title": "3.2 Œµ-Greedy",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{MAB $\\epsilon$-Greedy} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\\\ \\For{$t$ in range($len(data)$)} \\State $A_t \\gets \\begin{cases} \\text{a random action with probability } \\epsilon \\\\ \\text{argmax}_a\\, Q(a) \\text{ with probability } 1-\\epsilon \\end{cases}$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-3.html#pseudocode",
    "href": "lecture3/lecture3-3.html#pseudocode",
    "title": "3.3 Upper Confidence Boundary (UCB)",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{MAB Upper Confidence Boundary (UCB)} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\\\ \\For{$t$ in range($len(data)$)} \\State $A_t \\gets argmax_a[Q(a) + \\sqrt{(\\frac{2 ln(t)}{N(a)})}]$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.3 Upper Confidence Boundary (UCB)</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html#pseudocode",
    "href": "lecture3/lecture3-4.html#pseudocode",
    "title": "3.4 Thompson Sampling",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{MAB Thompson Sampling} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $\\alpha(a) \\gets 1$ \\State $\\beta(a) \\gets 1$ \\\\ \\For{$t$ in range($len(data)$)} \\State $Q(a) \\leftarrow beta(\\alpha(a),\\beta(a))$ \\State $A_t \\leftarrow argmax_a Q(a)$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $\\alpha(A_t) \\leftarrow \\alpha(A_t) + R_t$ \\State $\\beta(A_t) \\leftarrow \\beta(A_t) + 1 - R_t$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#coding-exercise-5-mab-algorithm-performance-comparison",
    "href": "homework/homework2.html#coding-exercise-5-mab-algorithm-performance-comparison",
    "title": "Homework 2",
    "section": "Coding Exercise 5: MAB Algorithm Performance Comparison",
    "text": "Coding Exercise 5: MAB Algorithm Performance Comparison\nFor 10,000 recommendations:\n\nDoes Epsilon-Greedy (\\(\\epsilon = 0.10\\)) perform better in the Bernoulli or Gaussian environment?\nDoes UCB perform better in the Bernoulli or Gaussian environment?\nDoes Thompson Sampling perform better in the Bernoulli or Gaussian environment?\nWhich algorithm performs best in the Bernoulli environment?\nWhich algorithm performs best in the Gaussian environment?\n\nHint: Check the performance of each MAB by observing the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html",
    "href": "lecture4/lecture4-1.html",
    "title": "4.1 Markov Chain",
    "section": "",
    "text": "Markov Models\nAll Markov models assume the Markov Property, meaning that future states depend only on current states and not previous states.\n\\[\nP(s‚Äô | s, s_{t-1}, s_{t-2}, \\dots) = P(s‚Äô | s)\n\\]\nMarkov models differ based on whether every sequential state is observable and whether the system is adjusted based on observations:",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#definition",
    "href": "lecture4/lecture4-1.html#definition",
    "title": "Markov Chain",
    "section": "Definition",
    "text": "Definition\n\nA Markov Chain is a model for transitions that are not controlled between fully observable states.\nA State is a node.\nA State Transition is one outward-going arrow.\nState transitions are conditional probabilities of going to the next state given the current state.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#probability-matrix",
    "href": "lecture4/lecture4-1.html#probability-matrix",
    "title": "4.1 Markov Chain",
    "section": "Probability Matrix",
    "text": "Probability Matrix\nSuppose a frog jumps from one lily pad to another with state transition probabilities:\n\n\n\n\\[\n\\mathbf{P} = \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.8 & 0.4 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#rewards-matrix",
    "href": "lecture4/lecture4-1.html#rewards-matrix",
    "title": "4.1 Markov Chain",
    "section": "Rewards Matrix",
    "text": "Rewards Matrix\nSuppose the frog has associated rewards:\n\n\n\n\\[\n\\mathbf{R} = \\begin{bmatrix} 6 & 1 \\\\ 1 & -2 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html",
    "href": "lecture4/lecture4-2.html",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "",
    "text": "Markov Decision Process\nA Markov Decision Process (MDP) is a model for transitions that are controlled between fully observable states.  The Agent is the learner and decision-maker.  The Environment is everything external to the agent.  From an Initial State, the agent interacts with the environment through its Actions.  These actions continuously give rise to different States and Rewards.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#markov-decision-process",
    "href": "lecture4/lecture4-2.html#markov-decision-process",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "",
    "text": "Environment GridWorld\n\n\n\n\n\n\nActions are equally likely to occur. \nActions that take the agent out of the environment receive a reward of \\(-1\\), actions that take the agent to the terminal state (shaded in gray) receive a reward of \\(+1\\), and all other actions receive a reward of \\(0\\). \nOur objective is to calculate the shortest path from any state to the optimal state.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#example-gridworld",
    "href": "lecture4/lecture4-2.html#example-gridworld",
    "title": "Markov Decision Process (MDPs)",
    "section": "Example: GridWorld",
    "text": "Example: GridWorld\nAgent, Environment, Initial State, and Actions:\n\nActions are equally likely to occur.  Actions that take the agent out of the environment receive a reward of \\(-1\\), actions that take the agent to the terminal state (shaded in gray) receive a reward of \\(+1\\), and all other actions receive a reward of \\(0\\).  Our objective is to calculate the shortest path from any state to the optimal state.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#sequential-interaction",
    "href": "lecture4/lecture4-2.html#sequential-interaction",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Sequential Interaction",
    "text": "Sequential Interaction\nFor a finite discrete number of time steps \\(t = 0, 1, 2, 3...,T\\) (where \\(T\\) is the terminal time step marking the end of an episode) the sequential interaction is:\n\nThe agent receives an interpretation from the state \\(s_t \\in S\\).\nThe agent makes an action \\(a_t \\in A(s_t)\\) based on the situation.\nThe agent receives a reward \\(r_{t+1} \\in R \\subseteq \\mathbb{R}\\) from its environment and finds itself in a new state \\(s_{t+1}\\) based on the action taken.\n\nThe sequence continues in the form:\n\\[\ns_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3,...\n\\]\n\nEnvironment GridWorld\nSequential interaction for one episode:\n\n\n\n\n\nNotice that, for now, state transitions are deterministic. In other words, we assume a perfect model of the environment. We do not care about stochastic state transitions (this is something that we will visit in the next lectures).",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#example-gridworld-sequential-interaction",
    "href": "lecture4/lecture4-2.html#example-gridworld-sequential-interaction",
    "title": "Markov Decision Process (MDPs)",
    "section": "Example: GridWorld Sequential Interaction",
    "text": "Example: GridWorld Sequential Interaction\nSequential interaction for one episode:\n\nNotice that, for now, state transitions are deterministic. In other words, we assume a perfect model of the environment. We do not care about stochastic state transitions (this is something that we will visit in the next lectures).",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#policy",
    "href": "lecture4/lecture4-2.html#policy",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Policy",
    "text": "Policy\nA Policy is a mapping from states to probabilities of selecting each possible action, denoted as:\n\\[\n\\pi(a|s)\n\\]\n\n\n\n\n\n\\[\n\\sum_{a \\in A(s)} \\pi(a|s) = 1 \\quad \\text{for all } s \\in S\n\\]\n\nEnvironment GridWorld\n\n\n\n\n\nSince all actions are equally likely, we are said to be following a random policy:\n\\[\n\\pi(a_0|s_0) = \\frac{1}{4}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#example-gridworld-policy",
    "href": "lecture4/lecture4-2.html#example-gridworld-policy",
    "title": "Markov Decision Process (MDPs)",
    "section": "Example: GridWorld Policy",
    "text": "Example: GridWorld Policy\n\nSince all actions are equally likely, we are said to be following a random policy:\n\\[\n\\pi(a_0|s_0) = \\frac{1}{4}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#dynamic-function",
    "href": "lecture4/lecture4-2.html#dynamic-function",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Dynamic Function",
    "text": "Dynamic Function\nThe dynamic function is a mapping of the state transition probabilities of the MDP for each possible reward:\n\\[\np(s^{'}, r | s, a)\n\\]\n\n\n\n\n\nAs mentioned before, the dynamics of GridWorld are deterministic leading to the same new state given each state and action:\n\\[\n\\sum_{s^{'} \\in S}\\sum_{r \\in R} p(s^{'}, r| s, a) = 1 \\quad \\text{for all } s \\in S \\text{ and } a \\in A(s)\n\\]\n\nEnvironment GridWorld\n\n\n\n\n\nAs mentioned before, the dynamics of GridWorld are deterministic, leading to the same new state given each state and action:\n\\[\np(s_{1}, r_{1}| s_0, a_{0}) = 1\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#example-gridworld-1",
    "href": "lecture4/lecture4-2.html#example-gridworld-1",
    "title": "Markov Decision Process (MDPs)",
    "section": "Example: GridWorld",
    "text": "Example: GridWorld\n\nAs mentioned before, the dynamics of GridWorld are deterministic, leading to the same new state given each state and action:\n\\[\np(s_{1}, r_{1}| s_0, a_{0}) = 1\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#goal",
    "href": "lecture4/lecture4-2.html#goal",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Goal",
    "text": "Goal\nOur Goal \\(G_{t}\\) is to maximize the expected return of the discounted reward sequence:\n\\[\n\\begin{aligned}\nG_{t} = r_{t+1} + \\gamma r_{t+2} + \\ldots + \\gamma^{T-1} r_{T} \\\\\n     = r_{t+1} + \\gamma (r_{t+2} + \\ldots + \\gamma^{T-2} r_{T}) \\\\\n     = r_{t+1} + \\gamma G_{t+1}\n\\end{aligned}\n\\]\n\nExercise\nSuppose \\(\\gamma = 0.5\\) and the following sequence of rewards is received \\(r_{1} = -1\\), \\(r_{2} = 2\\), \\(r_{3} = 6\\), \\(r_{4} = 3\\), and \\(r_{5} = 2\\), with \\(T = 5\\). What are \\(G_{0}\\), \\(G_{1}\\), ‚Ä¶, \\(G_{5}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\nG_{5} &= r_{6} + r_{7} + \\dots = 0 \\\\\nG_{4} &= r_{5} + 0.5(G_{5}) = 2 \\\\\nG_{3} &= r_{4} + 0.5(G_{4}) = 4 \\\\\nG_{2} &= r_{3} + 0.5(G_{3}) = 8 \\\\\nG_{1} &= r_{2} + 0.5(G_{2}) = 6 \\\\\nG_{0} &= r_{1} + 0.5(G_{1}) = 2 \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#example-goal",
    "href": "lecture4/lecture4-2.html#example-goal",
    "title": "Markov Decision Process (MDPs)",
    "section": "Example: Goal",
    "text": "Example: Goal\n\nExample: Goal\nSuppose \\(\\gamma = 0.5\\) and the following sequence of rewards is received \\(r_{1} = -1\\), \\(r_{2} = 2\\), \\(r_{3} = 6\\), \\(r_{4} = 3\\), and \\(r_{5} = 2\\), with \\(T = 5\\). What are \\(G_{0}\\), \\(G_{1}\\), ‚Ä¶, \\(G_{5}\\)?\n\\[\n\\begin{aligned}\nG_{5} &= r_{6} + r_{7} + \\dots = 0 \\\\\nG_{4} &= r_{5} + 0.5(G_{5}) = 2 \\\\\nG_{3} &= r_{4} + 0.5(G_{4}) = 4 \\\\\nG_{2} &= r_{3} + 0.5(G_{3}) = 8 \\\\\nG_{1} &= r_{2} + 0.5(G_{2}) = 6 \\\\\nG_{0} &= r_{1} + 0.5(G_{1}) = 2 \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#value-functions",
    "href": "lecture4/lecture4-2.html#value-functions",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Value Functions",
    "text": "Value Functions\nValue Functions calculate the expected reward when starting from the state \\(s\\) and then interacting with the environment according to the policy \\(\\pi\\), denoted as:\n\\[\nv_{\\pi}(s) = \\mathbb{E_{\\pi}}[G_{t}| \\ s]\n\\]\n\n\n\n\n\n\nBellman Equation\nFor any policy \\(\\pi\\) and any state \\(s\\), the Bellman equation holds:\n\\[\n\\begin{aligned}\nv_{\\pi}(s) &= \\mathbb{E_{\\pi}}[G_{t}| \\ s] \\\\\n&= \\mathbb{E_{\\pi}}[r_{t+1} + \\gamma G_{t+1}| \\ s] \\\\\n&= \\sum_{a} \\pi(a|s) \\sum_{s^{'},r} p(s^{'},r|s,a)[r_{t+1} + \\gamma \\mathbb{E_{\\pi}}[G_{t+1}| \\ s]] \\\\\n&= \\underbrace{\\sum_{a} \\pi(a|s)}_{Policy} \\underbrace{\\sum_{s^{'}, r} p(s^{'},r|s,a)}_{Dynamic \\ Function}\\underbrace{[r_{t+1} + \\gamma v_{\\pi}(s^{'})]}_{Discounted \\ Reward \\ Sequence}\n\\end{aligned}\n\\]\n\nExercise\nFor the first episode, calculate the value of each state using the Bellman equation:\n\\[\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s^{'}, r} p(s^{'},r|s,a)[r + \\gamma v_{\\pi}(s^{'})]\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#bellman-equation",
    "href": "lecture4/lecture4-2.html#bellman-equation",
    "title": "Markov Decision Process (MDPs)",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\nBellman Equation\nFor any policy \\(\\pi\\) and any state \\(s\\), the Bellman equation holds:\n\\[\n\\begin{aligned}\nv_{\\pi}(s) &= \\mathbb{E_{\\pi}}[G_{t}| \\ s] \\\\\n&= \\mathbb{E_{\\pi}}[r_{t+1} + \\gamma G_{t+1}| \\ s] \\\\\n&= \\sum_{a} \\pi(a|s) \\sum_{s^{'},r} p(s^{'},r|s,a)[r_{t+1} + \\gamma \\mathbb{E_{\\pi}}[G_{t+1}| \\ s]] \\\\\n&= \\underbrace{\\sum_{a} \\pi(a|s)}_{Policy} \\underbrace{\\sum_{s^{'}, r} p(s^{'},r|s,a)}_{Dynamic \\ Function}\\underbrace{[r_{t+1} + \\gamma v_{\\pi}(s^{'})]}_{Discounted \\ Reward \\ Sequence}\n\\end{aligned}\n\\]\n\n\nExercise\nFor the first episode, calculate the value of each state using the Bellman equation:\n\\[\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s^{'}, r} p(s^{'},r|s,a)[r + \\gamma v_{\\pi}(s^{'})]\n\\]\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#gridworld-practice-example",
    "href": "lecture4/lecture4-2.html#gridworld-practice-example",
    "title": "Markov Decision Process (MDPs)",
    "section": "GridWorld: Practice Example",
    "text": "GridWorld: Practice Example\n\nGridWorld: Practice Example\nFor the first episode, calculate the value of each state using the Bellman equation:\n\\[\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s^{'}, r} p(s^{'},r|s,a)[r + \\gamma v_{\\pi}(s^{'})]\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#gridworld-solution",
    "href": "lecture4/lecture4-2.html#gridworld-solution",
    "title": "Markov Decision Process (MDPs)",
    "section": "GridWorld: Solution",
    "text": "GridWorld: Solution\n\nGridWorld: Solution\n\nQuestion: Did we know that the value function was going to converge? Is this something that was expected?",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#action-value-functions",
    "href": "lecture4/lecture4-2.html#action-value-functions",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Action Value Functions",
    "text": "Action Value Functions\nAction Value Functions estimate how good it is for an agent to follow policy \\(\\pi\\) given the action taken under the previous state:\n\\[\nq_{\\pi}(s, a) = \\mathbb{E_{\\pi}}[G_{t}| \\ s, \\ a]\n\\]\n\n\n\n\n\n\nBellman Equation\nThe action value function is also expressed in terms of the Bellman equation:\n\\[\n\\begin{align}\n    q_{\\pi}(s, a) & = \\mathbb{E_{\\pi}}[G_{t}| \\ s, \\ a] \\\\\n     & = \\underbrace{\\sum_{s^{'}, r} p(s^{'},r|s,a)}_{Dynamic \\ Function} \\underbrace{[r + \\gamma v_{\\pi}(s^{'})]}_{Discounted \\ Reward \\  Sequence}\n\\end{align}\n\\]\nNotice that the policy is no longer calculated (since an action has already taken place according to the policy), and that the quality of following policy \\(\\pi\\) is calculated in \\(v_{\\pi}(s^{'})\\).",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#bellman-equation-2",
    "href": "lecture4/lecture4-2.html#bellman-equation-2",
    "title": "Markov Decision Process (MDPs)",
    "section": "Bellman Equation",
    "text": "Bellman Equation\nThe action value function is also expressed in terms of the Bellman equation:\n\\[\nq_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_{t}| \\ s, \\ a] = \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{\\pi}(s^{'}) \\big]\n\\]\nNotice that the policy is no longer calculated (since an action has already taken place according to the policy), and that the quality of following policy \\(\\pi\\) is calculated in \\(v_{\\pi}(s^{'})\\).",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#optimal-policy",
    "href": "lecture4/lecture4-2.html#optimal-policy",
    "title": "4.2 Markov Decision Process (MDPs)",
    "section": "Optimal Policy",
    "text": "Optimal Policy\nAn optimal policy \\(\\pi\\) is defined to be better than or equal to a policy \\(\\pi^{'}\\) if its expected return is greater than or equal to that of \\(\\pi^{'}\\) for all states:\n\\[\n\\pi \\geq \\pi^{'} \\quad \\text{I.F.F.} \\quad v_{\\pi} \\geq v_{\\pi^{'}} \\quad \\forall s \\in S\n\\]\nSince there may be more than one optimal policy, we denote all optimal policies by \\(\\pi_{*}\\).\n\nOptimal Value Function\n\\[\nv_{*}(s) = \\max_{a} \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\n\n\n\n\nOptimal Action-Value Function\n\\[\nq_{*}(s, a) = \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\n\n\n\nExercise\nFor the first episode, calculate the optimal value for each state using the bellman equation:\n\\[\nv_{*}(s) = \\max_{a} \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nNotice the behavior of the optimal policy as \\(\\pi_{*} \\to \\infty\\)",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#optimal-value-function",
    "href": "lecture4/lecture4-2.html#optimal-value-function",
    "title": "Markov Decision Process (MDPs)",
    "section": "Optimal Value Function",
    "text": "Optimal Value Function\nBellman Optimal Value Function:\n\\[\nv_{*}(s) = \\max_{a} \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\nOptimal Value Function",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-2.html#optimal-action-value-function",
    "href": "lecture4/lecture4-2.html#optimal-action-value-function",
    "title": "Markov Decision Process (MDPs)",
    "section": "Optimal Action-Value Function",
    "text": "Optimal Action-Value Function\nBellman Optimal Action-Value Function:\n\\[\nq_{*}(s, a) = \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\nOptimal Action-Value Function",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDPs)</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html",
    "href": "lecture4/lecture4-3.html",
    "title": "4.3 Dynamic Programming",
    "section": "",
    "text": "Dynamic Programming\nDynamic Programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).\nThese algorithms have limited utility in reinforcement learning due to their assumption of a perfect model and their computational expense, but they are still important theoretically.\nIn fact, the rest of the course could be seen as a way to replicate the same effect of dynamic programming without its assumptions.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#dynamic-programming",
    "href": "lecture4/lecture4-3.html#dynamic-programming",
    "title": "4.3 Dynamic Programming",
    "section": "",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{Iterative Policy Evaluation (Prediction)}\\begin{algorithmic} \\State \\textbf{Initialize:} \\State $V(s) \\gets 0$ for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\State $\\theta \\gets 10^{-4}$ \\Comment{small positive number for accuracy of estimation} \\Repeat \\State $\\Delta \\gets 0$ \\ForAll{$s \\in S$} \\State $v \\gets V(s)$ \\State $V(s) \\gets \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left[ r + \\gamma V(s') \\right]$ \\State $\\Delta \\gets \\max(\\Delta, |v - V(s)|)$ \\EndFor \\Until{$\\Delta &lt; \\theta$} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#bellman-equations",
    "href": "lecture4/lecture4-3.html#bellman-equations",
    "title": "Dynamic Programming",
    "section": "Bellman Equations",
    "text": "Bellman Equations\nSo far, we looked at how to evaluate Bellman equations:\nValue Function\n\\[ v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s^{'}, r} p(s^{'},r|s,a)[r + \\gamma v_{\\pi}(s^{'})] \\]\nIntuition: How good is it to be in state (s) according to policy ()?\nAction-Value Function\n\\[ q_{\\pi}(s, a) = \\sum_{s^{'}, r} p(s^{'},r|s,a)[r + \\gamma v_{\\pi}(s^{'})] \\]\nIntuition: Is it good to select (a) in (s) and thereafter follow policy ()?",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#policy-evaluation",
    "href": "lecture4/lecture4-3.html#policy-evaluation",
    "title": "Dynamic Programming",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#policy-improvement",
    "href": "lecture4/lecture4-3.html#policy-improvement",
    "title": "4.3 Dynamic Programming",
    "section": "Policy Improvement",
    "text": "Policy Improvement\nWe know how good it is to follow the current policy from \\(s\\) ‚Äî that is \\(v_{\\pi}(s)\\) ‚Äî but would it be better or worse to change to a new policy \\(\\pi^{'}\\)?\nOne way to check if it is better to switch from policy \\(\\pi\\) to \\(\\pi^{'}\\) is by checking if the following inequality holds:\n\\[\nq_{\\pi}(s, \\pi^{'}(s)) \\geq v_{\\pi}(s)\n\\]\nIntuition: If selecting \\(a\\) in \\(s\\) and thereafter following policy \\(\\pi\\) is better than just following \\(\\pi\\), then there must be a better policy \\(\\pi^{'}\\).\nThe special case when this inequality is true is referred to as the policy improvement theorem.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#value-iteration",
    "href": "lecture4/lecture4-3.html#value-iteration",
    "title": "4.3 Dynamic Programming",
    "section": "Value Iteration",
    "text": "Value Iteration\nOne drawback of policy iteration is that policy evaluation is done iteratively, requiring convergence exactly to \\(v_{\\pi}\\), which occurs only in the limit.\nValue Iteration is a dynamic programming algorithm that truncates the policy evaluation step after just one sweep.\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{Value Iteration} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $V(s) \\gets 0$ for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\State $\\theta \\gets 10^{-4}$ \\textit{// small positive number for accuracy of estimation} \\Repeat \\State $\\Delta \\gets 0$ \\Forall{$s \\in S$} \\State $v \\gets V(s)$ \\State $V(s) \\gets \\max_a \\sum_{s^{'},r} p(s^{'},r|s,a) \\left[ r + \\gamma V(s^{'}) \\right]$ \\State $\\Delta \\gets \\max(\\Delta, |v - V(s)|)$ \\Endfor \\Until{$\\Delta &lt; \\theta$} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#value-iteration-pseudocode",
    "href": "lecture4/lecture4-3.html#value-iteration-pseudocode",
    "title": "Dynamic Programming",
    "section": "Value Iteration Pseudocode",
    "text": "Value Iteration Pseudocode",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#generalized-policy-iteration",
    "href": "lecture4/lecture4-3.html#generalized-policy-iteration",
    "title": "4.3 Dynamic Programming",
    "section": "Generalized Policy Iteration",
    "text": "Generalized Policy Iteration\n\n\nGeneralized Policy Iteration refers to the general idea of letting policy evaluation and policy improvement processes interact, regardless of anything else.\nAlmost all of RL can be described as the policy always being improved with respect to the value function, and the value function always being driven toward the value function for the policy.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-3.html#definition",
    "href": "lecture4/lecture4-3.html#definition",
    "title": "Dynamic Programming",
    "section": "",
    "text": "Dynamic Programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).\nThese algorithms have limited utility in reinforcement learning due to their assumption of a perfect model and their computational expense, but they are still important theoretically.\nIn fact, the rest of the course could be seen as a way to replicate the same effect of dynamic programming without its assumptions.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.3 Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "lecture4/lo.html",
    "href": "lecture4/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 4: Dynamic Programming üéØ\n\n\n\n\nMarkov Chain.\nMarkov Decision Process (MDPs).\nDynamic Programming.\nHomemade GridWorld OpenAI environment using gymnasium, pygame & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#markov-models",
    "href": "lecture4/lecture4-1.html#markov-models",
    "title": "4.1 Markov Chain",
    "section": "",
    "text": "States are fully observable\nStates are partially observable\n\n\n\n\nDecision-making is not controlled\nMarkov Chain\nHidden Markov Model\n\n\nDecision-making is controlled\nMarkov Decision Process\nPartially Observable Markov Decision Process",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#markov-chain",
    "href": "lecture4/lecture4-1.html#markov-chain",
    "title": "4.1 Markov Chain",
    "section": "Markov Chain",
    "text": "Markov Chain\nA Markov Chain is a model for transitions that are not controlled between fully observable states.  A State is a node.  A State Transition is one outward-going arrow.  State transitions are conditional probabilities of going to the next state given the current state.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#value-function",
    "href": "lecture4/lecture4-1.html#value-function",
    "title": "4.1 Markov Chain",
    "section": "Value Function",
    "text": "Value Function\nWe want to calculate the expected value of moving from state \\(i\\) to state \\(j\\) for all situations \\(s \\in \\{1,2,...,S\\}\\):\n\\[\n\\begin{align*}\nv_{j}(t) & = \\sum_{i=1}^{S} p_{i,j} \\ [r_{i,j}+v_{i}(t-1)] \\\\\n& = \\sum_{i=1}^{S} p_{i,j} \\ r_{i,j} + \\sum_{i=1}^{S} p_{i,j} \\ v_{i}(t-1) \\\\\n& = \\textbf{q} + \\sum_{i=1}^{S} p_{i,j}\\ v_{i}(t-1)\n\\end{align*}\n\\]\n\\[\n\\mathbf{v}(t) = \\mathbf{q} + \\mathbf{v}(t-1) \\mathbf{P}\n\\]\nFirst, we need to calculate \\(\\textbf{q}\\), the expected reward in the next transition out of state \\(i\\):\n\\[\n\\mathbf{q} = \\sum_{i=1}^{S} p_{i,j} r_{i,j}\n\\]\n\\[\nq_{1} = p_{1,1} \\ r_{1,1} + r_{2,1} \\ p_{2,1}\n\\]\n\\[\nq_{2} = p_{1,2} \\ r_{1,2} + r_{2,2} \\ p_{2,2}\n\\]\n\\[\n\\textbf{q} =\n\\begin{bmatrix}\n    2 & -0.2\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix} v_{1}(t) \\ v_{2}(t) \\end{bmatrix} = \\begin{bmatrix} 2 \\ -0.2 \\end{bmatrix} + \\begin{bmatrix} v_{1}(t-1) \\ v_{2}(t-1) \\end{bmatrix} \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.8 & 0.4 \\end{bmatrix}\n\\]\nAt \\(t=100\\): \\[\n\\mathbf{v}(100) = \\begin{bmatrix} 77.88 & 76.59 \\end{bmatrix}\n\\]\nIn other words, the frogs expected value at \\(t = 100\\) is that lilly pad \\(1\\) will be greater (with \\(77.88\\) expected flies) than that of lilly pad \\(2\\) (with \\(76.59\\) expected flies)",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#asymptotic-behavior",
    "href": "lecture4/lecture4-1.html#asymptotic-behavior",
    "title": "4.1 Markov Chain",
    "section": "Asymptotic Behavior",
    "text": "Asymptotic Behavior",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#discounting-factor",
    "href": "lecture4/lecture4-1.html#discounting-factor",
    "title": "4.1 Markov Chain",
    "section": "Discounting Factor",
    "text": "Discounting Factor\nThe \\(\\gamma\\) allows us to place a higher value on the present rewards, rather than future uncertain rewards.\n\\[\n\\mathbf{v}(t) = \\mathbf{q} + \\gamma \\mathbf{v}(t-1) \\mathbf{P}\n\\]\n\\[\n\\begin{bmatrix} v_{1}(t) \\ v_{2}(t) \\end{bmatrix} = \\begin{bmatrix} 2 \\ -0.2 \\end{bmatrix} + \\gamma \\begin{bmatrix} v_{1}(t-1) \\ v_{2}(t-1) \\end{bmatrix} \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.8 & 0.4 \\end{bmatrix}\n\\]\nAt \\(\\gamma=0.9\\) and \\(t=100\\): \\[\n\\mathbf{v}(100) = \\begin{bmatrix} 8.47 & 7.15 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "lecture4/lecture4-1.html#discounted-asymptotic-behavior",
    "href": "lecture4/lecture4-1.html#discounted-asymptotic-behavior",
    "title": "4.1 Markov Chain",
    "section": "Discounted Asymptotic Behavior",
    "text": "Discounted Asymptotic Behavior",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-2",
    "href": "homework/homework2.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA retail store accepts either American Express or VISA. The percentages of customers carrying each card are: \n\nAmerican Express: \\(24%\\)\nVISA: \\(61%\\)\nBoth: \\(11%\\)\n\nWhat percentage of customers carry a card accepted by the store?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-3",
    "href": "homework/homework2.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSixty percent of students wear neither a ring nor a necklace. Given: \n\n\\(20%\\) wear a ring\n\\(30%\\) wear a necklace\n\nFind the probability that a randomly chosen student wears:\n\nA ring or a necklace\nBoth a ring and a necklace",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-4",
    "href": "homework/homework2.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nTwo fair dice are rolled. Find the conditional probability that at least one die lands on \\(6\\), given that they land on different numbers.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-5",
    "href": "homework/homework2.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nAn urn contains \\(6\\) white and \\(9\\) black balls. If \\(4\\) balls are selected without replacement, what is the probability that the first \\(2\\) are white and the last \\(2\\) are black?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-6",
    "href": "homework/homework2.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nA defendant is judged guilty if at least \\(2\\) out of \\(3\\) judges vote guilty. Given:\n\nProbability of a guilty vote when defendant is guilty: \\(0.7\\) \nProbability of a guilty vote when defendant is innocent: \\(0.2\\) \n\\(70%\\) of defendants are guilty\n\nCompute the conditional probability that judge \\(3\\) votes guilty given:\n\nJudges \\(1\\) and \\(2\\) vote guilty. \nJudges \\(1\\) and \\(2\\) split votes. \nJudges \\(1\\) and \\(2\\) vote not guilty.\n\nAre the judges‚Äô votes independent? Conditionally independent? Explain.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-7",
    "href": "homework/homework2.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nGiven the distribution function of \\(X\\):\n\\[\nF_{X}(\\lambda) =\n\\begin{cases}\n0, & \\lambda &lt; 0 \\\\\n\\frac{1}{2}, & 0 \\leq \\lambda &lt; 1 \\\\\n\\frac{3}{5}, & 1 \\leq \\lambda &lt; 2 \\\\\n\\frac{4}{5}, & 2 \\leq \\lambda &lt; 3 \\\\\n\\frac{9}{10}, & 3 \\leq \\lambda &lt; 3.5 \\\\\n1, & \\lambda \\geq 3.5 \\\\\n\\end{cases}\n\\]\nFind \\(p_X(\\lambda)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework2.html#question-8",
    "href": "homework/homework2.html#question-8",
    "title": "Homework 2",
    "section": "Question 8",
    "text": "Question 8\nA player rolls a fair die and flips a fair coin. If heads, they win twice the die value; if tails, they win half. Determine the expected winnings.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html",
    "href": "homework/homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Coding Exercise 1: Load Environments\nLoad existing Bernoulli and Gaussian environments from Utils.env using the default options and a random seed of 123.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-2-epsilon-greedy-recommendation-system",
    "href": "homework/homework3.html#coding-exercise-2-epsilon-greedy-recommendation-system",
    "title": "Homework 3",
    "section": "Coding Exercise 2: Epsilon-Greedy Recommendation System",
    "text": "Coding Exercise 2: Epsilon-Greedy Recommendation System\nUsing the existing Epsilon-Greedy (\\(\\epsilon = 0.10\\)) code, create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-3-ucb-algorithm-and-recommendation-system",
    "href": "homework/homework3.html#coding-exercise-3-ucb-algorithm-and-recommendation-system",
    "title": "Homework 3",
    "section": "Coding Exercise 3: UCB Algorithm and Recommendation System",
    "text": "Coding Exercise 3: UCB Algorithm and Recommendation System\nCode the Upper Confidence Boundary (UCB) algorithm and create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-4-thompson-sampling-algorithm-and-recommendation-system",
    "href": "homework/homework3.html#coding-exercise-4-thompson-sampling-algorithm-and-recommendation-system",
    "title": "Homework 3",
    "section": "Coding Exercise 4: Thompson Sampling Algorithm and Recommendation System",
    "text": "Coding Exercise 4: Thompson Sampling Algorithm and Recommendation System\nCode the Thompson Sampling algorithm and create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-5-mab-algorithm-performance-comparison",
    "href": "homework/homework3.html#coding-exercise-5-mab-algorithm-performance-comparison",
    "title": "Homework 3",
    "section": "Coding Exercise 5: MAB Algorithm Performance Comparison",
    "text": "Coding Exercise 5: MAB Algorithm Performance Comparison\nFor 10,000 recommendations:\n\nDoes Epsilon-Greedy (\\(\\epsilon = 0.10\\)) perform better in the Bernoulli or Gaussian environment?\nDoes UCB perform better in the Bernoulli or Gaussian environment?\nDoes Thompson Sampling perform better in the Bernoulli or Gaussian environment?\nWhich algorithm performs best in the Bernoulli environment?\nWhich algorithm performs best in the Gaussian environment?\n\nHint: Check the performance of each MAB by observing the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-6-random-seed-analysis",
    "href": "homework/homework3.html#coding-exercise-6-random-seed-analysis",
    "title": "Homework 3",
    "section": "Coding Exercise 6: Random Seed Analysis",
    "text": "Coding Exercise 6: Random Seed Analysis\nUsing random seeds 0-50, for 10,000 recommendations, do the algorithms perform the same?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-7-amazon-dataset-analysis",
    "href": "homework/homework3.html#coding-exercise-7-amazon-dataset-analysis",
    "title": "Homework 3",
    "section": "Coding Exercise 7: Amazon Dataset Analysis",
    "text": "Coding Exercise 7: Amazon Dataset Analysis\nFor the Amazon.csv dataset, repeat exercise Coding Exercise 6 and find the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-8-exp3-algorithm-and-recommendation-system",
    "href": "homework/homework3.html#coding-exercise-8-exp3-algorithm-and-recommendation-system",
    "title": "Homework 3",
    "section": "Coding Exercise 8: EXP3 Algorithm and Recommendation System",
    "text": "Coding Exercise 8: EXP3 Algorithm and Recommendation System\n\n\n\\begin{algorithm} \\caption{MAB EXP3} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets \\frac{1}{k}$ \\State $W(a) \\gets 1$ \\\\ \\For{$t$ in range($len(data)$)} \\State $Q(a) \\gets (1 - \\gamma) \\frac{W(a)}{\\sum_{i=1}^{k} W(a_{i})} + \\frac{\\gamma}{k}$ \\\\ \\State $A_t \\gets$ random choice with probabilities $Q(a)$ \\\\ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $\\hat{R_t} \\gets \\begin{cases} \\frac{R_t}{Q(A_t)} & \\text{if } A_t = a\\\\ 0 & \\text{else} \\end{cases}$ \\State $W(a) \\gets W(a) e^{\\frac{\\gamma \\hat{R_t}}{k}}$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework3.html#coding-exercise-9-gradient-method-algorithm-and-recommendation-system",
    "href": "homework/homework3.html#coding-exercise-9-gradient-method-algorithm-and-recommendation-system",
    "title": "Homework 3",
    "section": "Coding Exercise 9: Gradient Method Algorithm and Recommendation System",
    "text": "Coding Exercise 9: Gradient Method Algorithm and Recommendation System\n\n\n\\begin{algorithm} \\caption{MAB Gradient Method} \\begin{algorithmic} \\State Initialize, for $a = 1$ to $k$: \\State $Q(a) \\gets 0$ \\State $N(a) \\gets 0$ \\State $H(a) \\gets 0$ \\\\ \\For{$t$ in range($len(data)$)} \\State $\\pi(a) \\gets \\text{softmax}(H(a))$ \\State $A_t \\gets \\text{argmax}_a(\\pi(a))$ \\State $R_t \\gets \\text{bandit}(A_t)$ \\State $N(A_t) \\gets N(A_t) + 1$ \\State $Q(A_t) \\gets Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$ \\State $H(a) \\gets \\begin{cases} H(A_t) + \\alpha (R_t - Q(A_t)) (1 - \\pi(A_t)) & \\text{if } A_t = a\\\\ H(a) - \\alpha (R_t - Q(a)) \\pi(a) & \\text{else} \\end{cases}$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html",
    "href": "homework/homework4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Question 1\nIf the current state is \\(S_{t}\\), and actions are selected according to stochastic policy \\(\\pi\\), then what is the expectation of \\(R_{t+1}\\) in terms of \\(\\pi\\) and the four-argument function \\(p\\)?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#question-2",
    "href": "homework/homework4.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\nGive an equation for \\(v_{\\pi}\\) in terms of \\(q_{\\pi}\\) and \\(\\pi\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#question-3",
    "href": "homework/homework4.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\nGive an equation for \\(q_{\\pi}\\) in terms of \\(v_{\\pi}\\) and \\(\\pi\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#exercise-1",
    "href": "homework/homework4.html#exercise-1",
    "title": "Homework 4",
    "section": "Exercise 1",
    "text": "Exercise 1\nWith a discount factor of \\(\\gamma = 0.9\\), calculate \\(v(t)\\) when \\(t = 100\\) for the following Markov Chain:\nIMAGE\nRewards: - \\(R = 9\\) for self-loop on State 1 - \\(R = -7\\) for self-loop on State 2 - \\(R = 3\\) for transitions between states",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#exercise-2",
    "href": "homework/homework4.html#exercise-2",
    "title": "Homework 4",
    "section": "Exercise 2",
    "text": "Exercise 2\nWith a discount factor of \\(\\gamma = 0.9\\), calculate \\(v(t)\\) when \\(t = 100\\) for the following Markov Chain:\nIMAGE\nRewards: - Various rewards for different transitions as specified.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#exercise-3",
    "href": "homework/homework4.html#exercise-3",
    "title": "Homework 4",
    "section": "Exercise 3",
    "text": "Exercise 3\nWith a discount factor of \\(\\gamma = 0.9\\), code Iterative Policy Evaluation (Prediction) and Value Iteration algorithms for the following GridWorld MDP:\n\n\n\nGridWorld\n\n\nNote: The gray shaded areas are barriers. Moving into a barrier incurs a reward of \\(R = -1\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#coding-exercise-1",
    "href": "homework/homework4.html#coding-exercise-1",
    "title": "Homework 4",
    "section": "Coding Exercise 1",
    "text": "Coding Exercise 1\nWith a discount factor of \\(\\gamma = 0.9\\), calculate \\(v(t)\\) when \\(t = 100\\) for the following Markov Chain:",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#coding-exercise-2",
    "href": "homework/homework4.html#coding-exercise-2",
    "title": "Homework 4",
    "section": "Coding Exercise 2",
    "text": "Coding Exercise 2\nWith a discount factor of \\(\\gamma = 0.9\\), calculate \\(v(t)\\) when \\(t = 100\\) for the following Markov Chain:",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "homework/homework4.html#coding-exercise-3",
    "href": "homework/homework4.html#coding-exercise-3",
    "title": "Homework 4",
    "section": "Coding Exercise 3",
    "text": "Coding Exercise 3\nWith a discount factor of \\(\\gamma = 0.9\\), code Iterative Policy Evaluation (Prediction) and Value Iteration algorithms for the following GridWorld MDP:\n\n\n\n\n\nNote: The gray shaded areas are barriers. Moving into a barrier incurs a reward of \\(R = -1\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "lecture5/lo.html",
    "href": "lecture5/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 5: Monte Carlo üéØ\n\n\n\n\nMonte Carlo Prediction.\nOn-Policy Monte Carlo.\nOff-Policy Monte Carlo.\nHomemade GridWorld OpenAI environment using gymnasium, pygame & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html",
    "href": "lecture5/lecture5-1.html",
    "title": "5.1 Monte Carlo Prediction",
    "section": "",
    "text": "Monte Carlo\nMonte Carlo is a powerful learning rule for estimating value functions \\(v_{\\pi}\\) and action value functions \\(q_{\\pi}\\) in associative environments.\nThe power of Monte Carlo resides in its ability to learn the dynamics of any environment, without assuming any prior knowledge, only using experience.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#types-of-learning-rules",
    "href": "lecture5/lecture5-1.html#types-of-learning-rules",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Types of Learning Rules",
    "text": "Types of Learning Rules\nThere are two types of learning rules in Reinforcement Learning: on-policy and off-policy.\nOn-Policy methods evaluate or improve the policy that is used to make decisions.\nOff-Policy methods evaluate or improve a policy different from that used to generate the data.\nWe will cover both methods for Monte Carlo.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#monte-carlo-prediction",
    "href": "lecture5/lecture5-1.html#monte-carlo-prediction",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Monte Carlo Prediction",
    "text": "Monte Carlo Prediction\nMonte Carlo methods are based on averaging sample returns of trajectories following a policy \\(\\pi\\).\nRecall that returns are calculated as follows:\n\\[\nG_{t} = r_{t+1} + \\gamma G_{t+1}\n\\]\nOnly on the completion of an episode are value estimates \\(v_{\\pi}(s)\\) and action value estimates \\(q_{\\pi}(s,a)\\) updated.\n\n\n\n\n\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{Monte Carlo Prediction} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $V(s) \\gets 0$ for all $s \\in S$ \\State $Returns(s) \\gets \\{\\}$ for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\State \\textbf{Loop for each episode:} \\State Generate an episode following $\\pi$: $S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$S_{t}$ not in $\\{S_{0}, S_{1}, \\dots, S_{t-1}\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}) \\gets Returns(S_{t}) \\cup \\{G\\}$ \\State $V(S_{t}) \\gets \\text{average}(Returns(S_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}\n\n\n\n  \n\n\n\nEnvironment GridWorld\nAssume \\(\\gamma = 0.9\\)\nSuppose we followed the trajectory of \\(\\pi\\) for one episode:\n\n\n\n\n\nThe following illustrates a Monte Carlo update for the trajectory:",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#monte-carlo-prediction-pseudocode",
    "href": "lecture5/lecture5-1.html#monte-carlo-prediction-pseudocode",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Monte Carlo Prediction Pseudocode",
    "text": "Monte Carlo Prediction Pseudocode\n\n\n\\begin{algorithm} \\caption{Monte Carlo Prediction} \\begin{algorithmic} \\State Initialize: \\State $V(s) \\gets 0$, for all $s \\in S$ \\State $Returns(s) \\gets \\{\\}$, for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\\\~\\\\ \\State Loop for each episode: \\State \\hspace{0.3cm} Generate an episode following $\\pi$: $S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2},...,S_{T-1}, A_{T-1}, R_{T}$ \\State \\hspace{0.3cm} $G \\gets 0$ \\State \\hspace{0.3cm} \\textbf{for} $t$ in $T-1, T-2, ..., 0$ \\textbf{do} \\State \\hspace{0.6cm} $G \\gets \\gamma G + R_{t+1}$ \\State \\hspace{0.6cm} If $S_{t}$ not in $S_{0}, S_{1},...,S_{T-1}$: (first-visit check) \\State \\hspace{0.9cm} $Returns[S_{t}] \\gets G$ \\State \\hspace{0.9cm} $V(S_{t}) \\gets average(Returns[S_{t}])$ \\State \\hspace{0.3cm} \\textbf{end for} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#example-gridworld",
    "href": "lecture5/lecture5-1.html#example-gridworld",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Example: Gridworld",
    "text": "Example: Gridworld\nAssume \\(\\gamma = 0.9\\)\nSuppose we followed the trajectory of \\(\\pi\\) for one episode:\n\n\n\n\n\nThe following illustrates a Monte Carlo update for the trajectory:",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#estimation-of-action-values",
    "href": "lecture5/lecture5-1.html#estimation-of-action-values",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Estimation of Action Values",
    "text": "Estimation of Action Values\nWithout a model, state values \\(v_{\\pi}(s)\\) alone are not sufficient.\nWe must explicitly estimate the value of each action \\(q_{\\pi}(s,a)\\).\nMonte Carlo methods for this are similar to state value estimation, focusing on visits to state-action pairs \\((s,a)\\).",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#monte-carlo-control",
    "href": "lecture5/lecture5-1.html#monte-carlo-control",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Monte Carlo Control",
    "text": "Monte Carlo Control\nThe main advantage of estimating action values \\(q_{\\pi}(s,a)\\) in Monte Carlo methods lies in Control, which refers to finding approximate optimal policies \\(\\pi_{*}\\).\nProceeding with the idea of Generalized Policy Iteration (GPI), we evaluate and improve action values \\(q_{\\pi}(s,a)\\) to find optimal policies.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#exploring-starts",
    "href": "lecture5/lecture5-1.html#exploring-starts",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Exploring Starts",
    "text": "Exploring Starts\nThe only problem of estimating action values \\(q_{\\pi}(s,a)\\) is that some state action pairs \\((s,a)\\) may never be visited during an episode.\nWhich brings us back to the same dilemma we faced in the Multi-Armed Bandit chapter, that is:\nBalancing exploration and exploitation.\nOne ‚Äúquick-fix‚Äù is to start each episode from a random state \\(s\\) and take any action \\(a\\) with probability greater than \\(0\\).\nThis ‚Äúquick-fix‚Äù is referred to as .",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#monte-carlo-exploring-starts-pseudocode",
    "href": "lecture5/lecture5-1.html#monte-carlo-exploring-starts-pseudocode",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Monte Carlo Exploring Starts Pseudocode",
    "text": "Monte Carlo Exploring Starts Pseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html",
    "href": "lecture5/lecture5-2.html",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "",
    "text": "Monte Carlo Control\nWithout a model, state values \\(v_{\\pi}(s)\\) alone are not sufficient.\nWe must explicitly estimate the value of each action \\(q_{\\pi}(s,a)\\).\nMonte Carlo methods for this are similar to state value estimation, focusing on visits to state-action pairs \\((s,a)\\).\nThe main advantage of estimating action values \\(q_{\\pi}(s,a)\\) in Monte Carlo methods lies in Control, which refers to finding approximate optimal policies \\(\\pi_{*}\\).\nProceeding with the idea of Generalized Policy Iteration (GPI), we evaluate and improve action values \\(q_{\\pi}(s,a)\\) to find optimal policies.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.2 Exploring Starts Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#exploration-without-an-initial-random-state-and-action",
    "href": "lecture5/lecture5-2.html#exploration-without-an-initial-random-state-and-action",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "",
    "text": "How can we explore without having to rely on the unrealistic assumption of an initial random state and action?\n\n\n\nRecall, \\(\\epsilon\\)-greedy methods for balancing exploration vs.¬†exploitation in Multi-Armed Bandits.\n\n\n\n\nThese policies are usually referred to as \\(\\epsilon\\)-soft policies as they require that the probability of every action is non-zero for all states and actions pairs, that is:",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.2 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#epsilon-greedy",
    "href": "lecture5/lecture5-2.html#epsilon-greedy",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "\\(\\epsilon\\)-Greedy",
    "text": "\\(\\epsilon\\)-Greedy\n\nTo calculate the probabilities of selecting an action according to the \\(\\epsilon\\)-greedy policy \\(\\pi(a|s)\\), we use the following update rule:\n\n\\[\n\\pi(a|s) \\gets \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|A(S_{t})|}  & \\text{if} \\quad a = A_{t} \\quad \\text{(exploitation)} \\\\\n\\frac{\\epsilon}{|A(S_{t})|} & \\text{if} \\quad a \\neq A_{t} \\quad \\text{(exploration)}\n\\end{cases}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.2 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#on-policy-pseudocode",
    "href": "lecture5/lecture5-2.html#on-policy-pseudocode",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "On-Policy Pseudocode",
    "text": "On-Policy Pseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.2 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html",
    "href": "lecture5/lecture5-3.html",
    "title": "5.3 On-Policy Monte Carlo",
    "section": "",
    "text": "Exploration without an Initial Random State and Action\nHow can we explore without having to rely on the unrealistic assumption of an initial random state and action?\n\\[\n\\pi(a|s) &gt; 0 \\quad \\text{for all} \\quad s \\in S \\quad \\text{and} \\quad a \\in A(s)\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>5.3 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#off-policy",
    "href": "lecture5/lecture5-3.html#off-policy",
    "title": "5.3 Off-Policy Monte Carlo",
    "section": "",
    "text": "A more powerful and general approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior.\nA target policy, denoted \\(\\pi\\), is the policy being learned.\nA behavior policy, denoted \\(b\\), is the policy used to generate behavior.\nIn this case, we say that learning is from data ‚Äúoff‚Äù the target policy, and the overall process is termed off-policy learning.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.3 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#importance-sampling",
    "href": "lecture5/lecture5-3.html#importance-sampling",
    "title": "5.3 Off-Policy Monte Carlo",
    "section": "Importance Sampling",
    "text": "Importance Sampling\n\nImportance Sampling is a general technique for estimating expected values under one distribution given samples from another.\nWe apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio.\n\n\\[\n\\text{Pr}\\{A_{t}, S_{t+1}, A_{t+1}, \\dots , S_{T} \\mid S_{t}, A_{t:T-1} \\sim \\pi \\} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.3 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#incremental-implementation",
    "href": "lecture5/lecture5-3.html#incremental-implementation",
    "title": "5.3 Off-Policy Monte Carlo",
    "section": "Incremental Implementation",
    "text": "Incremental Implementation\n\nSimilarly to the Multi-Armed Bandits chapter, action values \\(q_{\\pi}(s,a)\\) can be calculated incrementally.\nIn order to do so, we must first begin by calculating a cumulative sum of the weights:\n\n\\[\nC(S_{t},A_{t}) = C(S_{t},A_{t}) + W\n\\]\n\nThen, we average returns of corresponding action values:\n\n\\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\frac{W}{C(S_{t},A_{t})}[G - Q(S_{t},A_{t})]\n\\]\n\nFinally, we update the weight according to our importance sampling ratio:\n\n\\[\nW = W \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.3 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#off-policy-pseudocode",
    "href": "lecture5/lecture5-3.html#off-policy-pseudocode",
    "title": "5.3 Off-Policy Monte Carlo",
    "section": "Off-Policy Pseudocode",
    "text": "Off-Policy Pseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.3 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#off-policy-control",
    "href": "lecture5/lecture5-3.html#off-policy-control",
    "title": "5.3 Off-Policy Monte Carlo",
    "section": "Off-Policy Control",
    "text": "Off-Policy Control\n\nWe can assure Off-Policy methods to achieve control by choosing \\(b\\) to be \\(\\epsilon\\)-soft.\nThe target policy \\(\\pi\\) converges to optimal at all encountered states even though actions are selected according to a different soft policy \\(b\\), which may change between or even within episodes.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.3 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#off-policy-control-pseudocode",
    "href": "lecture5/lecture5-3.html#off-policy-control-pseudocode",
    "title": "5.3 Off-Policy Monte Carlo",
    "section": "Off-Policy Control Pseudocode",
    "text": "Off-Policy Control Pseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.3 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "homework/homework5.html",
    "href": "homework/homework5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Coding Exercise 1\nFor the hw_env environment, code the On-Policy Monte Carlo Control algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#monte-carlo-control",
    "href": "lecture5/lecture5-2.html#monte-carlo-control",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "Monte Carlo Control",
    "text": "Monte Carlo Control\nThe main advantage of estimating action values \\(q_{\\pi}(s,a)\\) in Monte Carlo methods lies in Control, which refers to finding approximate optimal policies \\(\\pi_{*}\\).\nProceeding with the idea of Generalized Policy Iteration (GPI), we evaluate and improve action values \\(q_{\\pi}(s,a)\\) to find optimal policies.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.2 Exploring Starts Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#exploring-starts",
    "href": "lecture5/lecture5-2.html#exploring-starts",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "Exploring Starts",
    "text": "Exploring Starts\nThe only problem of estimating action values \\(q_{\\pi}(s,a)\\) is that some state action pairs \\((s,a)\\) may never be visited during an episode.\nWhich brings us back to the same dilemma we faced in the Multi-Armed Bandit chapter, that is:\nBalancing exploration and exploitation.\nOne ‚Äúquick-fix‚Äù is to start each episode from a random state \\(s\\) and take any action \\(a\\) with probability greater than \\(0\\).\nThis ‚Äúquick-fix‚Äù is referred to as Exploring Starts.\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{Monte Carlo Exploring Starts} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Returns(s, a) \\gets \\{\\}$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State \\textbf{Loop for each episode:} \\State Choose $S_{0}$ and $A_{0}$ randomly with probability $&gt; 0$ \\State Generate episode following $\\pi$: $S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$(S_{t}, A_{t})$ not in $\\{(S_{0}, A_{0}), (S_{1}, A_{1}), \\dots, (S_{t-1}, A_{t-1})\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}, A_{t}) \\gets Returns(S_{t}, A_{t}) \\cup \\{G\\}$ \\State $Q(S_{t}, A_{t}) \\gets \\text{average}(Returns(S_{t}, A_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.2 Exploring Starts Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#monte-carlo-exploring-starts-pseudocode",
    "href": "lecture5/lecture5-2.html#monte-carlo-exploring-starts-pseudocode",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "Monte Carlo Exploring Starts Pseudocode",
    "text": "Monte Carlo Exploring Starts Pseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.2 Exploring Starts Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#exploration-without-an-initial-random-state-and-action",
    "href": "lecture5/lecture5-3.html#exploration-without-an-initial-random-state-and-action",
    "title": "5.3 On-Policy Monte Carlo",
    "section": "",
    "text": "Recall, \\(\\epsilon\\)-greedy methods for balancing exploration vs.¬†exploitation in Multi-Armed Bandits.\n\n\nThese policies are usually referred to as \\(\\epsilon\\)-soft policies as they require that the probability of every action is non-zero for all states and actions pairs, that is:",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>5.3 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#epsilon-greedy",
    "href": "lecture5/lecture5-3.html#epsilon-greedy",
    "title": "5.3 On-Policy Monte Carlo",
    "section": "\\(\\epsilon\\)-Greedy",
    "text": "\\(\\epsilon\\)-Greedy\nTo calculate the probabilities of selecting an action according to the \\(\\epsilon\\)-greedy policy \\(\\pi(a|s)\\), we use the following update rule:\n\\[\n\\pi(a|s) \\gets \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|A(S_{t})|}  & \\text{if} \\quad a = A_{t} \\quad \\text{(exploitation)} \\\\\n\\frac{\\epsilon}{|A(S_{t})|} & \\text{if} \\quad a \\neq A_{t} \\quad \\text{(exploration)}\n\\end{cases}\n\\]\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{On-Policy Monte Carlo Control} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Returns(s, a) \\gets \\{\\}$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\epsilon \\in (0, 1]$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Generate episode following $\\pi$: $S_{0}, A_{0}, R_{1}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$(S_{t}, A_{t})$ not in $\\{(S_{0}, A_{0}), (S_{1}, A_{1}), \\dots, (S_{t-1}, A_{t-1})\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}, A_{t}) \\gets Returns(S_{t}, A_{t}) \\cup \\{G\\}$ \\State $Q(S_{t}, A_{t}) \\gets \\text{average}(Returns(S_{t}, A_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>5.3 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#on-policy-pseudocode",
    "href": "lecture5/lecture5-3.html#on-policy-pseudocode",
    "title": "5.3 On-Policy Monte Carlo",
    "section": "On-Policy Pseudocode",
    "text": "On-Policy Pseudocode\n\n\n\\begin{algorithm} \\caption{On-Policy Monte Carlo Control} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Returns(s, a) \\gets \\{\\}$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\epsilon \\in (0, 1]$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Generate episode following $\\pi$: $S_{0}, A_{0}, R_{1}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$(S_{t}, A_{t})$ not in $\\{(S_{0}, A_{0}), (S_{1}, A_{1}), \\dots, (S_{t-1}, A_{t-1})\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}, A_{t}) \\gets Returns(S_{t}, A_{t}) \\cup \\{G\\}$ \\State $Q(S_{t}, A_{t}) \\gets \\text{average}(Returns(S_{t}, A_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>5.3 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html",
    "href": "lecture5/lecture5-4.html",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "",
    "text": "Importance Sampling\nImportance Sampling is a general technique for estimating expected values under one distribution given samples from another.\nWe apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio.\n\\[\n\\text{Pr}\\{A_{t}, S_{t+1}, A_{t+1}, \\dots , S_{T} \\mid S_{t}, A_{t:T-1} \\sim \\pi \\} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html#off-policy",
    "href": "lecture5/lecture5-4.html#off-policy",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "",
    "text": "A more powerful and general approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior.\nA target policy, denoted \\(\\pi\\), is the policy being learned.\nA behavior policy, denoted \\(b\\), is the policy used to generate behavior.\nIn this case, we say that learning is from data ‚Äúoff‚Äù the target policy, and the overall process is termed off-policy learning.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html#importance-sampling",
    "href": "lecture5/lecture5-4.html#importance-sampling",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "Importance Sampling",
    "text": "Importance Sampling\n\nImportance Sampling is a general technique for estimating expected values under one distribution given samples from another.\nWe apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio.\n\n\\[\n\\text{Pr}\\{A_{t}, S_{t+1}, A_{t+1}, \\dots , S_{T} \\mid S_{t}, A_{t:T-1} \\sim \\pi \\} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html#incremental-implementation",
    "href": "lecture5/lecture5-4.html#incremental-implementation",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "Incremental Implementation",
    "text": "Incremental Implementation\nSimilarly to the Multi-Armed Bandits chapter, action values \\(q_{\\pi}(s,a)\\) can be calculated incrementally.\nIn order to do so, we must first begin by calculating a cumulative sum of the weights:\n\\[\nC(S_{t},A_{t}) = C(S_{t},A_{t}) + W\n\\]\nThen, we average returns of corresponding action values:\n\\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\frac{W}{C(S_{t},A_{t})}[G - Q(S_{t},A_{t})]\n\\]\nFinally, we update the weight according to our importance sampling ratio:\n\\[\nW = W \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html#off-policy-pseudocode",
    "href": "lecture5/lecture5-4.html#off-policy-pseudocode",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "Off-Policy Pseudocode",
    "text": "Off-Policy Pseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html#off-policy-control",
    "href": "lecture5/lecture5-4.html#off-policy-control",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "Off-Policy Control",
    "text": "Off-Policy Control\nWe can assure Off-Policy methods to achieve control by choosing \\(b\\) to be \\(\\epsilon\\)-soft.\nThe target policy \\(\\pi\\) converges to optimal at all encountered states even though actions are selected according to a different soft policy \\(b\\), which may change between or even within episodes.\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{Off-Policy Monte Carlo Control} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $C(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $b \\gets$ any soft policy \\State \\textbf{Loop forever (for each episode):} \\State Generate episode following $b$: $S_{0}, A_{0}, R_{1}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\State $W \\gets 1$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\State $C(S_{t}, A_{t}) \\gets C(S_{t}, A_{t}) + W$ \\State $Q(S_{t}, A_{t}) \\gets Q(S_{t}, A_{t}) + \\frac{W}{C(S_{t}, A_{t})} [G - Q(S_{t}, A_{t})]$ \\If{$A_{t} \\neq \\pi(S_{t})$} \\State \\textbf{break} \\Endif \\State $W \\gets W \\cdot \\frac{1}{b(A_{t} \\mid S_{t})}$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-4.html#off-policy-control-pseudocode",
    "href": "lecture5/lecture5-4.html#off-policy-control-pseudocode",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "Off-Policy Control Pseudocode",
    "text": "Off-Policy Control Pseudocode\n\n\n\\begin{algorithm} \\caption{Off-Policy Monte Carlo Control} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $C(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $b \\gets$ any soft policy \\State \\textbf{Loop forever (for each episode):} \\State Generate episode following $b$: $S_{0}, A_{0}, R_{1}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\State $W \\gets 1$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\State $C(S_{t}, A_{t}) \\gets C(S_{t}, A_{t}) + W$ \\State $Q(S_{t}, A_{t}) \\gets Q(S_{t}, A_{t}) + \\frac{W}{C(S_{t}, A_{t})} [G - Q(S_{t}, A_{t})]$ \\If{$A_{t} \\neq \\pi(S_{t})$} \\State \\textbf{break} \\Endif \\State $W \\gets W \\cdot \\frac{1}{b(A_{t} \\mid S_{t})}$ \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "homework/homework5.html#coding-exercise-2",
    "href": "homework/homework5.html#coding-exercise-2",
    "title": "Homework 5",
    "section": "Coding Exercise 2",
    "text": "Coding Exercise 2\nFor the hw_env environment, code the Off-Policy Monte Carlo Control algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "lecture2/lo.html",
    "href": "lecture2/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 2: Mathematical Foundations üéØ\n\n\n\n\nProbability Theory:\n\nSet Theory\nAxiomatic Probability\nConditioning\nIndependence\nDiscrete Random Variables\nContinuous Random Variables\nProbability Distributions\n\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 2: Mathematical Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#pseudocode",
    "href": "lecture5/lecture5-1.html#pseudocode",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{Monte Carlo Prediction} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $V(s) \\gets 0$ for all $s \\in S$ \\State $Returns(s) \\gets \\{\\}$ for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\State \\textbf{Loop for each episode:} \\State Generate an episode following $\\pi$: $S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$S_{t}$ not in $\\{S_{0}, S_{1}, \\dots, S_{t-1}\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}) \\gets Returns(S_{t}) \\cup \\{G\\}$ \\State $V(S_{t}) \\gets \\text{average}(Returns(S_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}\n\n\n\n  \n\n\nEnvironment GridWorld\nAssume \\(\\gamma = 0.9\\)\nSuppose we followed the trajectory of \\(\\pi\\) for one episode:\n\n\n\n\n\nThe following illustrates a Monte Carlo update for the trajectory:",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-1.html#environment-gridworld",
    "href": "lecture5/lecture5-1.html#environment-gridworld",
    "title": "5.1 Monte Carlo Prediction",
    "section": "Environment GridWorld",
    "text": "Environment GridWorld\nAssume \\(\\gamma = 0.9\\)\nSuppose we followed the trajectory of \\(\\pi\\) for one episode:\n\n\n\n\n\nThe following illustrates a Monte Carlo update for the trajectory:",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-2.html#pseudocode",
    "href": "lecture5/lecture5-2.html#pseudocode",
    "title": "5.2 On-Policy Monte Carlo",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{Monte Carlo Exploring Starts} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Returns(s, a) \\gets \\{\\}$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State \\textbf{Loop for each episode:} \\State Choose $S_{0}$ and $A_{0}$ randomly with probability $&gt; 0$ \\State Generate episode following $\\pi$: $S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$(S_{t}, A_{t})$ not in $\\{(S_{0}, A_{0}), (S_{1}, A_{1}), \\dots, (S_{t-1}, A_{t-1})\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}, A_{t}) \\gets Returns(S_{t}, A_{t}) \\cup \\{G\\}$ \\State $Q(S_{t}, A_{t}) \\gets \\text{average}(Returns(S_{t}, A_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>5.2 Exploring Starts Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture5/lecture5-3.html#pseudocode",
    "href": "lecture5/lecture5-3.html#pseudocode",
    "title": "5.3 On-Policy Monte Carlo",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{On-Policy Monte Carlo Control} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Returns(s, a) \\gets \\{\\}$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\epsilon \\in (0, 1]$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Generate episode following $\\pi$: $S_{0}, A_{0}, R_{1}, \\dots, S_{T-1}, A_{T-1}, R_{T}$ \\State $G \\gets 0$ \\For{$t = T-1, T-2, \\dots, 0$} \\State $G \\gets \\gamma G + R_{t+1}$ \\If{$(S_{t}, A_{t})$ not in $\\{(S_{0}, A_{0}), (S_{1}, A_{1}), \\dots, (S_{t-1}, A_{t-1})\\}$} \\Comment{First-visit check} \\State $Returns(S_{t}, A_{t}) \\gets Returns(S_{t}, A_{t}) \\cup \\{G\\}$ \\State $Q(S_{t}, A_{t}) \\gets \\text{average}(Returns(S_{t}, A_{t}))$ \\Endif \\Endfor \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>5.3 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lecture6/lo.html",
    "href": "lecture6/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 6: Temporal Difference üéØ\n\n\n\n\nTemporal Difference (TD) Prediction.\nSARSA.\nQ-Learning.\nDouble Q-Learning.\n(Optional) n-step Bootstrapping.\nHomemade GridWorld OpenAI environment using gymnasium, pygame & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-1.html",
    "href": "lecture6/lecture6-1.html",
    "title": "6.1 Temporal Difference (TD) Prediction",
    "section": "",
    "text": "TD Prediction\nTemporal Difference (TD) is a learning rule that is a combination of Monte Carlo and Dynamic Programming ideas.\nTD methods at time \\(t + 1\\) immediately form a target and make a useful update using the observed reward \\(R_{t+1}\\) and the estimate \\(V_(S_{t+1})\\) in a incremental fashion:\n\\[\nV(S_{t}) = V(S_{t}) + \\underbrace{\\alpha}_{Step \\ Size} [ \\underbrace{\\underbrace{R_{t+1} + \\gamma V(S_{t+1})}_{Target \\ Update} - V(S_{t})}_{TD \\  Error}]\n\\]",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>6.1 Temporal Difference (TD) Prediction</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-2.html",
    "href": "lecture6/lecture6-2.html",
    "title": "6.2 SARSA",
    "section": "",
    "text": "SARSA\nTD on-policy method we must estimate \\(q_{\\pi}(s, a)\\) for the current behavior policy \\(\\pi\\) and for all states \\(s \\in S\\) and actions \\(a \\in A(S)\\)\nThe theorems assuring the convergence of state values under TD Prediction also apply to the corresponding algorithm for action values: \\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\alpha [ R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t})]\n\\]\nThis rule uses every element of the quintuple of events (\\(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1}\\))",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>6.2 SARSA</span>"
    ]
  },
  {
    "objectID": "homework/homework6.html",
    "href": "homework/homework6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Coding Exercise 1\nFor the hw\\_env environment code the SARSA algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Homework 6</span>"
    ]
  },
  {
    "objectID": "homework/homework6.html#coding-exercise-2",
    "href": "homework/homework6.html#coding-exercise-2",
    "title": "Homework 6",
    "section": "Coding Exercise 2",
    "text": "Coding Exercise 2\nFor the hw\\_env environment code the Q-learning algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Homework 6</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-1.html#overview",
    "href": "lecture6/lecture6-1.html#overview",
    "title": "6.1 Temporal Difference (TD) Prediction",
    "section": "Overview",
    "text": "Overview\n\nTD methods, like Monte Carlo, learn from experience by updating estimates of nonterminal states along a trajectory \\(\\pi\\).\nTD methods, like Dynamic Programming, update based on an existing estimate \\(V(S_{t+1})\\).\n\nAt time \\(t + 1\\), TD methods update their estimate incrementally:\n\\[\nV(S_{t}) = V(S_{t}) + \\underbrace{\\alpha}_{\\text{Step Size}} \\left[ \\underbrace{\\underbrace{R_{t+1} + \\gamma V(S_{t+1})}_{\\text{Target Update}} - V(S_{t})}_{\\text{TD Error}} \\right]\n\\]",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>6.1 Temporal Difference (TD) Prediction</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-1.html#pseudocode",
    "href": "lecture6/lecture6-1.html#pseudocode",
    "title": "6.1 Temporal Difference (TD) Prediction",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD Prediction} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $V(s) \\gets 0$ for all $s \\in S$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\Repeat \\State $A_{t} \\gets$ action given by $\\pi$ for $S_{t}$ \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State $V(S_{t}) \\gets V(S_{t}) + \\alpha \\left[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})\\right]$ \\State $S_{t} \\gets S_{t+1}$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}\n\n\n\n  \n\n\nEnvironment GridWorld\nAssume \\(\\gamma = 0.9\\)\nSuppose we follow the trajectory of \\(\\pi\\) for one episode:",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>6.1 Temporal Difference (TD) Prediction</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-2.html#sarsa",
    "href": "lecture6/lecture6-2.html#sarsa",
    "title": "6.2 SARSA",
    "section": "",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD SARSA} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State $\\epsilon &gt; 0$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\State Choose $A_{0}$ from $S_{0}$ using $\\pi$ \\Repeat \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State Choose $A_{t+1}$ from $S_{t+1}$ using $\\pi$ \\State $Q(S_{t}, A_{t}) \\gets Q(S_{t}, A_{t}) + \\alpha \\left[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t}, A_{t})\\right]$ \\State $S_{t} \\gets S_{t+1}$ \\State $A_{t} \\gets A_{t+1}$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>6.2 SARSA</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-3.html",
    "href": "lecture6/lecture6-3.html",
    "title": "6.3 Q-Learning",
    "section": "",
    "text": "Q-Learning\nTD off-policy method we can both update and estimate the optimal action-value function directly:\n\\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\alpha [ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1},a) - Q(S_{t},A_{t})]\n\\]\nQ-Learning is a breakthrough in reinforcement learning due to its simplification of algorithm analysis and early convergence proofs.",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>6.3 Q-Learning</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-4.html",
    "href": "lecture6/lecture6-4.html",
    "title": "6.4 Double Q-Learning",
    "section": "",
    "text": "Double Q-Learning\nOne of the drawbacks of Q-Learning is the maximization bias, where maximization of action value estimates \\(Q(s,a)\\) is higher than those of the true action values \\(q(s,a)\\), leading to a bias.\nDouble Q-Learning addresses this bias by creating two action value estimates \\(Q_{1}(s,a)\\) and \\(Q_{2}(s,a)\\).\nWith equal likelihood, one action value estimate yields the maximization action \\(A_{t}\\) and the other provides the action value estimate \\(Q(S_{t}, A_{t})\\).\n\\[\nQ_{1}(S_{t},A_{t}) = Q_{1}(S_{t},A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{2}(S_{t+1},\\max_{a} Q_{1}(S_{t+1},a)) - Q_{1}(S_{t},A_{t})]\n\\]\n\\[\nQ_{2}(S_{t},A_{t}) = Q_{2}(S_{t},A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{1}(S_{t+1},\\max_{a} Q_{2}(S_{t+1},a)) - Q_{2}(S_{t},A_{t})]\n\\]",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>32</span>¬† <span class='chapter-title'>6.4 Double Q-Learning</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html",
    "href": "lecture6/lecture6-5.html",
    "title": "6.5 (Optional) n-step Bootstrapping",
    "section": "",
    "text": "n-step TD Prediction\nn-step Bootstrapping is a learning rule that is a combination of Monte Carlo and Temporal Difference ideas.",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html#n-step-sarsa",
    "href": "lecture6/lecture6-5.html#n-step-sarsa",
    "title": "6.5 (Optional) n-step Bootstrapping",
    "section": "n-step SARSA",
    "text": "n-step SARSA\nn-step SARSA extends the standard SARSA algorithm to incorporate multi-step returns. Instead of updating based on a single-step transition, it utilizes an accumulated return over n steps, striking a balance between bias and variance.\nUses an n-step return to update the action-value function.\nEnsures learning from actual experience while incorporating bootstrapping.\nBalances exploration and exploitation more effectively than 1-step SARSA.\n\n\n\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html#n-step-tree-backup",
    "href": "lecture6/lecture6-5.html#n-step-tree-backup",
    "title": "6.5 (Optional) n-step Bootstrapping",
    "section": "n-step Tree Backup",
    "text": "n-step Tree Backup\nn-step Tree Backup is an extension of Q-learning that allows updates without the requirement of selecting an on-policy action. It generalizes the Expected SARSA algorithm by propagating multiple steps of information while weighting future actions by their probability under the policy.\nEliminates the need for importance sampling in off-policy learning.\nUpdates the action-value function based on expected future returns.\nIncorporates multiple time steps, making it more stable in some environments.\n\n\n\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-1.html#td-prediction",
    "href": "lecture6/lecture6-1.html#td-prediction",
    "title": "6.1 Temporal Difference (TD) Prediction",
    "section": "",
    "text": "TD methods, like Monte Carlo, learn from experience by updating estimates of nonterminal states along a trajectory \\(\\pi\\).\nTD methods, like Dynamic Programming, update based on an existing estimate \\(V(S_{t+1})\\).\n\n\n\n\n\n\n\n\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{TD Prediction} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $V(s) \\gets 0$ for all $s \\in S$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\Repeat \\State $A_{t} \\gets$ action given by $\\pi$ for $S_{t}$ \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State $V(S_{t}) \\gets V(S_{t}) + \\alpha \\left[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})\\right]$ \\State $S_{t} \\gets S_{t+1}$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}\n\n\n\n  \n\n\n\nEnvironment GridWorld\nAssume \\(\\gamma = 0.9\\)\nSuppose we follow the trajectory of \\(\\pi\\) for one episode:",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>6.1 Temporal Difference (TD) Prediction</span>"
    ]
  },
  {
    "objectID": "homework/homework6.html#coding-exercise-3",
    "href": "homework/homework6.html#coding-exercise-3",
    "title": "Homework 6",
    "section": "Coding Exercise 3",
    "text": "Coding Exercise 3\nFor the hw\\_env environment code the Double Q-learning algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Homework 6</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-2.html#pseudocode",
    "href": "lecture6/lecture6-2.html#pseudocode",
    "title": "6.2 SARSA",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD SARSA} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State $\\epsilon &gt; 0$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\State Choose $A_{0}$ from $S_{0}$ using $\\pi$ \\Repeat \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State Choose $A_{t+1}$ from $S_{t+1}$ using $\\pi$ \\State $Q(S_{t}, A_{t}) \\gets Q(S_{t}, A_{t}) + \\alpha \\left[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t}, A_{t})\\right]$ \\State $S_{t} \\gets S_{t+1}$ \\State $A_{t} \\gets A_{t+1}$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>6.2 SARSA</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-3.html#pseudocode",
    "href": "lecture6/lecture6-3.html#pseudocode",
    "title": "6.3 Q-Learning",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD Q-Learning} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State $\\epsilon &gt; 0$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\Repeat \\State Choose $A_{t}$ from $S_{t}$ using $\\pi$ \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State $Q(S_{t}, A_{t}) \\gets Q(S_{t}, A_{t}) + \\alpha \\left[R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_{t}, A_{t})\\right]$ \\State $S_{t} \\gets S_{t+1}$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>6.3 Q-Learning</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-4.html#pseudocode",
    "href": "lecture6/lecture6-4.html#pseudocode",
    "title": "6.4 Double Q-Learning",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD Double Q-Learning} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q_{1}(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Q_{2}(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State $\\epsilon &gt; 0$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\Repeat \\State Choose $A_{t}$ from $S_{t}$ using $\\pi$ \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State With $0.5$ probability: \\State $Q_{1}(S_{t}, A_{t}) \\gets Q_{1}(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{2}(S_{t+1}, \\arg\\max_{a} Q_{1}(S_{t+1}, a)) - Q_{1}(S_{t}, A_{t})]$ \\State \\textbf{Else:} \\State $Q_{2}(S_{t}, A_{t}) \\gets Q_{2}(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{1}(S_{t+1}, \\arg\\max_{a} Q_{2}(S_{t+1}, a)) - Q_{2}(S_{t}, A_{t})]$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>32</span>¬† <span class='chapter-title'>6.4 Double Q-Learning</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html#n-step-td-prediction",
    "href": "lecture6/lecture6-5.html#n-step-td-prediction",
    "title": "6.5 (Optional) n-step Bootstrapping",
    "section": "",
    "text": "Like Monte Carlo, n-step methods learn from experience.\nLike Temporal Difference, n-step methods bootstrap multiple time steps.\n\n\n\n\n\n\n\nPseudocode\n\n\n\\begin{algorithm} \\caption{n-step TD Prediction} \\begin{algorithmic} \\State Initialize: \\State $V(s) \\gets 0$, for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\State $\\alpha \\in (0,1]$ \\State Loop for each episode: \\State Initialize $S_{0}$ \\State $T \\gets \\infty$ \\State Loop for each step $t = 0,...,T-2,T-1$ in episode: \\State If $t &lt; T$: \\State $A_{t} \\gets$ action given by $\\pi$ for $S_{t}$ \\State Take action $A_{t}$, observe $R_{t+1}$, $S_{t+1}$ \\State If $S_{t+1} = S_{T-1}$: \\State $T \\gets t + 1$ \\State $\\tau \\gets t - n + 1$ \\State If $\\tau \\geq 0$: \\State $G \\gets \\sum^{min(\\tau+n, T)}_{i=\\tau+1} \\gamma^{i-\\tau-1}R_{i}$ \\State If $\\tau + n &lt; T$: \\State $G \\gets G + \\gamma^{n}V(S_{\\tau+n})$ \\State $V(S_{\\tau}) \\gets V(S_{\\tau}) + \\alpha [G - V(S_{\\tau})]$ \\State Until $\\tau = T-1$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html#pseudocode",
    "href": "lecture6/lecture6-5.html#pseudocode",
    "title": "6.5 n-step Bootstrapping",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{n-step TD Prediction} \\begin{algorithmic} \\State Initialize: \\State $V(s) \\gets 0$, for all $s \\in S$ \\State $\\gamma \\in [0,1)$ \\State $\\alpha \\in (0,1]$ \\State Loop for each episode: \\State Initialize $S_{0}$ \\State $T \\gets \\infty$ \\State Loop for each step $t = 0,...,T-2,T-1$ in episode: \\State If $t &lt; T$: \\State $A_{t} \\gets$ action given by $\\pi$ for $S_{t}$ \\State Take action $A_{t}$, observe $R_{t+1}$, $S_{t+1}$ \\State If $S_{t+1} = S_{T-1}$: \\State $T \\gets t + 1$ \\State $\\tau \\gets t - n + 1$ \\State If $\\tau \\geq 0$: \\State $G \\gets \\sum^{min(\\tau+n, T)}_{i=\\tau+1} \\gamma^{i-\\tau-1}R_{i}$ \\State If $\\tau + n &lt; T$: \\State $G \\gets G + \\gamma^{n}V(S_{\\tau+n})$ \\State $V(S_{\\tau}) \\gets V(S_{\\tau}) + \\alpha [G - V(S_{\\tau})]$ \\State Until $\\tau = T-1$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html#pseudocode-1",
    "href": "lecture6/lecture6-5.html#pseudocode-1",
    "title": "6.5 n-step Bootstrapping",
    "section": "Pseudocode",
    "text": "Pseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-5.html#pseudocode-2",
    "href": "lecture6/lecture6-5.html#pseudocode-2",
    "title": "6.5 n-step Bootstrapping",
    "section": "Pseudocode",
    "text": "Pseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-3.html#q-learning",
    "href": "lecture6/lecture6-3.html#q-learning",
    "title": "6.3 Q-Learning",
    "section": "",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD Q-Learning} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State $\\epsilon &gt; 0$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\Repeat \\State Choose $A_{t}$ from $S_{t}$ using $\\pi$ \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State $Q(S_{t}, A_{t}) \\gets Q(S_{t}, A_{t}) + \\alpha \\left[R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_{t}, A_{t})\\right]$ \\State $S_{t} \\gets S_{t+1}$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>6.3 Q-Learning</span>"
    ]
  },
  {
    "objectID": "lecture6/lecture6-4.html#double-q-learning",
    "href": "lecture6/lecture6-4.html#double-q-learning",
    "title": "6.4 Double Q-Learning",
    "section": "",
    "text": "Pseudocode\n\n\n\\begin{algorithm} \\caption{TD Double Q-Learning} \\begin{algorithmic} \\State \\textbf{Initialize:} \\State $Q_{1}(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $Q_{2}(s, a) \\gets 0$ for all $(s, a) \\in S \\times A$ \\State $\\gamma \\in [0, 1)$ \\State $\\alpha \\in (0, 1]$ \\State $\\epsilon &gt; 0$ \\State $\\pi \\gets$ arbitrary $\\epsilon$-soft policy \\State \\textbf{Loop for each episode:} \\State Initialize $S_{0}$ \\Repeat \\State Choose $A_{t}$ from $S_{t}$ using $\\pi$ \\State Take action $A_{t}$, observe $R_{t+1}$ and $S_{t+1}$ \\State With $0.5$ probability: \\State $Q_{1}(S_{t}, A_{t}) \\gets Q_{1}(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{2}(S_{t+1}, \\arg\\max_{a} Q_{1}(S_{t+1}, a)) - Q_{1}(S_{t}, A_{t})]$ \\State \\textbf{Else:} \\State $Q_{2}(S_{t}, A_{t}) \\gets Q_{2}(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{1}(S_{t+1}, \\arg\\max_{a} Q_{2}(S_{t+1}, a)) - Q_{2}(S_{t}, A_{t})]$ \\Until{$S_{t}$ is terminal} \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>32</span>¬† <span class='chapter-title'>6.4 Double Q-Learning</span>"
    ]
  }
]