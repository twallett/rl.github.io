---
title: "Dynamic Programming"
format:
  html:
    number-sections: false
---

::: {.callout-note appearance="simple" icon=false}
A formal framework that defines probability using three fundamental rules, ensuring consistency in measuring uncertainty. ðŸŽ²
:::

## Dynamic Programming

**Dynamic Programming** refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).

These algorithms have limited utility in reinforcement learning due to their **assumption of a perfect model** and their **computational expense**, but they are still important theoretically.

In fact, the rest of the course could be seen as a way to replicate the same effect of dynamic programming without its assumptions.

### Pseudocode

```pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Iterative Policy Evaluation (Prediction)}
\begin{algorithmic}[1]  % [1] enables line numbers

\State \textbf{Initialize:} 
\State $V(s) \gets 0$ for all $s \in S$
\State $\gamma \in [0,1)$
\State $\theta \gets 10^{-4}$ \Comment{small positive number for accuracy of estimation}

\Repeat
    \State $\Delta \gets 0$
    \ForAll{$s \in S$}
        \State $v \gets V(s)$
        \State $V(s) \gets \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) \left[ r + \gamma V(s') \right]$
        \State $\Delta \gets \max(\Delta, |v - V(s)|)$
    \EndFor
\Until{$\Delta < \theta$}

\end{algorithmic}
\end{algorithm}
```

<div style="text-align: center;">
  <a href="https://github.com/">
    <img src="images/github.svg" alt="GitHub" width="5%"/> 
  </a>
</div>

## Policy Improvement

We know how good it is to follow the current policy from $s$ â€” that is $v_{\pi}(s)$ â€” but would it be better or worse to change to a new policy $\pi^{'}$?

One way to check if it is better to switch from policy $\pi$ to $\pi^{'}$ is by checking if the following inequality holds:

$$ 
q_{\pi}(s, \pi^{'}(s)) \geq v_{\pi}(s) 
$$

*Intuition:* If selecting $a$ in $s$ and thereafter following policy $\pi$ is better than just following $\pi$, then there must be a better policy $\pi^{'}$.

The special case when this inequality is true is referred to as the **policy improvement theorem**.

## Value Iteration

One drawback of policy iteration is that policy evaluation is done iteratively, requiring convergence exactly to $v_{\pi}$, which occurs only in the limit.

**Value Iteration** is a dynamic programming algorithm that truncates the policy evaluation step after just one sweep.

### Pseudocode

```pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Value Iteration}
\begin{algorithmic}

\State \textbf{Initialize:} 
\State $V(s) \gets 0$ for all $s \in S$
\State $\gamma \in [0,1)$
\State $\theta \gets 10^{-4}$  \textit{// small positive number for accuracy of estimation}

\Repeat
    \State $\Delta \gets 0$
    \Forall{$s \in S$}
        \State $v \gets V(s)$
        \State $V(s) \gets \max_a \sum_{s^{'},r} p(s^{'},r|s,a) \left[ r + \gamma V(s^{'}) \right]$
        \State $\Delta \gets \max(\Delta, |v - V(s)|)$
    \Endfor
\Until{$\Delta < \theta$}

\end{algorithmic}
\end{algorithm}
```

<div style="text-align: center;">
  <a href="https://github.com/">
    <img src="images/github.svg" alt="GitHub" width="5%"/> <!-- Adjust width as needed -->
  </a>
</div>

## Generalized Policy Iteration

::: {.columns}
::: {.column width="50%"}

**Generalized Policy Iteration** refers to the general idea of letting policy evaluation and policy improvement processes interact, regardless of anything else.

Almost all of RL can be described as the policy always being improved with respect to the value function, and the value function always being driven toward the value function for the policy.

:::

::: {.column width="50%"}

![](images/GPI.png){width=40% fig-align="center"}

:::
:::
