---
title: "Dynamic Programming"
format:
  html:
    number-sections: false
---

::: {.callout-note appearance="simple" icon=false}
A formal framework that defines probability using three fundamental rules, ensuring consistency in measuring uncertainty. ðŸŽ²
:::

## Definition

- **Dynamic Programming** refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).

- These algorithms have limited utility in reinforcement learning due to their **assumption of a perfect model** and their **computational expense**, but they are still important theoretically.

- In fact, the rest of the course could be seen as a way to replicate the same effect of dynamic programming without its assumptions.

---

## Bellman Equations

So far, we looked at how to evaluate Bellman equations:

**Value Function**  
$$ v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s^{'}, r} p(s^{'},r|s,a)[r + \gamma v_{\pi}(s^{'})] $$  
*Intuition: How good is it to be in state \(s\) according to policy \(\pi\)?*

**Action-Value Function**  
$$ q_{\pi}(s, a) = \sum_{s^{'}, r} p(s^{'},r|s,a)[r + \gamma v_{\pi}(s^{'})] $$  
*Intuition: Is it good to select \(a\) in \(s\) and thereafter follow policy \(\pi\)?*

---

## Policy Evaluation

![](DP/imgs/4-1DP.pdf)

---

## Policy Improvement

- We know how good it is to follow the current policy from \(s\) â€” that is \(v_{\pi}(s)\) â€” but would it be better or worse to change to a new policy \(\pi^{'}\)?

- One way to check if it is better to switch from policy \(\pi\) to \(\pi^{'}\) is by checking if the following inequality holds:

  $$ q_{\pi}(s, \pi^{'}(s)) \geq v_{\pi}(s) $$

- *Intuition:* If selecting \(a\) in \(s\) and thereafter following policy \(\pi\) is better than just following \(\pi\), then there must be a better policy \(\pi^{'}\).

- The special case when this inequality is true is referred to as the **policy improvement theorem**.

---

## Value Iteration

- One drawback of policy iteration is that policy evaluation is done iteratively, requiring convergence exactly to \(v_{\pi}\), which occurs only in the limit.

- **Value Iteration** is a dynamic programming algorithm that truncates the policy evaluation step after just one sweep.

---

## Value Iteration Pseudocode

![](DP/imgs/4-2DP.pdf)

---

## Generalized Policy Iteration

::: {.columns}
::: {.column width="50%"}

- **Generalized Policy Iteration** refers to the general idea of letting policy evaluation and policy improvement processes interact, regardless of anything else.

- Almost all of RL can be described as the policy always being improved with respect to the value function, and the value function always being driven toward the value function for the policy.

:::

::: {.column width="50%"}

![](DP/imgs/GPI.png)

:::
:::
